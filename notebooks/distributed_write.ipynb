{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Writes with Lance\n",
    "\n",
    "Lance provides low-level APIs which can be integrated with distributed frameworks.\n",
    "In this tutorial, we'll show how to use with PySpark, but the same principles apply\n",
    "to other frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/22 15:13:59 WARN Utils: Your hostname, Wills-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.13 instead (on interface en0)\n",
      "23/08/22 15:13:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/22 15:14:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id=10, value='value_10'),\n",
       " Row(id=2, value='value_2'),\n",
       " Row(id=21, value='value_21'),\n",
       " Row(id=35, value='value_35'),\n",
       " Row(id=24, value='value_24')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "import lance\n",
    "import pyarrow as pa\n",
    "import pickle\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Create a Spark DataFrame\n",
    "df = spark.createDataFrame(\n",
    "    [ (i, f\"value_{i}\") for i in range(100) ],\n",
    "    (\"id\", \"value\"),\n",
    ")\n",
    "df = df.repartition(10)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dataset_uri = \"my_dataset\"\n",
    "\n",
    "def write_fragment(data_iter):\n",
    "    table = pa.Table.from_batches(list(data_iter))\n",
    "    fragment_meta = lance.fragment.LanceFragment.create(\n",
    "        dataset_uri,\n",
    "        table,\n",
    "    )\n",
    "    data = pickle.dumps(fragment_meta)\n",
    "    yield pa.RecordBatch.from_arrays([pa.array([data])], [\"fragment_meta\"])\n",
    "\n",
    "# https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.mapInArrow.html\n",
    "write_results = df.mapInArrow(write_fragment, schema=\"fragment_meta binary\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Fragment { id: 0, files: [DataFile { path: \"4f40324c-70e1-42e3-93b5-77e30d4073cf.lance\", fields: [0, 1] }], deletion_file: None },\n",
       " Fragment { id: 0, files: [DataFile { path: \"ecc2ee2d-2eaf-48ab-8124-76fc90c2f8fa.lance\", fields: [0, 1] }], deletion_file: None },\n",
       " Fragment { id: 0, files: [DataFile { path: \"a4a2d1c1-df46-42b1-805d-786a5a9d04e4.lance\", fields: [0, 1] }], deletion_file: None },\n",
       " Fragment { id: 0, files: [DataFile { path: \"53d501f1-4137-42d0-9240-ed8da9c0e1f3.lance\", fields: [0, 1] }], deletion_file: None },\n",
       " Fragment { id: 0, files: [DataFile { path: \"1e34e06a-d4f1-4344-b172-a35a21ab84ec.lance\", fields: [0, 1] }], deletion_file: None },\n",
       " Fragment { id: 0, files: [DataFile { path: \"221d6cf3-70e4-4821-8f95-c0353d62e49e.lance\", fields: [0, 1] }], deletion_file: None },\n",
       " Fragment { id: 0, files: [DataFile { path: \"41fd6c30-a6b4-4743-9142-c904d799abd4.lance\", fields: [0, 1] }], deletion_file: None },\n",
       " Fragment { id: 0, files: [DataFile { path: \"280b155a-61ef-4cb0-b944-50ef83a080fc.lance\", fields: [0, 1] }], deletion_file: None },\n",
       " Fragment { id: 0, files: [DataFile { path: \"21e29545-42d1-4755-a35e-bc9e4718ffae.lance\", fields: [0, 1] }], deletion_file: None },\n",
       " Fragment { id: 0, files: [DataFile { path: \"0cdb9cdc-d103-40d1-b37b-c3e2a9a1f702.lance\", fields: [0, 1] }], deletion_file: None }]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fragments = [pickle.loads(row.fragment_meta)\n",
    "             for row in write_results]\n",
    "fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>value_10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>value_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "      <td>value_21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35</td>\n",
       "      <td>value_35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>value_24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>45</td>\n",
       "      <td>value_45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>52</td>\n",
       "      <td>value_52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>67</td>\n",
       "      <td>value_67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>83</td>\n",
       "      <td>value_83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>value_99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id     value\n",
       "0   10  value_10\n",
       "1    2   value_2\n",
       "2   21  value_21\n",
       "3   35  value_35\n",
       "4   24  value_24\n",
       "..  ..       ...\n",
       "95  45  value_45\n",
       "96  52  value_52\n",
       "97  67  value_67\n",
       "98  83  value_83\n",
       "99  99  value_99\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = pa.schema([\n",
    "    pa.field(\"id\", pa.int64()),\n",
    "    pa.field(\"value\", pa.string()),\n",
    "])\n",
    "dataset = lance.LanceDataset._commit(\n",
    "    dataset_uri,\n",
    "    schema,\n",
    "    fragments,\n",
    "    mode=\"create\"\n",
    ")\n",
    "dataset.to_table().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appending to the dataset\n",
    "\n",
    "To append to a dataset, you can extend the fragments list with new fragments.\n",
    "Write the new fragments just as before, then assign them new ids, and finally\n",
    "call commit with the combined list of fragments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>value_10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>value_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "      <td>value_21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35</td>\n",
       "      <td>value_35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>value_24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>45</td>\n",
       "      <td>value_45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>52</td>\n",
       "      <td>value_52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>67</td>\n",
       "      <td>value_67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>83</td>\n",
       "      <td>value_83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>99</td>\n",
       "      <td>value_99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id     value\n",
       "0    10  value_10\n",
       "1     2   value_2\n",
       "2    21  value_21\n",
       "3    35  value_35\n",
       "4    24  value_24\n",
       "..   ..       ...\n",
       "195  45  value_45\n",
       "196  52  value_52\n",
       "197  67  value_67\n",
       "198  83  value_83\n",
       "199  99  value_99\n",
       "\n",
       "[200 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dataset_append(dataset, df):\n",
    "    write_results = df.mapInArrow(write_fragment, schema=\"fragment_meta binary\").collect()\n",
    "    new_fragments = [pickle.loads(row.fragment_meta)\n",
    "                     for row in write_results]\n",
    "    fragments = [fragment.metadata for fragment in dataset.get_fragments()]\n",
    "\n",
    "    # Need to assign new fragment ids\n",
    "    max_fragment_id = max(fragment.id for fragment in fragments)\n",
    "    for fragment in new_fragments:\n",
    "        max_fragment_id += 1\n",
    "        fragment.id = max_fragment_id\n",
    "    \n",
    "    fragments.extend(new_fragments)\n",
    "\n",
    "    return lance.LanceDataset._commit(\n",
    "        dataset.uri,\n",
    "        dataset.schema,\n",
    "        fragments\n",
    "    )\n",
    "\n",
    "dataset = dataset_append(dataset, df)\n",
    "dataset.to_table().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a new column\n",
    "\n",
    "One operation that is unique to Lance is the ability to add a new column to an\n",
    "existing table without rewriting the entire table. This can even be done in a\n",
    "distributed fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [(fragment.metadata.id,) for fragment in dataset.get_fragments()],\n",
    "    (\"id\",),\n",
    ")\n",
    "# Put each fragment in it's own partition\n",
    "df = df.repartition('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>value_10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>value_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "      <td>value_21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35</td>\n",
       "      <td>value_35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>value_24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>46</td>\n",
       "      <td>value_46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>55</td>\n",
       "      <td>value_55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>61</td>\n",
       "      <td>value_61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>73</td>\n",
       "      <td>value_73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>87</td>\n",
       "      <td>value_87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     value\n",
       "0  10  value_10\n",
       "1   2   value_2\n",
       "2  21  value_21\n",
       "3  35  value_35\n",
       "4  24  value_24\n",
       "5  46  value_46\n",
       "6  55  value_55\n",
       "7  61  value_61\n",
       "8  73  value_73\n",
       "9  87  value_87"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to run for each chunk of the table to generate the new column(s)\n",
    "def add_column(batch: pa.RecordBatch) -> pa.RecordBatch:\n",
    "    import pyarrow.compute as pc\n",
    "    array = pc.multiply(batch.column(0), 2)\n",
    "    return pa.RecordBatch.from_arrays([array], names=[\"double_id\"])\n",
    "\n",
    "def add_double_id(data_iter):\n",
    "    # Get the fragment from the id\n",
    "    row = list(data_iter)[0]\n",
    "    fragment_id = row[\"id\"][0].as_py()\n",
    "    fragment = lance.fragment.LanceFragment(dataset, fragment_id)\n",
    "\n",
    "    # Call `add_columns()` on the fragment\n",
    "    fragment_meta = fragment.add_columns(add_column, columns=['id'])\n",
    "\n",
    "    # Save the fragment metadata\n",
    "    data = pickle.dumps(fragment_meta)\n",
    "    yield pa.RecordBatch.from_arrays([pa.array([data])], [\"fragment_meta\"])\n",
    "\n",
    "write_results = df.mapInArrow(add_double_id, schema=\"fragment_meta binary\").collect()\n",
    "fragments = [pickle.loads(row.fragment_meta) for row in write_results]\n",
    "\n",
    "dataset = lance.LanceDataset._commit(\n",
    "    dataset.uri,\n",
    "    dataset.schema, # This is flawed because of the schema argument\n",
    "    fragments\n",
    ")\n",
    "\n",
    "dataset.to_table().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrying and idempotency\n",
    "\n",
    "PySpark provides retries for failed tasks, but not all distributed frameworks\n",
    "provide that automatically. But they are not aware of temporary files Lance might\n",
    "create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lance-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
