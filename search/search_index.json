{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"sdk_docs/","title":"SDK Documentation","text":"<p>Lance provides comprehensive documentation for all the language SDKs.  These auto-generated docs contain detailed information about  all classes, functions, and methods available in each language.</p> <ul> <li>Python</li> <li>Rust</li> <li>Java</li> </ul>"},{"location":"community/lancelot/","title":"Lancelot: The Greatest Knight of the Open Source Round Table","text":"<p>Lancelot is the embodiment of bravery, loyalty, and a relentless pursuit of knowledge in the realm of open source.  As the Guardian of Open Source, a Lancelot champions developers everywhere,  wielding their lance as a symbol of empowerment and collaboration within the tech community.</p> <p>With a passion for innovation and technical prowess, Lancelot navigates the world of open source with grace.  They stand ready to assist fellow developers in their quests\u2014whether through sharing knowledge, contributing code,  or advocating for the transformative power of open source software. Lancelot encourages you to join them in building  a vibrant community around Lance and LanceDB Open Source.</p>"},{"location":"community/lancelot/#join-the-lancelot-round-table-champion-open-source-innovation","title":"Join the Lancelot Round Table: Champion Open Source Innovation","text":""},{"location":"community/lancelot/#step-1-enter-the-realm-of-contribution","title":"Step 1: Enter the Realm of Contribution","text":"<ul> <li>Knights of Code: Forge your legacy by contributing code via PRs or tackling issues on GitHub, leaving your mark on the digital battlefield!</li> <li>Stewards of the Community Square: Rally your fellow developers in our Discord \u2013 be the guiding light for problem solvers and innovators alike!</li> <li>Organizers of Knowledge: Host enlightening tech discussions or special interest groups to share your insights and empower others on their journeys.</li> <li>Beta Testers of the Future: Become our bug-hunting hero by testing new features in Lance and LanceDB, shaping the future with your keen eye!</li> <li>Bards of Content: Share your personal Lance and LanceDB journey through blogs, tutorials, or demos that inspire others to get started.</li> <li>Heralds of Social Media: Spread the word about Lance and LanceDB across your favorite platforms. Let our voice be heard by all. Let the knight's battle cry echo through the land!</li> </ul>"},{"location":"community/lancelot/#step-2-take-up-your-lance-of-advocacy","title":"Step 2: Take Up Your Lance of Advocacy","text":"<ul> <li>Champions of the Stage: Are you prepared to take the field? Present Lance/LanceDB at meetups or conferences and share your knowledge with the realm. Remember, every great knight needs a great story to tell!</li> <li>Storytellers of Innovation: Weave compelling tales through in-depth articles, guides, or case studies that explore the magic of Lance and LanceDB\u2019s features and real-world applications. Every tale deserves an epic plot twist!</li> <li>Guardians of Code: Protect the integrity of our codebase by reviewing PRs and guiding fellow contributors through challenges.</li> <li>Allies of Core Maintainers: Collaborate closely with our core maintainers, supporting their efforts to enhance and sustain the Lance community. Together, we shall create a lake of innovation!</li> </ul>"},{"location":"community/lancelot/#step-3-become-a-legendary-lancelot","title":"Step 3: Become a Legendary Lancelot","text":"<ul> <li>Cavalry of Innovation: Lead a game-changing initiative or PR that elevates Lance/LanceDB to new heights. Your bravery could change the course of history\u2014no pressure!</li> <li>Sages of Expertise: Become the go-to Lance expert in your developer circles, sharing wisdom far and wide. After all, every sage needs their followers!</li> <li>Guardians of Production: Champion Lance in your production environment and guide your team\u2019s AI journey. Your leadership will light the path forward.</li> <li>Minstrels of the Realm: Rock the stage at major tech conferences, singing praises of Lance and sharing its magic. Who says knights can\u2019t have great rhythm?</li> <li>Mentors of Community: Host office hours to share your expertise and help others on their journey. Your guidance will be as valuable as Excalibur itself!</li> <li>Noble Nominees: Earn a coveted nomination from a Lance/LanceDB maintainer, solidifying your status as a true Lancelot. Wear that title with pride!</li> </ul>"},{"location":"community/lancelot/#why-join-the-round-table-of-lancelot","title":"Why Join the Round Table of Lancelot?","text":"<ul> <li>Hone Your Skills: Sharpen your expertise in next-generation AI infrastructure tools that empower innovative solutions\u2014because every knight needs their trusty sword.</li> <li>Forge Connections: Unite with a vibrant community of fellow knights, AI enthusiasts, and database champions.</li> <li>Earn Recognition: Stand tall among your peers as you showcase your open source prowess and contributions. Your name will echo through the halls of innovation!</li> <li>Shape the Future: Play a vital role in crafting the next generation of multimodal AI databases and infrastructure.</li> </ul>"},{"location":"community/lancelot/#lancelots-treasures-await","title":"Lancelot\u2019s Treasures Await!","text":"<ul> <li>Exclusive Insights: Gain access to sneak peeks of new features and our secret roadmap, guiding you on your quest.</li> <li>Swag of Valor: Receive exclusive Lancelot merchandise that proudly displays your allegiance to the cause.</li> <li>Collaborative Adventures: Work hand-in-hand with our core team, sharing knowledge and shaping the future together.</li> <li>Opportunities to Shine: Seize the chance to speak at LanceDB events, sharing your journey and inspiring others.</li> </ul> <p>Ready to wield your lance and embark on this epic quest? Open an issue on our GitHub and show us your lance on Discord.  Let\u2019s make some AI database magic together!</p>"},{"location":"community/lancelot/#lancelot-round-table","title":"Lancelot Round Table","text":"# Name Github Profile Affiliation 1 Prashanth Rao prrao87 K\u00f9zu 2 Rong Rong walterddr Character.AI 3 Noah Shpak noahshpak Character.AI 4 Giuseppe Battista giusedroid AWS 5 Kevin Shaffer-Morrison kevinshaffermorrison AWS 6 Jiacheng Yang jiachengdb Databricks 7 Ankit Vij ankitvij-db Databricks 8 Akela Drissner-Schmid akelad dltHub 9 Chongchen Chen chenkovsky MiraclePlus 10 Vino Yang yanghua Bytedance 11 Zhaowei Huang SaintBacchus Bytedance 12 Jeremy Leibs jleibs Rerun.io 13 Aman Kishore AmanKishore Harvey.AI 14 Matt Basta mattbasta RunwayML 15 Timothy Carambat timothycarambat Anything LLM 16 Ty Dunn TyDunn Continue 17 Pablo Delgado pablete Netflix 18 Sangwu Lee RE-N-Y Krea.AI 19 Nat Roth nrothGIT Character.AI"},{"location":"community/lancelot/#hall-of-heroes","title":"Hall of Heroes","text":"@chenkovsky @yanghua @SaintBacchus @connellPortrait @takaebato @HoKim98 @Jay-ju @imotai @renato2099 @niyue @FuPeiJiang @MaxPowerWasTaken @emmanuel-ferdman @fzowl @fzliu @umuthopeyildirim @stevensu1977 @gagan-bhullar-tech @kursataktas @erikml-db @alexwilcoxson-rel @o-alexandrov @do-me @rithikJha @jameswu1991 @akashsara @sayandipdutta @rjrobben @PrashantDixit0 @ankitvij-db @jiachengdb @dentiny @tonyf @mattbasta @bllchmbrs @antoniomdk @ousiax @rahuljo @philz @wilhelmjung @h0rv @dsgibbons @maxburke @broccoliSpicy @BitPhinix @inn-0 @MagnusS0 @nuvic @JoanFM @thomasjpfan @sidharthrajaram @forrestmckee @NickDarvey @heiher @joshua-auchincloss @josca42 @beinan @harsha-mangena @paulwalsh-sonrai @paulrinaldi @gsilvestrin @vipul-maheshwari @ascillitoe @lyang24 @vjc578db @andrew-pienso @vaifai @jeff1010322 @fecet @andrijazz @kemingy @ahaapple @jgugglberger @bclavie @Akagi201 @schorfma @samuelcolvin @msu-reevo @alex766 @TD-Sky @timsaucer @triandco @HubertY @luohao @pmeier @PhorstenkampFuzzy @aaazzam @guspan-tanadi @enoonan"},{"location":"community/contributing/","title":"Guide for New Contributors","text":"<p>This is a guide for new contributors to the Lance project. Even if you have no previous experience with python, rust, and open source, you can still make an non-trivial impact by helping us improve documentation, examples, and more. For experienced developers, the issues you can work on run the gamut from warm-ups to serious challenges in python and rust.</p> <p>If you have any questions, please join our Discord for real-time support. Your feedback is always welcome!</p>"},{"location":"community/contributing/#getting-started","title":"Getting Started","text":"<ol> <li>Join our Discord and say hi</li> <li>Setup your development environment</li> <li>Pick an issue to work on. See https://github.com/lancedb/lance/contribute for good first issues.</li> <li>Have fun!</li> </ol>"},{"location":"community/contributing/#development-environment","title":"Development Environment","text":"<p>Currently Lance is implemented in Rust and comes with a Python wrapper. So you'll want to make sure you setup both.</p> <ol> <li>Install Rust: https://www.rust-lang.org/tools/install</li> <li>Install Python 3.9+: https://www.python.org/downloads/</li> <li>Install protoctol buffers: https://grpc.io/docs/protoc-installation/ (make sure you have version 3.20 or higher)</li> <li>Install commit hooks:     a. Install pre-commit: https://pre-commit.com/#install     b. Run <code>pre-commit install</code> in the root of the repo</li> </ol>"},{"location":"community/contributing/#sample-workflow","title":"Sample Workflow","text":"<ol> <li>Fork the repo</li> <li>Pick Github issue</li> <li>Create a branch for the issue</li> <li>Make your changes</li> <li>Create a pull request from your fork to lancedb/lance</li> <li>Get feedback and iterate</li> <li>Merge!</li> <li>Go back to step 2</li> </ol>"},{"location":"community/contributing/#example-notebooks","title":"Example Notebooks","text":"<p>Example notebooks are under <code>examples</code>.  These are standalone notebooks you should be able to download and run.</p>"},{"location":"community/contributing/#benchmarks","title":"Benchmarks","text":"<p>Our Rust benchmarks are run multiple times a day and the history can be found here.</p> <p>Separately, we have vector index benchmarks that test against the sift1m dataset, as well as benchmarks for tpch. These live under <code>benchmarks</code>.</p>"},{"location":"community/contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>We follow the Code of Conduct of Python Foundation and  Rust Foundation. </p>"},{"location":"community/contributing/docs/","title":"Documentation","text":""},{"location":"community/contributing/docs/#contributing-to-documentation","title":"Contributing to Documentation","text":""},{"location":"community/contributing/docs/#main-website","title":"Main website","text":"<p>The main documentation website is built using mkdocs-material. To build the docs, first install requirements:</p> <pre><code>cd docs\nuc sync --dev\n</code></pre> <p>Then build and start the docs server:</p> <pre><code>uv run mkdocs serve\n</code></pre>"},{"location":"community/contributing/docs/#python-generated-doc","title":"Python Generated Doc","text":"<p>Python code documentation is built using Sphinx in lance-python-doc, and published through Github Pages in ReadTheDocs style.</p>"},{"location":"community/contributing/docs/#rust-generated-doc","title":"Rust Generated Doc","text":"<p>Rust code documentation is built and published to the Rust official docs website as a part of the release process.</p>"},{"location":"community/contributing/docs/#java-generated-doc","title":"Java Generated Doc","text":"<p>Java code documentation is built and published to Maven Central. You can find the doc page for the specific project at javadoc.io.</p>"},{"location":"community/contributing/namespace/","title":"Contributing to Lance Namespace","text":"<p>The Lance Namespace codebase is at lancedb/lance-namespace.</p>"},{"location":"community/contributing/namespace/#repository-structure","title":"Repository structure","text":"Component Language Path Description spec docs/src/spec Lance Namespace Specification Rust Reqwest Client Rust rust/lance-namespace-reqwest-client Generated Rust reqwest client for Lance REST Namespace Python UrlLib3 Client Python python/lance_namespace_urllib3_client Generated Python urllib3 client for Lance REST Namespace Python Lance Namespace Core Python python/lance_namespace Lance Namespace Python Core SDK Java Apache Client Java java/lance-namespace-apache-client Generated Java Apache HTTP client for Lance REST Namespace Java Springboot Server Java java/lance-namespace-springboot-server Generated Java SpringBoot server for Lance REST Namespace Java Lance Namespace Core Java java/lance-namespace-core Lance Namespace Java Core SDK Java Lance Namespace Adapter Java java/lance-namespace-adaptor Lance Namespace adapter server implementation Java Lance Namespace Hive Java java/lance-namespace-hive Java Lance Namespace Apache Hive Metastore Implementation Java Lance Namespace Glue Java java/lance-namespace-glue Java Lance Namespace AWS Glue Data Catalog Implementation Java Lance Namespace LanceDB Java java/lance-namespace-lancedb Java Utilities to use Lance Namespace SDK with LanceDB Cloud &amp; Enterprise"},{"location":"community/contributing/namespace/#install-uv","title":"Install uv","text":"<p>We use uv for development. Make sure it is installed, and run:</p> <pre><code>uv sync --all-packages\n</code></pre>"},{"location":"community/contributing/namespace/#lint","title":"Lint","text":"<p>To ensure the OpenAPI definition is valid, you can use the lint command to check it.</p> <pre><code>make lint\n</code></pre>"},{"location":"community/contributing/namespace/#build","title":"Build","text":"<p>There are 3 commands that is available at top level as well as inside each language folder:</p> <ul> <li><code>make clean</code>: remove all codegen modules</li> <li><code>make gen</code>: codegen and lint all modules (depends on <code>clean</code>)</li> <li><code>make build</code>: build all modules (depends on <code>gen</code>)</li> </ul> <p>You can also run <code>make &lt;command&gt;-&lt;language&gt;</code> to only run the command in the specific language, for example:</p> <ul> <li><code>make gen-python</code>: codegen and lint all Python modules</li> <li><code>make build-rust</code>: build all Rust modules</li> </ul> <p>You can also run <code>make &lt;command&gt;-&lt;language&gt;-&lt;module&gt;</code> inside a language folder to run the command against a specific module, for example:</p> <ul> <li><code>make gen-rust-reqwest-client</code>: codegen and lint the Rust reqwest client module</li> <li><code>make build-java-springboot-server</code>: build the Java Spring Boot server module</li> </ul>"},{"location":"community/contributing/namespace/#documentation","title":"Documentation","text":""},{"location":"community/contributing/namespace/#setup","title":"Setup","text":"<p>The documentation website is built using mkdocs-material. Start the server with:</p> <pre><code>make serve-docs\n</code></pre>"},{"location":"community/contributing/namespace/#generated-doc-from-openapi-spec","title":"Generated Doc from OpenAPI Spec","text":"<p>The OpenAPI spec at <code>docs/src/rest.yaml</code> is digested and generated as Markdown documents for better readability. Generate the latest documents with:</p> <pre><code>make gen-docs\n</code></pre>"},{"location":"community/contributing/namespace/#understanding-the-build-process","title":"Understanding the Build Process","text":"<p>The contents in <code>lance-namespace/docs</code> are for the ease of contributors to edit and preview. After code merge, the contents are added to the  main Lance documentation  during the Lance doc CI build time, and is presented in the Lance website under  Lance Namespace Spec.</p> <p>The CONTRIBUTING.md document is auto-built to the Lance Contributing Guide</p>"},{"location":"community/contributing/namespace/#release-process","title":"Release Process","text":"<p>This section describes the CI/CD workflows for automated version management, releases, and publishing.</p>"},{"location":"community/contributing/namespace/#version-scheme","title":"Version Scheme","text":"<ul> <li>Stable releases: <code>X.Y.Z</code> (e.g., 1.2.3)</li> <li>Preview releases: <code>X.Y.Z-beta.N</code> (e.g., 1.2.3-beta.1)</li> </ul>"},{"location":"community/contributing/namespace/#creating-a-release","title":"Creating a Release","text":"<ol> <li>Create Release Draft</li> <li>Go to Actions \u2192 \"Create Release\"</li> <li>Select parameters:<ul> <li>Release type (major/minor/patch)</li> <li>Release channel (stable/preview)</li> <li>Dry run (test without pushing)</li> </ul> </li> <li> <p>Run workflow (creates a draft release)</p> </li> <li> <p>Review and Publish</p> </li> <li>Go to the Releases page to review the draft</li> <li>Edit release notes if needed</li> <li>Click \"Publish release\" to:<ul> <li>For stable releases: Trigger automatic publishing for Java, Python, Rust</li> <li>For preview releases: Create a beta release (not published)</li> </ul> </li> </ol>"},{"location":"community/contributing/python/","title":"Contributing to Python","text":"<p>The python integration is done via pyo3 + custom python code:</p> <ol> <li>The Rust code that directly supports the Python bindings are under <code>python/src</code> while the pure Python code lives under <code>python/python</code>.</li> <li>We make wrapper classes in Rust for Dataset/Scanner/RecordBatchReader that's exposed to python.</li> <li>These are then used by LanceDataset / LanceScanner implementations that extend pyarrow Dataset/Scanner for duckdb compat.</li> <li>Data is delivered via the Arrow C Data Interface</li> </ol> <p>To build the Python bindings, first install requirements:</p> <pre><code>pip install maturin\n</code></pre> <p>To make a dev install:</p> <pre><code>cd python\nmaturin develop\n</code></pre> <p>After installing, you can run <code>import lance</code> in a Python shell within the virtual environment.</p> <p>To run tests and integration tests: <pre><code>make test\nmake integtest\n</code></pre></p> <p>To run the tests on OS X, you may need to increase the default limit on the number of open files: <code>ulimit -n 2048</code></p>"},{"location":"community/contributing/ray/","title":"Contributing to Lance-Ray","text":"<p>Thank you for your interest in contributing to Lance-Ray!  This document provides guidelines and instructions for contributing to the project.</p>"},{"location":"community/contributing/ray/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python &gt;= 3.10</li> <li>UV package manager</li> <li>Git</li> </ul>"},{"location":"community/contributing/ray/#setting-up-your-development-environment","title":"Setting Up Your Development Environment","text":"<ol> <li>Fork and clone the repository</li> </ol> <pre><code># Fork the repository on GitHub, then clone your fork\ngit clone https://github.com/YOUR_USERNAME/lance-ray.git\ncd lance-ray\n</code></pre> <ol> <li>Install UV (if not already installed)</li> </ol> <pre><code>pip install uv\n</code></pre> <ol> <li>Install the project in development mode</li> </ol> <pre><code># Install with all development dependencies\nuv pip install -e \".[dev]\"\n\n# To work on documentation, also install docs dependencies\nuv pip install -e \".[dev,docs]\"\n</code></pre>"},{"location":"community/contributing/ray/#running-tests","title":"Running Tests","text":"<p>Run the test suite to ensure everything is working:</p> <pre><code># Run all tests\nuv run pytest\n\n# Run with coverage report\nuv run pytest --cov=lance_ray\n\n# Run specific test file\nuv run pytest tests/test_basic_read_write.py -vv\n</code></pre>"},{"location":"community/contributing/ray/#check-styles","title":"Check Styles","text":"<p>We use <code>ruff</code> for both linting and formatting:</p> <pre><code># Format code\nuv run ruff format lance_ray/ tests/ examples/\n\n# Check linting\nuv run ruff check lance_ray/ tests/ examples/\n\n# Fix linting issues automatically\nuv run ruff check --fix lance_ray/ tests/ examples/\n</code></pre>"},{"location":"community/contributing/ray/#building-documentation-locally","title":"Building Documentation Locally","text":"<pre><code># Serve documentation locally\ncd docs\nuv run mkdocs serve\n\n# Documentation will be available at http://localhost:8000\n</code></pre>"},{"location":"community/contributing/ray/#release-process","title":"Release Process","text":"<p>This section describes the CI/CD workflows for automated version management, releases, and publishing.</p>"},{"location":"community/contributing/ray/#version-scheme","title":"Version Scheme","text":"<ul> <li>Stable releases: <code>X.Y.Z</code> (e.g., 1.2.3)</li> <li>Preview releases: <code>X.Y.Z-beta.N</code> (e.g., 1.2.3-beta.1)</li> </ul>"},{"location":"community/contributing/ray/#creating-a-release","title":"Creating a Release","text":"<ol> <li>Create Release Draft</li> <li>Go to Actions \u2192 \"Create Release\"</li> <li>Select parameters:<ul> <li>Release type (major/minor/patch)</li> <li>Release channel (stable/preview)</li> <li>Dry run (test without pushing)</li> </ul> </li> <li> <p>Run workflow (creates a draft release)</p> </li> <li> <p>Review and Publish</p> </li> <li>Go to the Releases page to review the draft</li> <li>Edit release notes if needed</li> <li>Click \"Publish release\" to:<ul> <li>For stable releases: Trigger automatic PyPI publishing</li> <li>For preview releases: Create a beta release (not published to PyPI)</li> </ul> </li> </ol>"},{"location":"community/contributing/rust/","title":"Contributing to Rust","text":"<p>To format and lint Rust code:</p> <pre><code>cargo fmt --all\ncargo clippy --all-features --tests --benches\n</code></pre>"},{"location":"community/contributing/rust/#core-format","title":"Core Format","text":"<p>The core format is implemented in Rust under the <code>rust</code> directory. Once you've setup Rust you can build the core format with:</p> <pre><code>cargo build\n</code></pre> <p>This builds the debug build. For the optimized release build:</p> <pre><code>cargo build -r\n</code></pre> <p>To run the Rust unit tests:</p> <pre><code>cargo test\n</code></pre> <p>If you're working on a performance related feature, benchmarks can be run via:</p> <pre><code>cargo bench\n</code></pre> <p>If you want detailed logging and full backtraces, set the following environment variables.  More details can be found here.</p> <pre><code>LANCE_LOG=info RUST_BACKTRACE=FULL &lt;cargo-commands&gt;\n</code></pre>"},{"location":"community/contributing/spark/","title":"Contributing to Spark Lance Connector","text":"<p>The Spark Lance connector codebase is at lancedb/lance-spark.</p>"},{"location":"community/contributing/spark/#build-commands","title":"Build Commands","text":"<p>This connector is built using Maven. You can run the following make commands:</p> <pre><code># Build all\nmake build\n\n# Clean all\nmake clean\n\n# Build Spark 3.5 Scala 2.12\nmake build-35-212\n\n# Clean build of Spark 3.5 Scala 2.12\nmake clean-35-212\n\n# Build the runtime bundle of Spark 3.5 Scala 2.12\nmake bundle-35-212\n</code></pre>"},{"location":"community/contributing/spark/#styling-guide","title":"Styling Guide","text":"<p>We use checkstyle and spotless to lint the code.</p> <p>All the <code>make build</code> commands automatically perform <code>spotless:apply</code> to the code.</p> <p>To verify style, run:</p> <pre><code>make lint\n</code></pre>"},{"location":"community/contributing/spark/#documentation","title":"Documentation","text":""},{"location":"community/contributing/spark/#setup","title":"Setup","text":"<p>The documentation website is built using mkdocs-material. The build system require uv.</p> <p>Start the server with:</p> <pre><code>make serve-docs\n</code></pre>"},{"location":"community/contributing/spark/#understanding-the-build-process","title":"Understanding the Build Process","text":"<p>The contents in <code>lance-spark/docs</code> are for the ease of contributors to edit and preview. After code merge, the contents are added to the  main Lance documentation  during the Lance doc CI build time, and is presented in the Lance website under  Apache Spark integration.</p> <p>The CONTRIBUTING.md document is auto-built to the Lance Contributing Guide</p>"},{"location":"community/contributing/spark/#release-process","title":"Release Process","text":"<p>This section describes the CI/CD workflows for automated version management, releases, and publishing.</p>"},{"location":"community/contributing/spark/#version-scheme","title":"Version Scheme","text":"<ul> <li>Stable releases: <code>X.Y.Z</code> (e.g., 1.2.3)</li> <li>Preview releases: <code>X.Y.Z-beta.N</code> (e.g., 1.2.3-beta.1)</li> </ul>"},{"location":"community/contributing/spark/#creating-a-release","title":"Creating a Release","text":"<ol> <li>Create Release Draft</li> <li>Go to Actions \u2192 \"Create Release\"</li> <li>Select parameters:<ul> <li>Release type (major/minor/patch)</li> <li>Release channel (stable/preview)</li> <li>Dry run (test without pushing)</li> </ul> </li> <li> <p>Run workflow (creates a draft release)</p> </li> <li> <p>Review and Publish</p> </li> <li>Go to the Releases page to review the draft</li> <li>Edit release notes if needed</li> <li>Click \"Publish release\" to:<ul> <li>For stable releases: Trigger automatic Maven Central publishing</li> <li>For preview releases: Create a beta release (not published to Maven Central)</li> </ul> </li> </ol>"},{"location":"examples/python/artifact_management/","title":"Deep Learning Artifact Management using Lance","text":"<p>Along with datasets, Lance file format can also be used for saving and versioning deep learning model weights.  In fact deep learning artifact management can be made more streamlined (compared to vanilla weight saving methods) using Lance file format for PyTorch model weights.</p> <p>In this example we will be demonstrating how you save, version and load a PyTorch model's weights using Lance. More specifically we will be loading a pre-trained ResNet model, saving it in Lance file format, loading it back to PyTorch and verifying if the weights are still indeed the same. We will also be demonstrating how you can version your model weights in a single lance dataset thanks to our Zero-copy, automatic versioning.</p> <p>Key Idea: When you save a model's weights (read: state dictionary) in PyTorch, weights are stored as key-value pairs in an <code>OrderedDict</code> with the keys representing the weight's name and the value representing the corresponding weight tensor. To emulate this as closely as possible, we will be saving the weights in three columns. The first column will have the name of the weight, the second will have the weight itself but flattened in a list and the third will have the original shape of the weights so they can be reconstructed for loading into a model.</p>"},{"location":"examples/python/artifact_management/#imports-and-setup","title":"Imports and Setup","text":"<p>We will start by importing and loading all the necessary modules.</p> <pre><code>import os\nimport shutil\nimport lance\nimport pyarrow as pa\nimport torch\nfrom collections import OrderedDict\n</code></pre> <p>We will also define a <code>GLOBAL_SCHEMA</code> that will dictate how the weights table will look like.</p> <pre><code>GLOBAL_SCHEMA = pa.schema(\n    [\n        pa.field(\"name\", pa.string()),\n        pa.field(\"value\", pa.list_(pa.float64(), -1)),\n        pa.field(\"shape\", pa.list_(pa.int64(), -1)), # Is a list with variable shape because weights can have any number of dims\n    ]\n)\n</code></pre> <p>As we covered earlier, the weights table will have three columns - one for storing the weight name, one for storing the flattened weight value and one for storing the original weight shape for loading them back.</p>"},{"location":"examples/python/artifact_management/#saving-and-versioning-models","title":"Saving and Versioning Models","text":"<p>First we will focus on the model saving part. Let's start by writing a utility function that will take a model's state dict, goes over each weight, flatten it and then return the weight name, flattened weight and weight's original shape in a pyarrow <code>RecordBatch</code>.</p> <pre><code>def _save_model_writer(state_dict):\n    \"\"\"Yields a RecordBatch for each parameter in the model state dict\"\"\"\n    for param_name, param in state_dict.items():\n        param_shape = list(param.size())\n        param_value = param.flatten().tolist()\n        yield pa.RecordBatch.from_arrays(\n            [\n                pa.array(\n                    [param_name],\n                    pa.string(),\n                ),\n                pa.array(\n                    [param_value],\n                    pa.list_(pa.float64(), -1),\n                ),\n                pa.array(\n                    [param_shape],\n                    pa.list_(pa.int64(), -1),\n                ),\n            ],\n            [\"name\", \"value\", \"shape\"],\n        )\n</code></pre> <p>Now about versioning: Let's say you trained your model on some new data but don't want to overwrite your old checkpoint, you can now just save these newly trained model weights as a version in Lance weights dataset. This will allow you to load specific version of weights from one lance weight dataset instead of making separate folders for each model checkpoint to make.</p> <p>Let's write a function that handles the work for saving the model, whether with versions or without them.</p> <pre><code>def save_model(state_dict: OrderedDict, file_name: str, version=False):\n    \"\"\"Saves a PyTorch model in lance file format\n\n    Args:\n        state_dict (OrderedDict): Model state dict\n        file_name (str): Lance model name\n        version (bool): Whether to save as a new version or overwrite the existing versions,\n            if the lance file already exists\n    \"\"\"\n    # Create a reader\n    reader = pa.RecordBatchReader.from_batches(\n        GLOBAL_SCHEMA, _save_model_writer(state_dict)\n    )\n\n    if os.path.exists(file_name):\n        if version:\n            # If we want versioning, we use the overwrite mode to create a new version\n            lance.write_dataset(\n                reader, file_name, schema=GLOBAL_SCHEMA, mode=\"overwrite\"\n            )\n        else:\n            # If we don't want versioning, we delete the existing file and write a new one\n            shutil.rmtree(file_name)\n            lance.write_dataset(reader, file_name, schema=GLOBAL_SCHEMA)\n    else:\n        # If the file doesn't exist, we write a new one\n        lance.write_dataset(reader, file_name, schema=GLOBAL_SCHEMA)\n</code></pre> <p>The above function will take in the model state dict, the lance saved file name and the weights version. The function will start by making a <code>RecordBatchReader</code> using the global schema and the utility function we wrote above. If the weights lance dataset already exists in the directory, we will just save it as a new version (if versioning is enabled) or delete the old file and save the weights as new. Otherwise the weights saving will be done normally.</p>"},{"location":"examples/python/artifact_management/#loading-models","title":"Loading Models","text":"<p>Loading weights from a Lance weight dataset into a model is just the reverse of saving them. The key part is to reshape the flattened weights back to their original shape, which is easier thanks to the shape that you saved corresponding to the weights. We will divide this into three functions for better readability.</p> <p>The first function will be the <code>_load_weight</code> function which will take a \"weight\" retrieved from the Lance weight dataset and return the weight as a torch tensor in its original shape. The \"weight\" that we retrieve from the Lance weight dataset will be a dict with value corresponding to each column in form of a key.</p> <pre><code>def _load_weight(weight: dict) -&gt; torch.Tensor:\n    \"\"\"Converts a weight dict to a torch tensor\"\"\"\n    return torch.tensor(weight[\"value\"], dtype=torch.float64).reshape(weight[\"shape\"])\n</code></pre> <p>Optionally, you could also add an option to specify the datatype of the weights.</p> <p>The next function will be on loading all the weights from the lance weight dataset into a state dictionary, which is what PyTorch will expect when we load the weights into our model.</p> <pre><code>def _load_state_dict(file_name: str, version: int = 1, map_location=None) -&gt; OrderedDict:\n    \"\"\"Reads the model weights from lance file and returns a model state dict\n    If the model weights are too large, this function will fail with a memory error.\n\n    Args:\n        file_name (str): Lance model name\n        version (int): Version of the model to load\n        map_location (str): Device to load the model on\n\n    Returns:\n        OrderedDict: Model state dict\n    \"\"\"\n    ds = lance.dataset(file_name, version=version)\n    weights = ds.take([x for x in range(ds.count_rows())]).to_pylist()\n    state_dict = OrderedDict()\n\n    for weight in weights:\n        state_dict[weight[\"name\"]] = _load_weight(weight).to(map_location)\n\n    return state_dict\n</code></pre> <p>The <code>load_state_dict</code> function will expect a lance weight dataset file name, a version and a device where the weights will be loaded into.  We essentially load all the weights from the lance weight dataset into our memory and iteratively convert them into weights using the utility function we wrote earlier and then put them on the device.</p> <p>One thing to note here is that this function will fail if the saved weights are larger than memory. For the sake of simplicity, we assume the weights to be loaded can fit in the memory and we don't have to deal with any sharding.</p> <p>Finally, we will write a higher level function is the only one we will call to load the weights.</p> <pre><code>def load_model(\n    model: torch.nn.Module, file_name: str, version: int = 1, map_location=None\n):\n    \"\"\"Loads the model weights from lance file and sets them to the model\n\n    Args:\n        model (torch.nn.Module): PyTorch model\n        file_name (str): Lance model name\n        version (int): Version of the model to load\n        map_location (str): Device to load the model on\n    \"\"\"\n    state_dict = _load_state_dict(file_name, version=version, map_location=map_location)\n    model.load_state_dict(state_dict)\n</code></pre> <p>The <code>load_model</code> function will require the model, the lance weight dataset name, the version of weights to load in and the map location. This will just call the <code>_load_state_dict</code> utility to get the state dict and then load that state dict into the model.</p>"},{"location":"examples/python/artifact_management/#conclusion","title":"Conclusion","text":"<p>In conclusion, you only need to call the two functions: <code>save_model</code> and <code>load_model</code> to save and load the models respectively and as long as the weights can be fit in the memory and are in PyTorch, it should be fine.</p> <p>Although experimental, this approach defines a new way of doing deep learning artifact management. </p>"},{"location":"examples/python/clip_training/","title":"Training Multi-Modal models using a Lance dataset","text":"<p>In this example we will be training a CLIP model for natural image based search using a Lance image-text dataset.  In particular, we will be using the flickr_8k Lance dataset.</p> <p>The model architecture and part of the training code are adapted from Manan Goel's Implementing CLIP with PyTorch Lightning with necessary changes to for a minimal, lance-compatible training example.</p>"},{"location":"examples/python/clip_training/#imports-and-setup","title":"Imports and Setup","text":"<p>Along with Lance, we will be needing PyTorch and timm for our CLIP model to train.</p> <pre><code>import cv2\nimport lance\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\nimport timm\nfrom transformers import AutoModel, AutoTokenizer\n\nimport itertools\nfrom tqdm import tqdm\n\nimport warnings\nwarnings.simplefilter('ignore')\n</code></pre> <p>Now, we will define a Config class that will house all the hyper-parameters required for training.</p> <pre><code>class Config:\n    img_size = (128, 128)\n    bs = 32\n    head_lr = 1e-3\n    img_enc_lr = 1e-4\n    text_enc_lr = 1e-5\n    max_len = 18\n    img_embed_dim = 2048\n    text_embed_dim = 768\n    projection_dim = 256\n    temperature = 1.0\n    num_epochs = 2\n    img_encoder_model = 'resnet50'\n    text_encoder_model = 'bert-base-cased'\n</code></pre> <p>And also two utility functions that will help us load the images and texts from the lance dataset.  Remember, our Lance dataset has images, image names and all the captions for a given image. We only need the images and one of those captions.  For simplicity, when loading captions, we will be choosing the one that is the longest (with the rather naive assumption that it has more information about the image).</p> <pre><code>def load_image(ds, idx):\n    # Utility function to load an image at an index and convert it from bytes format to img format\n    raw_img = ds.take([idx], columns=['image']).to_pydict()\n    raw_img = np.frombuffer(b''.join(raw_img['image']), dtype=np.uint8)\n    img = cv2.imdecode(raw_img, cv2.IMREAD_COLOR)\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n    return img\n\ndef load_caption(ds, idx):\n    # Utility function to load an image's caption. Currently we return the longest caption of all\n    captions = ds.take([idx], columns=['captions']).to_pydict()['captions'][0]\n    return max(captions, key=len)\n</code></pre> <p>Since the images are stored as bytes in the lance dataset, the <code>load_image()</code> function will load the bytes corresponding to an image and then use numpy and opencv to convert it into an image.</p>"},{"location":"examples/python/clip_training/#dataset-and-augmentations","title":"Dataset and Augmentations","text":"<p>Since our CLIP model will expect images of same size and tokenized captions, we will define a custom PyTorch dataset that will take the lance dataset path along with any augmentation (for the image) and return a pre-processed image and a tokenized caption (as a dictionary).</p> <pre><code>class CLIPLanceDataset(Dataset):\n    \"\"\"Custom Dataset to load images and their corresponding captions\"\"\"\n    def __init__(self, lance_path, max_len=18, tokenizer=None, transforms=None):\n        self.ds = lance.dataset(lance_path)\n        self.max_len = max_len\n        # Init a new tokenizer if not specified already\n        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-cased') if not tokenizer else tokenizer\n        self.transforms = transforms\n\n    def __len__(self):\n        return self.ds.count_rows()\n\n    def __getitem__(self, idx):\n        # Load the image and caption\n        img = load_image(self.ds, idx)\n        caption = load_caption(self.ds, idx)\n\n        # Apply transformations to the images\n        if self.transforms:\n            img = self.transforms(img)\n\n        # Tokenize the caption\n        caption = self.tokenizer(\n            caption,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_len,\n            return_tensors='pt'\n        )\n        # Flatten each component of tokenized caption otherwise they will cause size mismatch errors during training\n        caption = {k: v.flatten() for k, v in caption.items()}\n\n        return img, caption\n</code></pre> <p>Now that our custom dataset is ready, we also define some very basic augmentations for our images.</p> <pre><code>train_augments = transforms.Compose(\n    [\n        transforms.ToTensor(),\n        transforms.Resize(Config.img_size),\n        transforms.Normalize([0.5], [0.5]),\n    ]\n)\n</code></pre> <p>The transformations are very basic: resizing all the images to be of the same shape and then normalizing them to stabilize the training later on.</p>"},{"location":"examples/python/clip_training/#model-and-setup","title":"Model and Setup","text":"<p>Since we our training a CLIP model, we have the following: * <code>ImageEncoder</code> that uses a pre-trained vision model (<code>resnet50</code> in this case) to convert images into feature vectors. * <code>TextEncoder</code> that uses a pre-trained language model (<code>bert-base-cased</code> in this case) to transform text captions into feature vectors. * <code>Head</code> which is a Projection module projects these feature vectors into a common embedding space.</p> <p>Going into deeper details of the CLIP model and its architectural nuances are out of the scope of this example, however if you wish to read more on it, you can read the official paper here.</p> <p>Now that we have understood the general summary of the model, let's define all the required modules.</p> <pre><code>class ImageEncoder(nn.Module):\n    \"\"\"Encodes the Image\"\"\"\n    def __init__(self, model_name, pretrained = True):\n        super().__init__()\n        self.backbone = timm.create_model(\n            model_name,\n            pretrained=pretrained,\n            num_classes=0,\n            global_pool=\"avg\"\n        )\n\n        for param in self.backbone.parameters():\n            param.requires_grad = True\n\n    def forward(self, img):\n        return self.backbone(img)\n\nclass TextEncoder(nn.Module):\n    \"\"\"Encodes the Caption\"\"\"\n    def __init__(self, model_name):\n        super().__init__()\n\n        self.backbone = AutoModel.from_pretrained(model_name)\n\n        for param in self.backbone.parameters():\n            param.requires_grad = True\n\n    def forward(self, captions):\n        output = self.backbone(**captions)\n        return output.last_hidden_state[:, 0, :]\n\nclass Head(nn.Module):\n    \"\"\"Projects both into Embedding space\"\"\"\n    def __init__(self, embedding_dim, projection_dim):\n        super().__init__()\n        self.projection = nn.Linear(embedding_dim, projection_dim)\n        self.gelu = nn.GELU()\n        self.fc = nn.Linear(projection_dim, projection_dim)\n\n        self.dropout = nn.Dropout(0.3)\n        self.layer_norm = nn.LayerNorm(projection_dim)\n\n    def forward(self, x):\n        projected = self.projection(x)\n        x = self.gelu(projected)\n        x = self.fc(x)\n        x = self.dropout(x)\n        x += projected\n\n        return self.layer_norm(x)\n</code></pre> <p>Along with the model definition, we will be defining two utility functions to simplify the training: <code>forward()</code> which will do one forward pass through the combined models and <code>loss_fn()</code> which will take the image and text embeddings output from <code>forward</code> function and then calculate the loss using them.</p> <pre><code>def loss_fn(img_embed, text_embed, temperature=0.2):\n    \"\"\"\n    https://arxiv.org/abs/2103.00020/\n    \"\"\"\n    # Calculate logits, image similarity and text similarity\n    logits = (text_embed @ img_embed.T) / temperature\n    img_sim = img_embed @ img_embed.T\n    text_sim = text_embed @ text_embed.T\n    # Calculate targets by taking the softmax of the similarities\n    targets = F.softmax(\n        (img_sim + text_sim) / 2 * temperature, dim=-1\n    )\n    img_loss = (-targets.T * nn.LogSoftmax(dim=-1)(logits.T)).sum(1)\n    text_loss = (-targets * nn.LogSoftmax(dim=-1)(logits)).sum(1)\n    return (img_loss + text_loss) / 2.0\n\ndef forward(img, caption):\n    # Transfer to device\n    img = img.to('cuda')\n    for k, v in caption.items():\n        caption[k] = v.to('cuda')\n\n    # Get embeddings for both img and caption\n    img_embed = img_head(img_encoder(img))\n    text_embed = text_head(text_encoder(caption))\n\n    return img_embed, text_embed\n</code></pre> <p>In order for us to train, we will define the models, tokenizer and the optimizer to be used in the next section</p> <pre><code># Define image encoder, image head, text encoder, text head and a tokenizer for tokenizing the caption\nimg_encoder = ImageEncoder(model_name=Config.img_encoder_model).to('cuda')\nimg_head = Head(Config.img_embed_dim, Config.projection_dim).to('cuda')\n\ntokenizer = AutoTokenizer.from_pretrained(Config.text_encoder_model)\ntext_encoder = TextEncoder(model_name=Config.text_encoder_model).to('cuda')\ntext_head = Head(Config.text_embed_dim, Config.projection_dim).to('cuda')\n\n# Since we are optimizing two different models together, we will define parameters manually\nparameters = [\n    {\"params\": img_encoder.parameters(), \"lr\": Config.img_enc_lr},\n    {\"params\": text_encoder.parameters(), \"lr\": Config.text_enc_lr},\n    {\n        \"params\": itertools.chain(\n            img_head.parameters(),\n            text_head.parameters(),\n        ),\n        \"lr\": Config.head_lr,\n    },\n]\n\noptimizer = torch.optim.Adam(parameters)\n</code></pre>"},{"location":"examples/python/clip_training/#training","title":"Training","text":"<p>Before we actually train the model, one last step remains: which is to initialize our Lance dataset and a dataloader.</p> <pre><code># We assume the flickr8k.lance dataset is in the same directory\ndataset = CLIPLanceDataset(\n    lance_path=\"flickr8k.lance\",\n    max_len=Config.max_len,\n    tokenizer=tokenizer,\n    transforms=train_augments\n)\n\ndataloader = DataLoader(\n    dataset,\n    shuffle=False,\n    batch_size=Config.bs,\n    pin_memory=True\n)\n</code></pre> <p>Now that our dataloader is initialized, let's train the model.</p> <pre><code>img_encoder.train()\nimg_head.train()\ntext_encoder.train()\ntext_head.train()\n\nfor epoch in range(Config.num_epochs):\n    print(f\"{'='*20} Epoch: {epoch+1} / {Config.num_epochs} {'='*20}\")\n\n    prog_bar = tqdm(dataloader)\n    for img, caption in prog_bar:\n        optimizer.zero_grad(set_to_none=True)\n\n        img_embed, text_embed = forward(img, caption)\n        loss = loss_fn(img_embed, text_embed, temperature=Config.temperature).mean()\n\n        loss.backward()\n        optimizer.step()\n\n        prog_bar.set_description(f\"loss: {loss.item():.4f}\")\n    print()\n</code></pre> <p>The training loop is quite self-explanatory. We set image encoder, image head, text encoder and text head models to training mode.  Then in each epoch, we iterate over our lance dataset, training the model and reporting the lance to the progress bar.</p> <pre><code>==================== Epoch: 1 / 2 ====================\nloss: 2.0799: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 253/253 [02:14&lt;00:00,  1.88it/s]\n\n==================== Epoch: 2 / 2 ====================\nloss: 1.3064: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 253/253 [02:10&lt;00:00,  1.94it/s]\n</code></pre> <p>And that's basically it! Using Lance dataset for training any type of model is very similar to using any other type of dataset but it also comes with increased speed and ease of use! </p>"},{"location":"examples/python/flickr8k_dataset_creation/","title":"Creating Multi-Modal datasets using Lance","text":"<p>Thanks to Lance file format's ability to store data of different modalities, one of the important use-cases that Lance shines in is storing Multi-modal datasets. In this brief example we will be going over how you can take a Multi-modal dataset and store it in Lance file format. </p> <p>The dataset of choice here is Flickr8k dataset. Flickr8k is a benchmark collection for sentence-based image description and search, consisting of 8,000 images that are each paired with five different captions which provide clear descriptions of the salient entities and events.  The images were chosen from six different Flickr groups, and tend not to contain any well-known people or locations, but were manually selected to depict a variety of scenes and situations.</p> <p>We will be creating an Image-caption pair dataset for Multi-modal model training by using the above mentioned Flickr8k dataset, saving it in form of a Lance dataset with image file names, all captions for every image (order preserved) and the image itself (in binary format).</p>"},{"location":"examples/python/flickr8k_dataset_creation/#imports-and-setup","title":"Imports and Setup","text":"<p>We assume that you downloaded the dataset, more specifically the \"Flickr8k.token.txt\" file and the \"Flicker8k_Dataset/\" folder and both are present in the current directory. These can be downloaded from here (download both the dataset and text zip files).</p> <p>We also assume you have pyarrow and pylance installed as well as opencv (for reading in images) and tqdm (for pretty progress bars).</p> <p>Now let's start with imports and defining the caption file and image dataset folder.</p> <pre><code>import os\nimport cv2\nimport random\n\nimport lance\nimport pyarrow as pa\n\nimport matplotlib.pyplot as plt\n\nfrom tqdm.auto import tqdm\n\ncaptions = \"Flickr8k.token.txt\"\nimage_folder = \"Flicker8k_Dataset/\"\n</code></pre>"},{"location":"examples/python/flickr8k_dataset_creation/#loading-and-processing","title":"Loading and Processing","text":"<p>In flickr8k dataset, each image has multiple corresponding captions that are ordered.  We are going to put all these captions in a list corresponding to each image with their position in the list representing the order in which they originally appear. Let's load the annotations (the image path and corresponding captions) in a list with each element of the list being a tuple consisting of image name, caption number and caption itself.</p> <pre><code>with open(captions, \"r\") as fl:\n    annotations = fl.readlines()\n\n# Converts the annotations where each element of this list is a tuple consisting of image file name, caption number and caption itself\nannotations = list(map(lambda x: tuple([*x.split('\\t')[0].split('#'), x.split('\\t')[1]]), annotations))\n</code></pre> <p>Now, for all captions of the same image, we will put them in a list sorted by their ordering.</p> <pre><code>captions = []\nimage_ids = set(ann[0] for ann in annotations)\nfor img_id in tqdm(image_ids):\n    current_img_captions = []\n    for ann_img_id, num, caption in annotations:\n        if img_id == ann_img_id:\n            current_img_captions.append((num, caption))\n\n    # Sort by the annotation number\n    current_img_captions.sort(key=lambda x: x[0])\n    captions.append((img_id, tuple([x[1] for x in current_img_captions])))\n</code></pre>"},{"location":"examples/python/flickr8k_dataset_creation/#converting-to-a-lance-dataset","title":"Converting to a Lance Dataset","text":"<p>Now that our captions list is in a proper format, we will write a <code>process()</code> function that will take the said captions as argument and yield a Pyarrow record batch consisting of the <code>image_id</code>, <code>image</code> and <code>captions</code>. The image in this record batch will be in binary format and all the captions for an image will be in a list with their ordering preserved.</p> <pre><code>def process(captions):\n    for img_id, img_captions in tqdm(captions):\n        try:\n            with open(os.path.join(image_folder, img_id), 'rb') as im:\n                binary_im = im.read()\n\n        except FileNotFoundError:\n            print(f\"img_id '{img_id}' not found in the folder, skipping.\")\n            continue\n\n        img_id = pa.array([img_id], type=pa.string())\n        img = pa.array([binary_im], type=pa.binary())\n        capt = pa.array([img_captions], pa.list_(pa.string(), -1))\n\n        yield pa.RecordBatch.from_arrays(\n            [img_id, img, capt], \n            [\"image_id\", \"image\", \"captions\"]\n        )\n</code></pre> <p>Let's also define the same schema to tell Pyarrow the type of data it should be expecting in the table.</p> <pre><code>schema = pa.schema([\n    pa.field(\"image_id\", pa.string()),\n    pa.field(\"image\", pa.binary()),\n    pa.field(\"captions\", pa.list_(pa.string(), -1)),\n])\n</code></pre> <p>We are including the <code>image_id</code> (which is the original image name) so it can be easier to reference and debug in the future.</p> <p>Finally, we define a reader to iteratively read those record batches and then write them to a lance dataset on the disk.</p> <pre><code>reader = pa.RecordBatchReader.from_batches(schema, process(captions))\nlance.write_dataset(reader, \"flickr8k.lance\", schema)\n</code></pre> <p>And that's basically it! If you want to execute this in a notebook form, you can check out this example in our deeplearning-recipes repository here.</p> <p>For more Deep learning related examples using Lance dataset, be sure to check out the lance-deeplearning-recipes repository! </p>"},{"location":"examples/python/llm_dataset_creation/","title":"Creating text dataset for LLM training using Lance","text":"<p>Lance can be used for creating and caching a text (or code) dataset for pre-training / fine-tuning of Large Language Models. The need for this arises when one needs to train a model on a subset of data or process the data in chunks without downloading all of it on the disk at once. This becomes a considerable problem when you just want a subset of a Terabyte or Petabyte-scale dataset.</p> <p>In this example, we will be bypassing this problem by downloading a text dataset in parts, tokenizing it and saving it as a Lance dataset.  This can be done for as many or as few data samples as you wish with average memory consumption approximately 3-4 GBs!</p> <p>For this example, we are working with the wikitext dataset, which is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia.</p>"},{"location":"examples/python/llm_dataset_creation/#preparing-and-pre-processing-the-raw-dataset","title":"Preparing and pre-processing the raw dataset","text":"<p>Let's first define the dataset and the tokenizer</p> <pre><code>import lance\nimport pyarrow as pa\n\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\nfrom tqdm.auto import tqdm  # optional for progress tracking\n\ntokenizer = AutoTokenizer.from_pretrained('gpt2')\n\ndataset = load_dataset('wikitext', 'wikitext-103-raw-v1', streaming=True)['train']\ndataset = dataset.shuffle(seed=1337)\n</code></pre> <p>The <code>streaming</code> argument in <code>load_dataset</code> is especially important because if you run it without setting it to  <code>True</code>, the datasets library will download the entire dataset first, even though you only wish to use a subset of it. With <code>streaming</code> set to <code>True</code>, the samples will be downloaded as they are needed.</p> <p>Now we will define a function to help us with tokenizing our samples, one-by-one.</p> <pre><code>def tokenize(sample, field='text'):\n    return tokenizer(sample[field])['input_ids']\n</code></pre> <p>This function will receive a sample from a huggingface dataset and tokenize the values in the <code>field</code> column. This is the main text you want  to tokenize.</p>"},{"location":"examples/python/llm_dataset_creation/#creating-a-lance-dataset","title":"Creating a Lance dataset","text":"<p>Now that we have set up our raw dataset and pre-processing code,  let's define the main function that takes in the dataset, number of samples and field, and returns a pyarrow batch that will later be written into a lance dataset.</p> <pre><code>def process_samples(dataset, num_samples=100_000, field='text'):\n    current_sample = 0\n    for sample in tqdm(dataset, total=num_samples):\n        # If we have added all 5M samples, stop\n        if current_sample == num_samples:\n            break\n        if not sample[field]:\n            continue\n        # Tokenize the current sample\n        tokenized_sample = tokenize(sample, field)\n        # Increment the counter\n        current_sample += 1\n        # Yield a PyArrow RecordBatch\n        yield pa.RecordBatch.from_arrays(\n            [tokenized_sample], \n            names=[\"input_ids\"]\n        )\n</code></pre> <p>This function will be iterating over the huggingface dataset, one sample at a time, tokenizing the sample and yielding a pyarrow <code>RecordBatch</code> with all the tokens. We will do this until we have reached the <code>num_samples</code> number of samples or the end of the dataset, whichever comes first.</p> <p>Please note that by 'sample', we mean one example (row) in the original dataset. What one example exactly means will depend on the dataset itself as it could  be one line or an entire file of text. In this example, it varies in length between a line and a paragraph of text.</p> <p>We also need to define a schema to tell Lance what type of data we are expecting in our table. Since our dataset consists only of tokens which are long integers, <code>int64</code> is the suitable datatype.</p> <pre><code>schema = pa.schema([\n    pa.field(\"input_ids\", pa.int64())\n])\n</code></pre> <p>Finally, we need to define a <code>reader</code> that will be reading a stream of record batches from our <code>process_samples</code> function that yields  said record batches consisting of individual tokenized samples.</p> <pre><code>reader = pa.RecordBatchReader.from_batches(\n    schema, \n    process_samples(dataset, num_samples=500_000, field='text') # For 500K samples\n)\n</code></pre> <p>And finally we use the <code>lance.write_dataset</code> which will write the dataset to the disk.</p> <pre><code># Write the dataset to disk\nlance.write_dataset(\n    reader, \n    \"wikitext_500K.lance\",\n    schema\n)\n</code></pre> <p>If you want to apply some other pre-processing to the tokens before saving it to the disk (like masking, etc), you may add it in the  <code>process_samples</code> function.</p> <p>And that's it! Your dataset has been tokenized and saved to the disk! </p>"},{"location":"examples/python/llm_training/","title":"Training LLMs using a Lance text dataset","text":"<p>Using a Lance text dataset for pre-training / fine-tuning a Large Language model is straightforward and memory-efficient. This example follows up on the Creating text dataset for LLM training using Lance example. Check it out if you haven't already.</p> <p>In this example, we will be training an LLM using \ud83e\udd17 transformers on the tokenized \"wikitext_500K\" lance dataset we created in the aforementioned example.</p>"},{"location":"examples/python/llm_training/#imports-and-setup","title":"Imports and Setup","text":"<p>Let's setup our environment by doing all the necessary imports and defining a few basic things.</p> <pre><code>import numpy as np\nimport lance\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, Sampler\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom tqdm.auto import tqdm\n\n# We'll be training the pre-trained GPT2 model in this example\nmodel_name = 'gpt2'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n# Also define some hyperparameters\nlr = 3e-4\nnb_epochs = 10\nblock_size = 1024\nbatch_size = 8\ndevice = 'cuda:0'\ndataset_path = 'wikitext_500K.lance'\n</code></pre> <p>Now that the basic setup is out of the way, let's define our custom Dataset and a Sampler for streaming the tokens from our Lance dataset.</p>"},{"location":"examples/python/llm_training/#data-loading-setup","title":"Data-loading Setup","text":"<p>We start by defining a utility function that will help us load any number of tokens from our lance dataset in a 'chunk'.</p> <pre><code>def from_indices(dataset, indices):\n    \"\"\"Load the elements on given indices from the dataset\"\"\"\n    chunk = dataset.take(indices).to_pylist()\n    chunk = list(map(lambda x: x['input_ids'], chunk))\n    return chunk\n</code></pre> <p>Now let's define our custom dataset and sampler for loading the tokens.</p> <pre><code>class LanceDataset(Dataset):\n    def __init__(\n        self,\n        dataset_path,\n        block_size,\n    ):\n        # Load the lance dataset from the saved path\n        self.ds = lance.dataset(dataset_path)\n        self.block_size = block_size\n\n        # Doing this so the sampler never asks for an index at the end of text\n        self.length = self.ds.count_rows() - block_size\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Generate a window of indices starting from the current idx to idx+block_size\n        and return the tokens at those indices\n        \"\"\"\n        window = np.arange(idx, idx + self.block_size)\n        sample = from_indices(self.ds, window)\n\n        return {\"input_ids\": torch.tensor(sample), \"labels\": torch.tensor(sample)}\n</code></pre> <p>When given a random index by the sampler, the dataset will load the next <code>block_size</code> number of tokens starting from current index. This would in-essence form a sample as the loaded tokens would be causal.</p> <p>However we also need to make sure that the tokens we get from the dataset aren't overlapping. Let's understand this from an example:</p> <p>Let's say, for some arbitrary block size, during the training loop the dataset return the following tokens:</p> <p><code>\"Vienna is the capital of Austria\"</code> at index = 12 for sample #1, and,</p> <p><code>\"is the capital of Austria and\"</code> at index = 13 for sample #2, and so on</p> <p>The problem here is that if we allow the dataloader to fetch the 'samples' for any arbitrary number of indices, they may overlap (as we see above). This is not good for the model as it may start to overfit after seeing sufficient overlapping tokens.</p> <p>To solve this problem, we define a custom Sampler that only returns the indices that are 'block_size' apart from each other, ensuring that we don't see any overlapping samples.</p> <pre><code>class LanceSampler(Sampler):\n    r\"\"\"Samples tokens randomly but `block_size` indices apart.\n\n    Args:\n        data_source (Dataset): dataset to sample from\n        block_size (int): minimum index distance between each random sample\n    \"\"\"\n\n    def __init__(self, data_source, block_size=512):\n        self.data_source = data_source\n        self.num_samples = len(self.data_source)\n        self.available_indices = list(range(0, self.num_samples, block_size))\n        np.random.shuffle(self.available_indices)\n\n    def __iter__(self):\n        yield from self.available_indices\n\n    def __len__(self) -&gt; int:\n        return len(self.available_indices)\n</code></pre> <p>Now when we fetch the tokens from our dataset with sampler being the <code>LanceSampler</code>, all samples in all the batches that our model sees during the training are guaranteed to be non-overlapping.</p> <p>This is done by generating a list of indices starting from 0 to the end of the dataset (which if you remember is lance dataset length - block size) with each index 'block_size' apart from the other. We then shuffle this list and yield indices from it.</p> <p>And that's basically it for the Dataloading! Now all we are left is to train the model!</p>"},{"location":"examples/python/llm_training/#model-training","title":"Model Training","text":"<p>Now you train the model just like you would with any other dataset!</p> <pre><code># Define the dataset, sampler and dataloader\ndataset = LanceDataset(dataset_path, block_size)\nsampler = LanceSampler(dataset, block_size)\ndataloader = DataLoader(\n    dataset,\n    shuffle=False,\n    batch_size=batch_size,\n    sampler=sampler,\n    pin_memory=True\n)\n\n# Define the optimizer, training loop and train the model!\nmodel = model.to(device)\nmodel.train()\noptimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n\nfor epoch in range(nb_epochs):\n    print(f\"========= Epoch: {epoch+1} / {nb_epochs} =========\")\n    epoch_loss = []\n    prog_bar = tqdm(dataloader, total=len(dataloader))\n    for batch in prog_bar:\n        optimizer.zero_grad(set_to_none=True)\n\n        # Put both input_ids and labels to the device\n        for k, v in batch.items():\n            batch[k] = v.to(device)\n\n        # Perform one forward pass and get the loss\n        outputs = model(**batch)\n        loss = outputs.loss\n\n        # Perform backward pass\n        loss.backward()\n        optimizer.step()\n\n        prog_bar.set_description(f\"loss: {loss.item():.4f}\")\n\n        epoch_loss.append(loss.item())\n\n    # Calculate training perplexity for this epoch\n    try:\n        perplexity = np.exp(np.mean(epoch_loss))\n    except OverflowError:\n        perplexity = float(\"-inf\")\n\n    print(f\"train_perplexity: {perplexity}\")\n</code></pre> <p>One tip: If your lance dataset is huge (like the wikitext_500K is), and you want to debug the model to look out for errors, you may want to wrap the dataloader in an <code>iter()</code> function and only run it for a couple batches.</p> <p>And that's basically it!</p> <p>The best part about using Lance, the custom Dataset and Sampler is that you get a whopping 95% average GPU utilisation and minimal CPU overhead thanks to the lightning fast random access that Lance provides \ud83d\ude80 </p>"},{"location":"examples/rust/hnsw/","title":"Indexing a dataset with HNSW (Hierarchical Navigable Small World)","text":"<p>HNSW is a graph based algorithm for approximate neighbor search in high-dimensional spaces. In this example, we will demonstrate how to build an HNSW vector index against a Lance dataset.</p> <p>This example will show how to:</p> <ol> <li>Generate synthetic test data of specified dimensions</li> <li>Build a hierarchical graph structure for efficient vector search using Lance API</li> <li>Perform vector search with different parameters and compute the ground truth using L2 distance search</li> </ol>"},{"location":"examples/rust/hnsw/#complete-example","title":"Complete Example","text":"<pre><code>use std::collections::HashSet;\nuse std::sync::Arc;\n\nuse arrow::array::{types::Float32Type, Array, FixedSizeListArray};\nuse arrow::array::{AsArray, FixedSizeListBuilder, Float32Builder};\nuse arrow::datatypes::{DataType, Field, Schema};\nuse arrow::record_batch::RecordBatch;\nuse arrow::record_batch::RecordBatchIterator;\nuse arrow_select::concat::concat;\nuse futures::stream::StreamExt;\nuse lance::Dataset;\nuse lance_index::vector::v3::subindex::IvfSubIndex;\nuse lance_index::vector::{\n    flat::storage::FlatFloatStorage,\n    hnsw::{builder::HnswBuildParams, HNSW},\n};\nuse lance_linalg::distance::DistanceType;\n\nfn ground_truth(fsl: &amp;FixedSizeListArray, query: &amp;[f32], k: usize) -&gt; HashSet&lt;u32&gt; {\n    let mut dists = vec![];\n    for i in 0..fsl.len() {\n        let dist = lance_linalg::distance::l2_distance(\n            query,\n            fsl.value(i).as_primitive::&lt;Float32Type&gt;().values(),\n        );\n        dists.push((dist, i as u32));\n    }\n    dists.sort_by(|a, b| a.0.partial_cmp(&amp;b.0).unwrap());\n    dists.truncate(k);\n    dists.into_iter().map(|(_, i)| i).collect()\n}\n\npub async fn create_test_vector_dataset(output: &amp;str, num_rows: usize, dim: i32) {\n    let schema = Arc::new(Schema::new(vec![Field::new(\n        \"vector\",\n        DataType::FixedSizeList(Arc::new(Field::new(\"item\", DataType::Float32, true)), dim),\n        false,\n    )]));\n\n    let mut batches = Vec::new();\n\n    // Create a few batches\n    for _ in 0..2 {\n        let v_builder = Float32Builder::new();\n        let mut list_builder = FixedSizeListBuilder::new(v_builder, dim);\n\n        for _ in 0..num_rows {\n            for _ in 0..dim {\n                list_builder.values().append_value(rand::random::&lt;f32&gt;());\n            }\n            list_builder.append(true);\n        }\n        let array = Arc::new(list_builder.finish());\n        let batch = RecordBatch::try_new(schema.clone(), vec![array]).unwrap();\n        batches.push(batch);\n    }\n    let batch_reader = RecordBatchIterator::new(batches.into_iter().map(Ok), schema.clone());\n    println!(\"Writing dataset to {}\", output);\n    Dataset::write(batch_reader, output, None).await.unwrap();\n}\n\n#[tokio::main]\nasync fn main() {\n    let uri: Option&lt;String&gt; = None; // None means generate test data\n    let column = \"vector\";\n    let ef = 100;\n    let max_edges = 30;\n    let max_level = 7;\n\n    // 1. Generate a synthetic test data of specified dimensions\n    let dataset = if uri.is_none() {\n        println!(\"No uri is provided, generating test dataset...\");\n        let output = \"test_vectors.lance\";\n        create_test_vector_dataset(output, 1000, 64).await;\n        Dataset::open(output).await.expect(\"Failed to open dataset\")\n    } else {\n        Dataset::open(uri.as_ref().unwrap())\n            .await\n            .expect(\"Failed to open dataset\")\n    };\n\n    println!(\"Dataset schema: {:#?}\", dataset.schema());\n    let batches = dataset\n        .scan()\n        .project(&amp;[column])\n        .unwrap()\n        .try_into_stream()\n        .await\n        .unwrap()\n        .then(|batch| async move { batch.unwrap().column_by_name(column).unwrap().clone() })\n        .collect::&lt;Vec&lt;_&gt;&gt;()\n        .await;\n    let arrs = batches.iter().map(|b| b.as_ref()).collect::&lt;Vec&lt;_&gt;&gt;();\n    let fsl = concat(&amp;arrs).unwrap().as_fixed_size_list().clone();\n    println!(\"Loaded {:?} batches\", fsl.len());\n\n    let vector_store = Arc::new(FlatFloatStorage::new(fsl.clone(), DistanceType::L2));\n\n    let q = fsl.value(0);\n    let k = 10;\n    let gt = ground_truth(&amp;fsl, q.as_primitive::&lt;Float32Type&gt;().values(), k);\n\n    for ef_construction in [15, 30, 50] {\n        let now = std::time::Instant::now();\n        // 2. Build a hierarchical graph structure for efficient vector search using Lance API\n        let hnsw = HNSW::index_vectors(\n            vector_store.as_ref(),\n            HnswBuildParams::default()\n                .max_level(max_level)\n                .num_edges(max_edges)\n                .ef_construction(ef_construction),\n        )\n        .unwrap();\n        let construct_time = now.elapsed().as_secs_f32();\n        let now = std::time::Instant::now();\n        // 3. Perform vector search with different parameters and compute the ground truth using L2 distance search\n        let results: HashSet&lt;u32&gt; = hnsw\n            .search_basic(q.clone(), k, ef, None, vector_store.as_ref())\n            .unwrap()\n            .iter()\n            .map(|node| node.id)\n            .collect();\n        let search_time = now.elapsed().as_micros();\n        println!(\n            \"level={}, ef_construct={}, ef={} recall={}: construct={:.3}s search={:.3} us\",\n            max_level,\n            ef_construction,\n            ef,\n            results.intersection(&amp;gt).count() as f32 / k as f32,\n            construct_time,\n            search_time\n        );\n    }\n}\n</code></pre>"},{"location":"examples/rust/llm_dataset_creation/","title":"Creating text dataset for LLM training using Lance in Rust","text":"<p>In this example, we will demonstrate how to achieve the Python example - LLM dataset creation shown in the Python examples in Rust.</p> <p>Note</p> <p>The huggingface Python API supports loading data in streaming mode and shuffling is provided as a builtin feature. Rust API lacks these feature thus the data are manually downloaded and shuffled within each batch.</p> <p>This example will show how to:</p> <ol> <li>Download and process a text dataset in parts from huggingface</li> <li>Tokenize the text data with a custom RecordBatchReader</li> <li>Save it as a Lance dataset using Lance API</li> </ol> <p>The implementation details in Rust will follow similar concepts as the Python version, but with Rust-specific APIs and patterns which are significantly more verbose.</p>"},{"location":"examples/rust/llm_dataset_creation/#complete-example","title":"Complete Example","text":"<pre><code>use arrow::array::{Array, Int64Builder, ListBuilder, UInt32Array};\nuse arrow::datatypes::{DataType, Field, Schema};\nuse arrow::record_batch::RecordBatch;\nuse arrow::record_batch::RecordBatchReader;\nuse futures::StreamExt;\nuse hf_hub::{api::sync::Api, Repo, RepoType};\nuse lance::dataset::WriteParams;\nuse parquet::arrow::arrow_reader::ParquetRecordBatchReaderBuilder;\nuse rand::seq::SliceRandom;\nuse rand::SeedableRng;\nuse std::error::Error;\nuse std::fs::File;\nuse std::io::Write;\nuse std::sync::Arc;\nuse tempfile::NamedTempFile;\nuse tokenizers::Tokenizer;\n\n// Implement a custom stream batch reader\nstruct WikiTextBatchReader {\n    schema: Arc&lt;Schema&gt;,\n    parquet_readers: Vec&lt;Option&lt;ParquetRecordBatchReaderBuilder&lt;File&gt;&gt;&gt;,\n    current_reader_idx: usize,\n    current_reader: Option&lt;Box&lt;dyn RecordBatchReader + Send&gt;&gt;,\n    tokenizer: Tokenizer,\n    num_samples: u64,\n    cur_samples_cnt: u64,\n}\n\nimpl WikiTextBatchReader {\n    fn new(\n        parquet_readers: Vec&lt;ParquetRecordBatchReaderBuilder&lt;File&gt;&gt;,\n        tokenizer: Tokenizer,\n        num_samples: Option&lt;u64&gt;,\n    ) -&gt; Result&lt;Self, Box&lt;dyn Error + Send + Sync&gt;&gt; {\n        let schema = Arc::new(Schema::new(vec![Field::new(\n            \"input_ids\",\n            DataType::List(Arc::new(Field::new(\"item\", DataType::Int64, true))),\n            false,\n        )]));\n\n        Ok(Self {\n            schema,\n            parquet_readers: parquet_readers.into_iter().map(Some).collect(),\n            current_reader_idx: 0,\n            current_reader: None,\n            tokenizer,\n            num_samples: num_samples.unwrap_or(100_000),\n            cur_samples_cnt: 0,\n        })\n    }\n\n    fn process_batch(\n        &amp;mut self,\n        input_batch: &amp;RecordBatch,\n    ) -&gt; Result&lt;RecordBatch, arrow::error::ArrowError&gt; {\n        let num_rows = input_batch.num_rows();\n        let mut token_builder = ListBuilder::new(Int64Builder::with_capacity(num_rows * 1024)); // Pre-allocate space\n        let mut should_break = false;\n\n        let column = input_batch.column_by_name(\"text\").unwrap();\n        let string_array = column\n            .as_any()\n            .downcast_ref::&lt;arrow::array::StringArray&gt;()\n            .unwrap();\n        for i in 0..num_rows {\n            if self.cur_samples_cnt &gt;= self.num_samples {\n                should_break = true;\n                break;\n            }\n            if !Array::is_null(string_array, i) {\n                let text = string_array.value(i);\n                // Split paragraph into lines\n                for line in text.split('\\n') {\n                    if let Ok(encoding) = self.tokenizer.encode(line, true) {\n                        let tb_values = token_builder.values();\n                        for &amp;id in encoding.get_ids() {\n                            tb_values.append_value(id as i64);\n                        }\n                        token_builder.append(true);\n                        self.cur_samples_cnt += 1;\n                        if self.cur_samples_cnt % 5000 == 0 {\n                            println!(\"Processed {} rows\", self.cur_samples_cnt);\n                        }\n                        if self.cur_samples_cnt &gt;= self.num_samples {\n                            should_break = true;\n                            break;\n                        }\n                    }\n                }\n            }\n        }\n\n        // Create array and shuffle it\n        let input_ids_array = token_builder.finish();\n\n        // Create shuffled array by randomly sampling indices\n        let mut rng = rand::rngs::StdRng::seed_from_u64(1337);\n        let len = input_ids_array.len();\n        let mut indices: Vec&lt;u32&gt; = (0..len as u32).collect();\n        indices.shuffle(&amp;mut rng);\n\n        // Take values in shuffled order\n        let indices_array = UInt32Array::from(indices);\n        let shuffled = arrow::compute::take(&amp;input_ids_array, &amp;indices_array, None)?;\n\n        let batch = RecordBatch::try_new(self.schema.clone(), vec![Arc::new(shuffled)]);\n        if should_break {\n            println!(\"Stop at {} rows\", self.cur_samples_cnt);\n            self.parquet_readers.clear();\n            self.current_reader = None;\n        }\n\n        batch\n    }\n}\n\nimpl RecordBatchReader for WikiTextBatchReader {\n    fn schema(&amp;self) -&gt; Arc&lt;Schema&gt; {\n        self.schema.clone()\n    }\n}\n\nimpl Iterator for WikiTextBatchReader {\n    type Item = Result&lt;RecordBatch, arrow::error::ArrowError&gt;;\n    fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; {\n        loop {\n            // If we have a current reader, try to get next batch\n            if let Some(reader) = &amp;mut self.current_reader {\n                if let Some(batch_result) = reader.next() {\n                    return Some(batch_result.and_then(|batch| self.process_batch(&amp;batch)));\n                }\n            }\n\n            // If no current reader or current reader is exhausted, try to get next reader\n            if self.current_reader_idx &lt; self.parquet_readers.len() {\n                if let Some(builder) = self.parquet_readers[self.current_reader_idx].take() {\n                    match builder.build() {\n                        Ok(reader) =&gt; {\n                            self.current_reader = Some(Box::new(reader));\n                            self.current_reader_idx += 1;\n                            continue;\n                        }\n                        Err(e) =&gt; {\n                            return Some(Err(arrow::error::ArrowError::ExternalError(Box::new(e))))\n                        }\n                    }\n                }\n            }\n\n            // No more readers available\n            return None;\n        }\n    }\n}\n\nfn main() -&gt; Result&lt;(), Box&lt;dyn Error + Send + Sync&gt;&gt; {\n    let rt = tokio::runtime::Runtime::new()?;\n    rt.block_on(async {\n        // Load tokenizer\n        let tokenizer = load_tokenizer(\"gpt2\")?;\n\n        // Set up Hugging Face API\n        // Download from https://huggingface.co/datasets/Salesforce/wikitext/tree/main/wikitext-103-raw-v1\n        let api = Api::new()?;\n        let repo = api.repo(Repo::with_revision(\n            \"Salesforce/wikitext\".into(),\n            RepoType::Dataset,\n            \"main\".into(),\n        ));\n\n        // Define the parquet files we want to download\n        let train_files = vec![\n            \"wikitext-103-raw-v1/train-00000-of-00002.parquet\",\n            \"wikitext-103-raw-v1/train-00001-of-00002.parquet\",\n        ];\n\n        let mut parquet_readers = Vec::new();\n        for file in &amp;train_files {\n            println!(\"Downloading file: {}\", file);\n            let file_path = repo.get(file)?;\n            let data = std::fs::read(file_path)?;\n\n            // Create a temporary file in the system temp directory and write the downloaded data to it\n            let mut temp_file = NamedTempFile::new()?;\n            temp_file.write_all(&amp;data)?;\n\n            // Create the parquet reader builder with a larger batch size\n            let builder = ParquetRecordBatchReaderBuilder::try_new(temp_file.into_file())?\n                .with_batch_size(8192); // Increase batch size for better performance\n            parquet_readers.push(builder);\n        }\n\n        if parquet_readers.is_empty() {\n            println!(\"No parquet files found to process.\");\n            return Ok(());\n        }\n\n        // Create batch reader\n        let num_samples: u64 = 500_000;\n        let batch_reader = WikiTextBatchReader::new(parquet_readers, tokenizer, Some(num_samples))?;\n\n        // Save as Lance dataset\n        println!(\"Writing to Lance dataset...\");\n        let lance_dataset_path = \"rust_wikitext_lance_dataset.lance\";\n\n        let write_params = WriteParams::default();\n        lance::Dataset::write(batch_reader, lance_dataset_path, Some(write_params)).await?;\n\n        // Verify the dataset\n        let ds = lance::Dataset::open(lance_dataset_path).await?;\n        let scanner = ds.scan();\n        let mut stream = scanner.try_into_stream().await?;\n\n        let mut total_rows = 0;\n        while let Some(batch_result) = stream.next().await {\n            let batch = batch_result?;\n            total_rows += batch.num_rows();\n        }\n\n        println!(\n            \"Lance dataset created successfully with {} rows\",\n            total_rows\n        );\n        println!(\"Dataset location: {}\", lance_dataset_path);\n\n        Ok(())\n    })\n}\n\nfn load_tokenizer(model_name: &amp;str) -&gt; Result&lt;Tokenizer, Box&lt;dyn Error + Send + Sync&gt;&gt; {\n    let api = Api::new()?;\n    let repo = api.repo(Repo::with_revision(\n        model_name.into(),\n        RepoType::Model,\n        \"main\".into(),\n    ));\n\n    let tokenizer_path = repo.get(\"tokenizer.json\")?;\n    let tokenizer = Tokenizer::from_file(tokenizer_path)?;\n\n    Ok(tokenizer)\n}\n</code></pre>"},{"location":"examples/rust/write_read_dataset/","title":"Writing and reading a dataset using Lance","text":"<p>In this example, we will write a simple lance dataset to disk. Then we will read it and print out some basic properties like the schema and sizes for each record batch in the dataset. The example uses only one record batch, however it should work for larger datasets (multiple record batches) as well.</p>"},{"location":"examples/rust/write_read_dataset/#writing-the-raw-dataset","title":"Writing the raw dataset","text":"<pre><code>// Writes sample dataset to the given path\nasync fn write_dataset(data_path: &amp;str) {\n    // Define new schema\n    let schema = Arc::new(Schema::new(vec![\n        Field::new(\"key\", DataType::UInt32, false),\n        Field::new(\"value\", DataType::UInt32, false),\n    ]));\n\n    // Create new record batches\n    let batch = RecordBatch::try_new(\n        schema.clone(),\n        vec![\n            Arc::new(UInt32Array::from(vec![1, 2, 3, 4, 5, 6])),\n            Arc::new(UInt32Array::from(vec![6, 7, 8, 9, 10, 11])),\n        ],\n    )\n    .unwrap();\n\n    let batches = RecordBatchIterator::new([Ok(batch)], schema.clone());\n\n    // Define write parameters (e.g. overwrite dataset)\n    let write_params = WriteParams {\n        mode: WriteMode::Overwrite,\n        ..Default::default()\n    };\n\n    Dataset::write(batches, data_path, Some(write_params))\n        .await\n        .unwrap();\n} // End write dataset\n</code></pre> <p>First we define a schema for our dataset, and create a record batch from that schema. Next we iterate over the record batches (only one in this case) and write them to disk. We also define the write parameters (set to overwrite) and then write the dataset to disk.</p>"},{"location":"examples/rust/write_read_dataset/#reading-a-lance-dataset","title":"Reading a Lance dataset","text":"<p>Now that we have written the dataset to a new directory, we can read it back and print out some basic properties.</p> <pre><code>// Reads dataset from the given path and prints batch size, schema for all record batches. Also extracts and prints a slice from the first batch\nasync fn read_dataset(data_path: &amp;str) {\n    let dataset = Dataset::open(data_path).await.unwrap();\n    let scanner = dataset.scan();\n\n    let mut batch_stream = scanner.try_into_stream().await.unwrap().map(|b| b.unwrap());\n\n    while let Some(batch) = batch_stream.next().await {\n        println!(\"Batch size: {}, {}\", batch.num_rows(), batch.num_columns()); // print size of batch\n        println!(\"Schema: {:?}\", batch.schema()); // print schema of recordbatch\n\n        println!(\"Batch: {:?}\", batch); // print the entire recordbatch (schema and data)\n    }\n} // End read dataset\n</code></pre> <p>First we open the dataset, and create a scanner object. We use it to create a <code>batch_stream</code> that will let us access each record batch in the dataset. Then we iterate over the record batches and print out the size and schema of each one.</p>"},{"location":"examples/rust/write_read_dataset/#complete-example","title":"Complete Example","text":"<pre><code>use arrow::array::UInt32Array;\nuse arrow::datatypes::{DataType, Field, Schema};\nuse arrow::record_batch::{RecordBatch, RecordBatchIterator};\nuse futures::StreamExt;\nuse lance::dataset::{WriteMode, WriteParams};\nuse lance::Dataset;\nuse std::sync::Arc;\n\n#[tokio::main]\nasync fn main() {\n    let data_path: &amp;str = \"./temp_data.lance\";\n\n    write_dataset(data_path).await;\n    read_dataset(data_path).await;\n}\n</code></pre>"},{"location":"format/","title":"Lance Format Specification","text":"<p>Lance is a Lakehouse Format that spans three specification layers: file format, table format, and catalog spec.</p>"},{"location":"format/#understanding-the-lakehouse-stack","title":"Understanding the Lakehouse Stack","text":"<p>To understand where Lance fits in the data ecosystem, let's first map out the complete lakehouse technology stack. The modern lakehouse architecture consists of six distinct layers:</p> <p></p>"},{"location":"format/#1-object-store","title":"1. Object Store","text":"<p>At the foundation lies the object store\u2014storage systems characterized by their object-based simple hierarchy,  typically providing highly durable guarantees with HTTP-based communication protocols for data transfer. This includes systems like S3, GCS, and Azure Blob Storage.</p>"},{"location":"format/#2-file-format","title":"2. File Format","text":"<p>Above the storage layer, the file format describes how a single file should be stored on disk. This is where formats like Apache Parquet operate, defining the internal structure, encoding, and compression of individual data files.</p>"},{"location":"format/#3-table-format","title":"3. Table Format","text":"<p>The table format layer describes how multiple files work together to form a logical table. The key feature that modern table formats enable is transactional commits and read isolation to allow multiple writers and readers to safely operate against the same table. All major open source table formats including Iceberg and Lance implement these features through MVCC (Multi-Version Concurrency Control),  where each commit atomically produces a new table version, and all table versions form a serializable history for the specific table. This also unlocks features like time travel and makes features like schema evolution easy to develop.</p>"},{"location":"format/#4-catalog-spec","title":"4. Catalog Spec","text":"<p>The catalog spec defines how any system can discover and manage a collection of tables within storage. This is where the lower storage and format stack meets the upper service and compute stack.</p> <p>Table formats require at least a way to list all available tables and to describe, add and drop tables in the list. This is necessary for actually building the so-called connectors in compute engines so they can discover and start working on the table according to the format. Historically, Hive has defined the Hive MetaStore spec that is sufficient for most table formats including Delta Lake, Hudi, Paimon, and also Lance. Iceberg offers its unique Iceberg REST Catalog spec.</p> <p>From the top down, projects like Apache Polaris, Unity Catalog, and Apache Gravitino usually offer additional specification for operating against table derivatives  (e.g. views, materialized views, user-defined table functions) and objects used in table operations (e.g. user-defined functions, policies).</p> <p>This intersection between top and bottom stack is also why typically a catalog service would provide both the catalog specifications offered by the format side for easy connectivity to compute engines,  as well as providing their own APIs for extended management features.</p> <p>Another key differentiation of a catalog spec versus a catalog service is that there can be multiple different vendors implementing the same spec. For example, for Polaris REST spec we have open source Apache Polaris server, Snowflake Horizon Catalog, and Polaris-compatible services in AWS Glue, Azure OneLake, etc.</p>"},{"location":"format/#5-catalog-service","title":"5. Catalog Service","text":"<p>A catalog service implements one or more catalog specifications to provide both table metadata and optionally continuous background maintenance (compaction, optimization, index updates) that table formats require to stay performant. Catalog services typically implement multiple specifications to support different table formats. For example, Polaris, Unity and Gravitino all support the Iceberg REST catalog specification for Iceberg tables, and have their own generic table API for other table formats.</p> <p>Since table formats are static specifications, catalog services supply the active operational work needed for production deployments. This is often where open source transitions to commercial offerings, as open source projects typically provide metadata functionality, while commercial solutions offer the full operational experience including automated maintenance. There are also open source solutions like Apache Amoro emerging to fill this gap with complete open source catalog service implementations that offer both table metadata access and continuous optimization.</p>"},{"location":"format/#6-compute-engine","title":"6. Compute Engine","text":"<p>Finally, compute engines are the workhorses that visit catalog services and leverage their knowledge of file formats, table formats, and catalog specifications to perform complex data workflows, including SQL queries, analytics processing, vector search, full-text search, and machine learning training. All sorts of applications can be built on top of compute engines to serve more concrete analytics, ML and AI use cases.</p>"},{"location":"format/#the-overall-lakehouse-architecture","title":"The Overall Lakehouse Architecture","text":"<p>In the lakehouse architecture, compute power resides in the object store, catalog services, and compute engines. The middle three layers (file format, table format, catalog spec) are specifications without compute. This separation enables portability and interoperability.</p>"},{"location":"format/#understanding-lance-as-a-lakehouse-format","title":"Understanding Lance as a Lakehouse Format","text":"<p>Lance spans all three specification layers:</p> <ol> <li>File Format: The Lance columnar file format, read specification \u2192</li> <li>Table Format: The Lance table format, read specification \u2192</li> <li>Catalog Spec: The Lance Namespace specification, read specification \u2192</li> </ol> <p>For comparison:</p> <ul> <li>Apache Iceberg operates at the table format and catalog spec layers, using Apache Parquet, Apache Avro and Apache ORC as the file format</li> <li>Delta Lake and Apache Hudi operate at only the table format layer, using Apache Parquet as the file format</li> </ul>"},{"location":"format/file/","title":"Lance File Format","text":""},{"location":"format/file/#file-structure","title":"File Structure","text":"<p>A Lance file is a container for tabular data. The data is stored in \"disk pages\". Each disk page contains some rows for a single column. There may be one or more disk pages per column. Different columns may have different numbers of disk pages. Metadata at the end of the file describes where the pages are located and how the data is encoded.</p> <p></p> <p>Note</p> <p>This page describes the container specification. We also have a set of default encodings that are used to encode data into disk pages. See the Encoding Strategy page for more details.</p>"},{"location":"format/file/#disk-pages","title":"Disk Pages","text":"<p>Disk pages are designed to be large enough to justify a dedicated I/O operation, even on cloud storage, typically several megabytes. Using a larger page size may reduce the number of I/O operations required to read a file, but it also increases the amount of memory required to write the file. In practice, very large page sizes are not useful when high speed reads are required because large contiguous reads need to be broken into smaller reads for performance (particularly on cloud storage). As a result, a default of 8MB is recommended for the page size and should yield ideal performance on all storage systems.</p> <p>Disk pages should not generally be opaque. It is possible to read a portion of a disk page when a subset of the rows are required. However, the specifics of this process depend on the column encoding which is described in a later section.</p>"},{"location":"format/file/#no-row-groups","title":"No Row Groups","text":"<p>Unlike similar formats, there is no \"row group\" concept, only pages. We believe the concept of row groups to be fundamentally harmful to performance. If the row group size is too small then columns will be split into \"runt pages\" which yield poor read performance on cloud storage. If the row group size is too large then a file writer will need a large amount of RAM since an entire row group must be buffered in memory before it can be written. Instead, to split a file amongst multiple readers we rely on the fact that partial page reads are possible and have minimal read amplification. As a result, you can split the file at whatever row boundary you want.</p>"},{"location":"format/file/#buffer-alignment","title":"Buffer Alignment","text":"<p>The file format does not require that buffers be contiguous as buffers are referenced by absolute offsets. In practice, we always align buffers to 64 byte boundaries.</p>"},{"location":"format/file/#external-buffers","title":"External Buffers","text":"<p>Every page in the file is referenced by an absolute offset. This means that non-page data may be inserted amongst the pages. This can be useful for storing extremely large data types which might only fit a few rows per page otherwise. We can instead store the data out-of-line and store the locations in a page.</p> <p>In addition, the file format supports \"global buffers\" which can be used for auxiliary data. This may be used to store a file schema, file indexes, column statistics, or other metadata. References to the global buffers are stored in a special spot in the footer.</p>"},{"location":"format/file/#column-descriptors","title":"Column Descriptors","text":"<p>At the tail of the file is metadata that describes each page in the file, particularly the encoding strategy used. This metadata consists of a series of \"column descriptors\", which are standalone protobuf messages for each column in the file. Since each column has its own message there is no need to read all file metadata if you are only interested in a subset of the columns. However, in many cases, the column descriptors are small enough that it is cheaper to read the entire footer in a single read than split it into multiple reads.</p>"},{"location":"format/file/#offsets-footer","title":"Offsets &amp; Footer","text":"<p>After the column descriptors there are offset arrays for the column descriptors and global buffers. These simply point to the locations of each item. Finally, there is a fixed-size footer which describes the position of the offset arrays and start of the metadata section.</p>"},{"location":"format/file/#identifiers-and-type-systems","title":"Identifiers and Type Systems","text":"<p>This basic container format has no concept of types. These are added later by the encoding layer. All columns are referenced by an integer \"column index\". All global buffers are referenced by an integer \"global buffer index\". The schema is typically stored in the global buffers, but the file format is unaware of this.</p>"},{"location":"format/file/#reading-strategy","title":"Reading Strategy","text":"<p>The file metadata will need to be known before reading the data. A simple approach for loading the footer is to read one sector from the end (sector depends on the filesystem, 4KiB for local disk, larger for cloud storage). Then parse the footer and read the rest of the metadata (at this point the size will be known). This requires 1-2 IOPS. By storing the metadata size in some other location (e.g. table manifest) it is possible to always read the footer in a single IOP. If there are many columns in the file and only some are desired then it may be better to read individual columns instead of reading all column metadata, increasing the number of IOPS but decreasing the amount of data read.</p> <p>Next, to read the data, scan through the pages for each column to determine which pages are needed. Each page stores the row offset of the first row in the page. This makes it easy to quickly determine the required pages. The encoding information for the page can then be used to determine exactly which byte ranges are needed from the page.</p> <p>Disk pages should be large enough that there should no significant benefit to sequentially reading the file. However, if such a use case is desired then the file can be read sequentially once the metadata is known, assuming you want to read all columns in the file.</p>"},{"location":"format/file/#detailed-overview","title":"Detailed Overview","text":"<p>A detailed description of the file layout follows:</p> <pre><code>// Note: the number of buffers (BN) is independent of the number of columns (CN)\n//       and pages.\n//\n//       Buffers often need to be aligned.  64-byte alignment is common when\n//       working with SIMD operations.  4096-byte alignment is common when\n//       working with direct I/O.  In order to ensure these buffers are aligned\n//       writers may need to insert padding before the buffers.\n//\n//       If direct I/O is required then most (but not all) fields described\n//       below must be sector aligned.  We have marked these fields with an\n//       asterisk for clarity.  Readers should assume there will be optional\n//       padding inserted before these fields.\n//\n//       All footer fields are unsigned integers written with little endian\n//       byte order.\n//\n// \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n// | Data Pages                       |\n// |   Data Buffer 0*                 |\n// |   ...                            |\n// |   Data Buffer BN*                |\n// \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n// | Column Metadatas                 |\n// | |A| Column 0 Metadata*           |\n// |     Column 1 Metadata*           |\n// |     ...                          |\n// |     Column CN Metadata*          |\n// \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n// | Column Metadata Offset Table     |\n// | |B| Column 0 Metadata Position*  |\n// |     Column 0 Metadata Size       |\n// |     ...                          |\n// |     Column CN Metadata Position  |\n// |     Column CN Metadata Size      |\n// \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n// | Global Buffers Offset Table      |\n// | |C| Global Buffer 0 Position*    |\n// |     Global Buffer 0 Size         |\n// |     ...                          |\n// |     Global Buffer GN Position    |\n// |     Global Buffer GN Size        |\n// \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n// | Footer                           |\n// | A u64: Offset to column meta 0   |\n// | B u64: Offset to CMO table       |\n// | C u64: Offset to GBO table       |\n// |   u32: Number of global bufs     |\n// |   u32: Number of columns         |\n// |   u16: Major version             |\n// |   u16: Minor version             |\n// |   \"LANC\"                         |\n// \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n//\n// File Layout-End\n</code></pre>"},{"location":"format/file/#column-metadata","title":"Column Metadata","text":"<p>The protobuf messages for the column metadata are as follows:</p> <pre><code>message ColumnMetadata {\n\n  // This describes a page of column data.\n  message Page {\n    // The file offsets for each of the page buffers\n    //\n    // The number of buffers is variable and depends on the encoding.  There\n    // may be zero buffers (e.g. constant encoded data) in which case this\n    // could be empty.\n    repeated uint64 buffer_offsets = 1;\n    // The size (in bytes) of each of the page buffers\n    //\n    // This field will have the same length as `buffer_offsets` and\n    // may be empty.\n    repeated uint64 buffer_sizes = 2;\n    // Logical length (e.g. # rows) of the page\n    uint64 length = 3;\n    // The encoding used to encode the page\n    Encoding encoding = 4;\n    // The priority of the page\n    //\n    // For tabular data this will be the top-level row number of the first row\n    // in the page (and top-level rows should not split across pages).\n    uint64 priority = 5;\n  }\n  // Encoding information about the column itself.  This typically describes\n  // how to interpret the column metadata buffers.  For example, it could\n  // describe how statistics or dictionaries are stored in the column metadata.\n  Encoding encoding = 1;\n  // The pages in the column\n  repeated Page pages = 2;   \n  // The file offsets of each of the column metadata buffers\n  //\n  // There may be zero buffers.\n  repeated uint64 buffer_offsets = 3;\n  // The size (in bytes) of each of the column metadata buffers\n  //\n  // This field will have the same length as `buffer_offsets` and\n  // may be empty.\n  repeated uint64 buffer_sizes = 4;\n\n}\n</code></pre>"},{"location":"format/file/encoding/","title":"Lance Encoding Strategy","text":"<p>The encoding strategy determines how array data is encoded into a disk page. The encoding strategy tends to evolve more quickly than the file format itself.</p>"},{"location":"format/file/encoding/#older-encoding-strategies","title":"Older Encoding Strategies","text":"<p>The 0.1 and 2.0 encoding strategies are no longer documented. They were significantly different from future encoding strategies and describing them in detail would be a distraction.</p>"},{"location":"format/file/encoding/#terminology","title":"Terminology","text":"<p>An array is a sequence of values. An array has a data type which describes the semantic interpretation of the values. A layout is a way to encode an array into a set of buffers and child arrays. A buffer is a contiguous sequence of bytes. An encoding describes how the semantic interpretation of data is mapped to the layout. An encoder converts data from one layout to another.</p> <p>Data types and layouts are orthogonal concepts. An integer array might be encoded into two completely different layouts which represent the same data.</p> <p></p>"},{"location":"format/file/encoding/#data-types","title":"Data Types","text":"<p>Lance uses a subset of Arrow's type system for data types. An Arrow data type is both a data type and an encoding. When writing data Lance will often normalize Arrow data types. For example, a string array and a large string array might end up traveling down the same path (variable width data). In fact, most types fall into two general paths. One for fixed-width data and one for variable-width data (where we recognize both 32-bit and 64-bit offsets).</p> <p>At read time, the Arrow data type is used to determine the target encoding. For example, a string array and large string array might both be stored in the same layout but, at read time, we will use the Arrow data type to determine the size of the offsets returned to the user. There is no requirement the output Arrow type matches the input Arrow type. For example, it is acceptable to write an array as \"large string\" and then read it back as \"string\".</p>"},{"location":"format/file/encoding/#search-cache","title":"Search Cache","text":"<p>The search cache is a key component of the Lance file reader. Random access requires that we locate the physical location of the data in the file. To do so we need to know information such as the encoding used for a column, the location of the page, and potentially other information. This information is collectively known as the \"search cache\" and is implemented as a basic LRU cache. We define a \"initialization phase\" which is when we load the various indexing information into the search cache. The cost of initialization is assumed to be amortized over the lifetime of the reader.</p> <p>When performing full scans (i.e. not random access), we should be able to ignore the search cache and sometimes can avoid loading it entirely. We do want to optimize for cold scans as the initialization phase is often not amortized over the lifetime of the reader.</p>"},{"location":"format/file/encoding/#structural-encoding","title":"Structural Encoding","text":"<p>The first step in encoding an array is to determine the structural encoding of the array. A structural encoding breaks the data into smaller units which can be independently decoded. Structural encodings are also responsible for encoding the \"structure\" (struct validity, list validity, list offsets, etc.) typically utilizing repetition levels and definition levels.</p> <p>Structural encoding is fairly complicated! However, the goal is to suck out all the details related to I/O scheduling so that compression libraries can focus on compression. This keeps our compression traits simple without sacrificing our ability to perform random access.</p> <p>There are only a few structural encodings. The structural encoding is described by the <code>PageLayout</code> message and is the top-level message for the encoding.</p> <pre><code>message PageLayout {\n  oneof layout {\n    // A layout used for pages where the data is small\n    MiniBlockLayout mini_block_layout = 1;\n    // A layout used for pages where all values are null\n    AllNullLayout all_null_layout = 2;\n    // A layout used for pages where the data is large\n    FullZipLayout full_zip_layout = 3;\n    // A layout where large binary data is encoded externally\n    // and only the descriptions are put in the page\n    BlobLayout blob_layout = 4;\n  }\n\n}\n</code></pre>"},{"location":"format/file/encoding/#repetition-and-definition-levels","title":"Repetition and Definition Levels","text":"<p>Repetition and definition levels are an alternative to validity bitmaps and offset arrays for expressing struct and list information. They have a significant advantage in that they combine all of these buffers into a single buffer which allows us to avoid multiple IOPS.</p> <p>A more extensive explanation of repetition and definition levels can be found in the code. One particular note is that we use 0 to represent the \"inner-most\" item and Parquet uses 0 to represent the \"outer-most\" item. Here is an example:</p>"},{"location":"format/file/encoding/#definition-levels","title":"Definition Levels","text":"<p>Consider the following array:</p> <pre><code>[{\"middle\": {\"inner\": 1]}}, NULL, {\"middle\": NULL}, {\"middle\": {\"inner\": NULL}}]\n</code></pre> <p>In Arrow we would have the following validity arrays:</p> <pre><code>Outer validity : 1, 0, 1, 1\nMiddle validity: 1, ?, 0, 1\nInner validity : 1, ?, ?, 0\nValues         : 1, ?, ?, ?\n</code></pre> <p>The ? values are undefined in the Arrow format. We can convert these into definition levels as follows:</p> Values Definition Notes 1 0 Valid at all levels ? 3 Null at outer level ? 2 Null at middle level ? 1 Null at inner level"},{"location":"format/file/encoding/#repetition-levels","title":"Repetition Levels","text":"<p>Consider the following list array with 3 rows</p> <pre><code>[{&lt;0,1&gt;, &lt;&gt;, &lt;2&gt;}, {&lt;3&gt;}, {}], [], [{&lt;4&gt;}]\n</code></pre> <p>We would have three offsets arrays in Arrow:</p> <pre><code>Outer-most ([]): [0, 3, 3, 4]\nMiddle     ({}): [0, 3, 4, 4, 5]\nInner      (&lt;&gt;): [0, 2, 2, 3, 4, 5]\nValues         : [0, 1, 2, 3, 4]\n</code></pre> <p>We can convert these into repetition levels as follows:</p> Values Repetition Notes 0 3 Start of outer-most list 1 0 Continues inner-most list (no new lists) ? 1 Start of new inner-most list (empty list) 2 1 Start of new inner-most list 3 2 Start of new middle list ? 2 Start of new inner-most list (empty list) ? 3 Start of new outer-most list (empty list) 4 0 Start of new outer-most list"},{"location":"format/file/encoding/#mini-block-page-layout","title":"Mini Block Page Layout","text":"<p>The mini block page layout is the default layout for smallish types. This fits most of the classical data types (integers, floats, booleans, small strings, etc.) that Parquet and related formats already handle well. As is no surprise, the approach used is pretty similar to those formats.</p> <p></p> <p>The data is divided into small mini-blocks. Each mini-block should contain a power-of-two number of values (except for the last mini-block) and should be less than 32KiB of compressed data. We have to read an entire mini-block to get a single value so we want to keep the mini-block size small. Mini blocks are padded to 8 byte boundaries. This helps to avoid alignment issues. Each mini-block starts with a small header which helps us figure out how much padding has been applied.</p> <p>The repetition and definition levels are sliced up and stored in the mini-blocks along with the compressed buffers. Since we need to read an entire mini-block there is no need to zip up the various buffers and they are stored one after the other (repetition, definition, values, ...).</p>"},{"location":"format/file/encoding/#buffer-1-mini-blocks","title":"Buffer 1 (Mini Blocks)","text":"Bytes Meaning 1 Number of buffers in the mini-block 2 Size of buffer 0 2 Size of buffer 1 ... ... 2 Size of buffer N 0-7 Padding to ensure 8 byte alignment * Buffer 0 0-7 Padding to ensure 8 byte alignment * Buffer 1 ... ... 0-7 Padding to ensure 8 byte alignment * Buffer N 0-7 Padding to ensure 8 byte alignment <p>Note: It is natural to explain this buffer first but it is actually the second buffer in the page.</p>"},{"location":"format/file/encoding/#buffer-0-mini-block-metadata","title":"Buffer 0 (Mini Block Metadata)","text":"<p>To enable random access we have a small metadata lookup which contains two bytes per mini-block. This lookup tells us how many bytes are in each mini block and how many items are in the mini block. This metadata lookup must be loaded at initialization time and placed in the search cache.</p> Bits (not bytes) Meaning 12 Number of 8-byte words in block 0 4 Log2 of number of values in block 0 12 Number of 8-byte words in block 1 4 Log2 of number of values in block 1 ... ... 12 Number of 8-byte words in block N 4 Log2 of number of values in block N <p>The last 4 bits are special and we just store 0 today. This is because the protobuf contains the number of values (not required to be a power of 2) in the entire disk page. We can subtract the values in the other blocks to get the number of values in the last block.</p>"},{"location":"format/file/encoding/#buffer-2-dictionary-optional","title":"Buffer 2 (Dictionary, optional)","text":"<p>Dictionary encoding is an encoding that can be applied at many different levels throughout a file. For example, it could be used as a compressive encoding or it could even be entirely external to the file. We've found the most convenient simple place to apply dictionary encoding is at the structural level. Since dictionary indices are small we always use the mini block layout for dictionary encoding. When we use dictionary encoding we store the dictionary in the buffer at index 2. We require the dictionary to be full loaded and decoded at initialization time. This means we don't have to load the dictionary during random access but it does require the dictionary be placed in the search cache.</p> <p>Dictionary encoding is one of the few spots today where we have no rules on how it is encoded and compressed. We treat the entire dictionary as a single opaque buffer. As a result we rely on the block compression trait to handle dictionary compression.</p>"},{"location":"format/file/encoding/#buffer-2-or-3-repetition-index-optional","title":"Buffer 2 (or 3) (Repetition Index, optional)","text":"<p>If there is repetition (list levels) then we need some way to translate row offsets into item offsets. The mini blocks always store items. During a full scan the list offsets are restored when we decode the repetition levels. However, to support random access, we don't have the repetition levels available. Instead we store a repetition index in the next available buffer (index 2 or 3 depending on whether the dictionary is present).</p> <p>The repetition index is a flat buffer of u64 values. We have N * D values where N is the number of mini blocks and D Is the desired depth of random access plus one. For example, to support 1-dimensional lookups (random access by rows) then D is 2. To support two-dimensional lookups (e.g. rows[50][17]) then we could set D to 3.</p> <p>Currently we only support 1-dimensional random access. Currently we do not compress the repetition index.</p> <p>This may change in future versions.</p> Bytes Meaning 8 Number of rows in block 0 8 Number of partial items in block 0 8 Number of rows in block 1 8 Number of partial items in block 1 ... ... 8 Number of rows in block N 8 Number of partial items in block N <p>The last 8 bytes of each block stores the number of \"partial\" items. These are items leftover after the last complete row. We don't require rows to be bounded by mini-blocks so we need to keep track of this. For example, if we have 10,000 items per row then we might have several mini-blocks with only partial items and 0 rows.</p> <p>At read time we can use this repetition index to translate row offsets into item offsets.</p>"},{"location":"format/file/encoding/#mini-block-compression","title":"Mini Block Compression","text":"<p>The mini block layout relies on the compression algorithm to handle the splitting of data into mini-blocks. This is because the number of values per block will depend on the compressibility of the data. As a result, there is a special trait for mini block compression.</p> <p>The data compression algorithm is the algorithm that decides chunk boundaries. The repetition and definition levels are then sliced appropriately and sent to a block compressor. This means there are no constraints on how the repetition and definition levels are compressed.</p> <p>Beyond splitting the data into mini-blocks, there are no additional constraints. We expect to fully decode mini blocks as opaque chunks. This means we can use any compression algorithm that we deem suitable.</p>"},{"location":"format/file/encoding/#protobuf","title":"Protobuf","text":"<pre><code>message MiniBlockLayout {\n  // Description of the compression of repetition levels (e.g. how many bits per rep)\n  //\n  // Optional, if there is no repetition then this field is not present\n  CompressiveEncoding rep_compression = 1;\n  // Description of the compression of definition levels (e.g. how many bits per def)\n  //\n  // Optional, if there is no definition then this field is not present\n  CompressiveEncoding def_compression = 2;\n  // Description of the compression of values\n  CompressiveEncoding value_compression = 3;\n  // Description of the compression of the dictionary data\n  //\n  // Optional, if there is no dictionary then this field is not present\n  CompressiveEncoding dictionary = 4;\n  // Number of items in the dictionary\n  uint64 num_dictionary_items = 5;\n  // The meaning of each repdef layer, used to interpret repdef buffers correctly\n  repeated RepDefLayer layers = 6;\n  // The number of buffers in each mini-block, this is determined by the compression and does\n  // NOT include the repetition or definition buffers (the presence of these buffers can be determined\n  // by looking at the rep_compression and def_compression fields)\n  uint64 num_buffers = 7;\n  // The depth of the repetition index.\n  //\n  // If there is repetition then the depth must be at least 1.  If there are many layers\n  // of repetition then deeper repetition indices will support deeper nested random access.  For\n  // example, given 5 layers of repetition then the repetition index depth must be at least\n  // 3 to support access like rows[50][17][3].\n  //\n  // We require `repetition_index_depth + 1` u64 values per mini-block to store the repetition\n  // index if the `repetition_index_depth` is greater than 0.  The +1 is because we need to store\n  // the number of \"leftover items\" at the end of the chunk.  Otherwise, we wouldn't have any way\n  // to know if the final item in a chunk is valid or not.\n  uint32 repetition_index_depth = 8;\n  // The page already records how many rows are in the page.  For mini-block we also need to know how\n  // many \"items\" are in the page.  A row and an item are the same thing unless the page has lists.\n  uint64 num_items = 9;\n\n}\n</code></pre> <p>The protobuf for the mini block layout describes the compression of the various buffers. It also tells us some information about the dictionary (if present) and the repetition index (if present).</p>"},{"location":"format/file/encoding/#full-zip-page-layout","title":"Full Zip Page Layout","text":"<p>The full zip page layout is a layout for larger values (e.g. vector embeddings) which are large but not so large that we can justify a single IOP per value. In this case we are trying to avoid storing a large amount of \"chunk overhead\" (both in terms of buffer space and the RAM space in the search cache that we would need to store the repetition index). As a tradeoff, we are introducing a second IOP per-range for random access reads (unless the data is fixed-width such as vector embeddings).</p> <p>We currently use 256 bytes as the cutoff for the full zip layout. At this point we would only be fitting 16 values in a 4KiB disk sector and so creating a mini-block descriptor for every 16 values would be too much overhead.</p> <p>As a further consequence, we must ensure that the compression algorithm is \"transparent\" so that we can index individual values after compression has been applied. This prevents us from using compression algorithms such as delta encoding. If we want to apply general compression we have to apply them on a per-value basis. The way we enforce this is by requiring the compression to return either a flat fixed-width or variable-width layout so that we know the location of each element.</p> <p>The repetition and definition levels, along with all compressed buffers, are all zipped together into a single buffer.</p>"},{"location":"format/file/encoding/#data-buffer-buffer-0","title":"Data Buffer (Buffer 0)","text":"<p>The data buffer is a single buffer that contains the repetition, definition, and value data, all zipped into a single buffer. The repetition and definition information are combined and byte packed. This is referred to as a control word. If the value is null or an empty list, then the control word is all that is serialized. If there is no validity or repetition information then control words are not serialized. If the value is variable-width then we encode the size of the value. This is either a 4-byte or 8-byte integer depending on the width used in the offsets returned by the compression (in future versions this will likely be encoded with some kind of variable-width integer encoding). Finally the value buffers themselves are appended.</p> Bytes Meaning 0-4 Control word 0 0/4/8 Value 0 size * Value 0 data ... ... 0-4 Control word N 0/4/8 Value N size * Value N data <p>Note: a fixed-width data type that has no validity information (e.g. non-nullable vector embeddings) is simply a flat buffer of data.</p>"},{"location":"format/file/encoding/#repetition-index-buffer-1","title":"Repetition Index (Buffer 1)","text":"<p>If there is repetition information or the values are variable width then we need additional help to locate values in the disk page. The repetition index is an array of u64 values. There is one value per row and the value is an offset to the start of that row in the data buffer. To perform random access we require two IOPS. First we issue an IOP into the repetition index to determine the location and then a second IOP into the data buffer to load the data. Alternatively, the entire repetition index can be loaded into memory in the initialization phase though this can lead to high RAM usage by the search cache.</p> <p>The repetition index must have a fixed width (or else we would need a repetition index to read the repetition index!) and be transparent. As a result the compression options are limited. That being said, there is little value (in terms of performance) in compressing the repetition index. It is never read in its entirety as it is not needed for full scans. Currently the repetition index is always compressed with simple (non-chunked) byte packing into 1,2,4, or 8 byte values.</p>"},{"location":"format/file/encoding/#protobuf_1","title":"Protobuf","text":"<pre><code>message FullZipLayout {\n  // The number of bits of repetition info (0 if there is no repetition)\n  uint32 bits_rep = 1;\n  // The number of bits of definition info (0 if there is no definition)\n  uint32 bits_def = 2;\n  // The number of bits of value info\n  //\n  // Note: we use bits here (and not bytes) for consistency with other encodings.  However, in practice,\n  // there is never a reason to use a bits per value that is not a multiple of 8.  The complexity is not\n  // worth the small savings in space since this encoding is typically used with large values already.\n  oneof details {\n    // If this is a fixed width block then we need to have a fixed number of bits per value\n    uint32 bits_per_value = 3;\n    // If this is a variable width block then we need to have a fixed number of bits per offset\n    uint32 bits_per_offset = 4;\n  }\n  // The number of items in the page\n  uint32 num_items = 5;\n  // The number of visible items in the page\n  uint32 num_visible_items = 6;\n  // Description of the compression of values\n  CompressiveEncoding value_compression = 7;\n  // The meaning of each repdef layer, used to interpret repdef buffers correctly\n  repeated RepDefLayer layers = 8;\n\n}\n</code></pre> <p>The protobuf for the full zip layout describes the compression of the data buffer. It also tells us the size of the control words and how many bits we have per value (for fixed-width data) or how many bits we have per offset (for variable-width data).</p>"},{"location":"format/file/encoding/#all-null-page-layout","title":"All Null Page Layout","text":"<p>This layout is used when all the values are null. Surprisingly, this does not mean there is no data. If there are any levels of struct or list then we need to store the rep/def levels so that we can distinguish between null structs, null lists, empty lists, and null values.</p>"},{"location":"format/file/encoding/#repetition-and-definition-levels-buffers-0-and-1","title":"Repetition and Definition Levels (Buffers 0 and 1)","text":"<p>Note: We currently store rep levels in the first buffer with a flat layout of 16-bit values and def levels in the second buffer with a flat layout of 16-bit values. This will likely change in future versions.</p>"},{"location":"format/file/encoding/#protobuf_2","title":"Protobuf","text":"<pre><code>message AllNullLayout {\n  // The meaning of each repdef layer, used to interpret repdef buffers correctly\n  repeated RepDefLayer layers = 5;\n\n}\n</code></pre> <p>All we need to know is the meaning of each rep/def level.</p>"},{"location":"format/file/encoding/#blob-page-layout","title":"Blob Page Layout","text":"<p>The blob page layout is a layout for large binary values where we would only have a few values per disk page. The actual data is stored out-of-line in external buffers. The disk page stores a \"description\" which is a struct array of two fields: <code>position</code> and <code>size</code>. The <code>position</code> is the absolute file offset of the blob and the <code>size</code> is the size (in bytes) of the blob. The inner page layout describes how the descriptions are encoded.</p> <p>The validity information (definition levels) is smuggled into the descriptions. If the size and position are both zero then the value is empty. Otherwise, if the size is zero and the position is non-zero then the value is null and the position is the definition level.</p> <p>This layout is only recommended when you can justify a single IOP per value. For example, when values are 1MiB or larger.</p> <p>This layout has no buffers of its own and merely wraps an inner layout.</p>"},{"location":"format/file/encoding/#protobuf_3","title":"Protobuf","text":"<pre><code>message BlobLayout {\n  // The inner layout used to store the descriptions\n  PageLayout inner_layout = 1;\n  // The meaning of each repdef layer, used to interpret repdef buffers correctly\n  //\n  // The inner layout's repdef layers will always be 1 all valid item layer\n  repeated RepDefLayer layers = 2;\n\n}\n</code></pre> <p>Since we smuggle the validity into the descriptions we don't need to store it in the inner layout and so the rep/def meaning is stored in the blob layout and the rep/def meaning in the inner layout will be 1 all valid item layer.</p>"},{"location":"format/file/encoding/#semi-structural-transformations","title":"Semi-Structural Transformations","text":"<p>There are some data transformations that are applied to the data before (or during) the structural encoding process. These are described here.</p>"},{"location":"format/file/encoding/#dictionary-encoding","title":"Dictionary Encoding","text":"<p>Dictionary encoding is a technique that can be applied to any kind of array. It is useful when there are not very many unique values in the array. First, a \"dictionary\" of unique values is created. Then we create a second array of indices into the dictionary.</p> <p>Dictionary encoding is also known as \"categorical encoding\" in other contexts.</p> <p>Dictionary encoding could be treated as simply another compression technique but, when applied, it would be an opaque compression technique which would limit its usability (e.g. in a full zip context). As a result, we apply it before any structural encoding takes place. This allows us to place the dictionary in the search cache for random access.</p>"},{"location":"format/file/encoding/#struct-packing","title":"Struct Packing","text":"<p>Struct packing is an alternative representation to apply to struct values. Instead of storing that struct in a columnar fashion it will be stored in a row-major fashion. This will reduce the number of IOPS needed for random access but will prevent the ability to read a single field at a time. This is useful when all fields in the struct are always accessed together.</p> <p>Packed struct is always opt-in (see section on configuration below).</p> <p>Currently packed struct is limited to fixed-width data.</p>"},{"location":"format/file/encoding/#fixed-size-list","title":"Fixed Size List","text":"<p>Fixed size lists are an Arrow data type that needs specialized handling at the structural level. If the underlying data type is primitive then the fixed size list will be primitive (e.g. a tensor). If the underlying data type is structural (struct/list) then the fixed size list is structural and should be treated the same as a variable-size list.</p> <p>We don't want compression libraries to need to worry about the intricacies of fixed-size lists. As a result we flatten the list as part of structural encoding. This complicates random access as we must translate between rows (an entire fixed size list) and items (a single item in the list).</p> <p>If the items in a fixed size list are nullable then we do not treat that validity array as a repetition or definition level. Instead, we store the validity as a separate buffer. For example, when encoding nullable fixed size lists with mini-block encoding the validity buffer is another buffer in the mini-block. When encoding nullable fixed size lists with full-zip encoding the validity buffer is zipped together with the values.</p> <p>The good news is that fixed size lists are entirely a structural encoding concern. Compression techniques are free to pretend that the fixed-size list data type does not exist.</p>"},{"location":"format/file/encoding/#compression","title":"Compression","text":"<p>Once a structural encoding is chosen we must determine how to compress the data. There are various buffers that might be compressed (e.g. data, repetition, definition, dictionary, etc.). The available compression algorithms are also constrained by the structural encoding chosen. For example, when using the full zip layout we require transparent compression. As a result, each encoding technique may or may not be usable in a given scenario. In addition, the same technique may be applied in a different way depending on the encoding chosen.</p> <p>In implementation terms we have a trait for each compression constraint. The techniques then implement the traits that they can be applied to. To start with, here is a summary of compression techniques which are implemented in at least one scenario and a list of which traits the technique implements. A \u2753 is used to indicate that the technique should be usable in that context but we do not yet do so while a \u274c indicates that the technique is not usable because it is not transparent. Note, even though a technique is not transparent it can still be applied on a per-value basis. We use \u2611\ufe0f to mark a technique that is applied on a per-value basis:</p> Compression Used in Block Context Used in Full Zip Context Used in Mini-Block Context Flat \u2705 (2.1) \u2705 (2.1) \u2705 (2.1) Variable \u2705 (2.1) \u2705 (2.1) \u2705 (2.1) Constant \u2705 (2.1) \u2753 \u2753 Bitpacking \u2705 (2.1) \u2753 \u2705 (2.1) Fsst \u2753 \u2705 (2.1) \u2705 (2.1) Rle \u2753 \u274c \u2705 (2.1) ByteStreamSplit \u2753 \u274c \u2705 (2.1) General \u2753 \u2611\ufe0f (2.1) \u2705 (2.1) <p>In the following sections we will describe each technique in a bit more detail and explain how it is utilized in various contexts.</p>"},{"location":"format/file/encoding/#flat","title":"Flat","text":"<p>Flat compression is the uncompressed representation of fixed-width data. There is a single buffer of data with a fixed number of bits per value.</p> <p>When applied in a mini-block context we find the largest power of 2 number of values that will be less than 8,186 bytes and use that as the block size.</p>"},{"location":"format/file/encoding/#variable","title":"Variable","text":"<p>Variable compression is the uncompressed representation of variable-width data. There is a buffer of values and a buffer of offsets.</p> <p>When applied in a mini-block context each block may have a different number of values. We walk through the values until we find the point that would exceed 4,096 bytes and then use the most recent power of 2 number of values that we have passed.</p>"},{"location":"format/file/encoding/#constant","title":"Constant","text":"<p>Constant compression is currently only utilized in a few specialized scenarios such as all-null arrays.</p> <p>This will likely change in future versions.</p>"},{"location":"format/file/encoding/#bitpacking","title":"Bitpacking","text":"<p>Bitpacking is a compression technique that removes the unused bits from a set of values. For example, if we have a u32 array and the maximum value is 5000 then we only need 13 bits to store each value.</p> <p>When used in a mini-block context we always use 1024 values per block. In addition, we store the compressed bit width inline in the block itself.</p> <p>Bitpacking is, in theory, usable in a full zip context. However, values in this context are so large that shaving off a few bits is unlikely to have any meaningful impact. Also, the full-zip context keeps things byte-aligned and so we would have to remove at least 8 bits per value.</p>"},{"location":"format/file/encoding/#fsst","title":"Fsst","text":"<p>Fsst is a fast and transparent compression algorithm for variable-width data. It is the primary compression algorithm that we apply to variable-width data.</p> <p>Currently we use a single FSST symbol table per disk page and store that symbol table in the protobuf description. This is for historical reasons and is not ideal and will likely change in future versions.</p> <p>When FSST is applied in a mini-block context we simply compress the data and let the underlying compressor (always <code>Variable</code> at the moment) handle the chunking.</p>"},{"location":"format/file/encoding/#run-length-encoding-rle","title":"Run Length Encoding (RLE)","text":"<p>Run length encoding is a compression technique that compresses large runs of identical values into an array of values and an array of run lengths. This is currently used in the mini-block context. To determine if we should apply run-length encoding we look at the number of runs divided by the number of values. If the ratio is below a threshold (by default 0.5) then we apply run-length encoding.</p>"},{"location":"format/file/encoding/#byte-stream-split-bss","title":"Byte Stream Split (BSS)","text":"<p>Byte stream split is a compression technique that splits multi-byte values by byte position, creating separate streams for each byte position across all values. This is a rudimentary and simple form of translating floating point values into a more compressible format because it tends to cluster the mantissa bits together which are often consistent across a column of floating point values. It does not actually make the data smaller by itself. As a result, BSS is only applied if general compression is also applied on the column.</p> <p>We currently determine whether or not to apply BSS by looking at an entropy statistics. There is a configurable sensitivity parameter. A sensitivity of 0.0 means never apply BSS and a sensitivity of 1.0 means always apply BSS.</p>"},{"location":"format/file/encoding/#general","title":"General","text":"<p>General compression is a catch-all term for classical opaque compression techniques such as LZ4, ZStandard, Snappy, etc. These techniques are typically back-referencing compressors which replace values with a \"back reference\" to a spot where we already saw the value.</p> <p>When applied in a mini-block context we run general compression after all other compression and compress the entire mini-block.</p> <p>When applied in a full zip context we run general compression on each value.</p> <p>The only time general compression is automatically applied is in a full-zip context when we have values that are at least 32KiB large. This is because general compression can be CPU intensive.</p> <p>However, general compression is highly effective and we allow it to be opted into in other contexts via configuration.</p>"},{"location":"format/file/encoding/#compression-configuration","title":"Compression Configuration","text":"<p>The following section lists the available configuration options. These can be set programmatically through writer options. However, they can also be set in the field metadata in the schema.</p> Key Values Default Description <code>lance-encoding:compression</code> <code>lz4</code>, <code>zstd</code>, <code>none</code>, ... <code>none</code> Opt-in to general compression. The value indicates the scheme. <code>lance-encoding:compression-level</code> Integers (range is scheme dependent) Varies by scheme Higher indicates more work should be done to compress the data. <code>lance-encoding:rle-threshold</code> <code>0.0-1.0</code> <code>0.5</code> See below <code>lance-encoding:bss</code> <code>off</code>, <code>on</code>, <code>auto</code> <code>auto</code> See below <code>lance-encoding:dict-divisor</code> Integers greater than 1 <code>2</code> See below <code>lance-encoding:general</code> <code>off</code>, <code>on</code> <code>off</code> Whether to apply general compression. <code>lance-encoding:packed</code> Any string Not set Whether to apply packed struct encoding (see above). <code>lance-encoding:structural-encoding</code> <code>miniblock</code>, <code>fullzip</code> Not set Force a particular structural encoding to be applied (only useful for testing purposes)"},{"location":"format/file/encoding/#configuration-details","title":"Configuration Details","text":""},{"location":"format/file/encoding/#compression-scheme","title":"Compression Scheme","text":"<p>The <code>lance-encoding:compression</code> setting enables general-purpose compression algorithms to be applied. Available schemes:</p> <ul> <li><code>lz4</code>: Fast compression with good compression ratios. Default compression level is fast mode.</li> <li><code>zstd</code>: High compression ratios with configurable levels (0-22). Better compression than LZ4 but slower.</li> <li><code>none</code>: No general compression applied (default).</li> <li><code>fsst</code>: Fast Static Symbol Table compression for string data.</li> </ul> <p>General compression is applied on top of other encoding techniques (RLE, BSS, bitpacking, etc.) to further reduce data size. For mini-block layouts, compression is applied to entire mini-blocks. For full-zip layouts with large values (\u226532KiB), compression is automatically applied per-value.</p>"},{"location":"format/file/encoding/#compression-level","title":"Compression Level","text":"<p>The compression level is scheme dependent. Currently the following schemes support the following levels:</p> Scheme Crate Used Levels Default <code>zstd</code> <code>zstd</code> <code>0-22</code> <code>crate dependent</code> (3 as of this writing) <code>lz4</code> <code>lz4</code> N/A The LZ4 crate has two modes (fast and high compression) and currently this is not exposed to configuration. The LZ4 crate wraps a C library and the default is dependent on the C library. The default as of this writing is fast <p>Higher compression levels generally provide better compression at the cost of slower encoding speed. Decoding speed is typically less affected by the compression level.</p>"},{"location":"format/file/encoding/#run-length-encoding-rle-threshold","title":"Run Length Encoding (RLE) Threshold","text":"<p>The RLE threshold is used to determine whether or not to apply run-length encoding. The threshold is a ratio calculated by dividing the number of runs by the number of values. If the ratio is less than the threshold then we apply run-length encoding. The default is 0.5 which means we apply run-length encoding if the number of runs is less than half the number of values.</p> <p>Key points: - RLE is automatically selected when data has sufficient repetition (run_count / num_values &lt; threshold) - Supported types: All fixed-width primitives (u8, i8, u16, i16, u32, i32, f32, u64, i64, f64) - Maximum chunk size: 2048 values per mini-block - Setting threshold to <code>0.0</code> effectively disables RLE - Setting threshold to <code>1.0</code> makes RLE very aggressive (used whenever any runs exist)</p> <p>RLE is particularly effective for: - Sorted or partially sorted data - Columns with many repeated values (status codes, categories, etc.) - Low-cardinality columns</p>"},{"location":"format/file/encoding/#byte-stream-split-bss_1","title":"Byte Stream Split (BSS)","text":"<p>The configuration variable for BSS is a simple enum. A value of <code>off</code> means to never apply BSS, a value of <code>on</code> means to always apply BSS, and a value of <code>auto</code> means to apply BSS based on an entropy calculation (see code for details).</p> <p>Important: BSS is only applied when the <code>lance-encoding:compression</code> variable is also set (to a non-<code>none</code> value). BSS is a data transformation that makes floating-point data more compressible; it does not reduce size on its own.</p> <p>Key points: - Supported types: Only 32-bit and 64-bit data (f32, f64, timestamps) - Maximum chunk sizes: 1024 values (f32), 512 values (f64) - <code>auto</code> mode: Uses entropy analysis with 0.5 sensitivity threshold - <code>on</code> mode: Always applies BSS for supported types - <code>off</code> mode: Never applies BSS</p> <p>BSS works by splitting multi-byte values by byte position, creating separate byte streams. This clusters similar bits together (especially mantissa bits in floating-point numbers), which general compression algorithms can then compress more effectively.</p> <p>BSS is particularly effective for: - Floating-point measurements with similar ranges - Time-series data with consistent precision - Scientific data with correlated mantissa patterns</p>"},{"location":"format/file/encoding/#dictionary-divisor","title":"Dictionary Divisor","text":"<p>Currently this is used to determine whether or not we apply dictionary encoding. First, we use HLL to estimate the number of unique values in the column. Then we divide the number of total values by the divisor to get a threshold. If the number of unique values is less than the threshold then we apply dictionary encoding. The configuration variable defines the divisor that we apply and it defaults to 2 which means we apply dictionary encoding if we estimate that less than half the values are unique.</p> <p>Dictionary encoding is effective for columns with low cardinality where the same values repeat many times. The dictionary is stored once per page and indices are stored in place of the actual values.</p> <p>This is likely to change in future versions.</p>"},{"location":"format/file/encoding/#packed-struct-encoding","title":"Packed Struct Encoding","text":"<p>Packed struct encoding is a semi-structural transformation described above. When enabled, struct values are stored in row-major format rather than the default columnar format. This reduces the number of I/O operations needed for random access but prevents reading individual fields independently.</p> <p>This is always opt-in and should only be used when all struct fields are typically accessed together.</p>"},{"location":"format/file/versioning/","title":"Versioning","text":"<p>The Lance file format has a single version number for both the overall file format and the encoding strategy. The major number is changed when the file format itself is modified while the minor number is changed when only the encoding strategy is modified. Newer versions will typically have better performance and compression but may not be readable by older versions of Lance.</p> <p>In addition, the latest version of the file format (next) is unstable and should not be used for production use cases. Breaking changes could be made to unstable encodings and that would mean that files written with these encodings are no longer readable by any newer versions of Lance. The <code>next</code> version should only be used for experimentation and benchmarking upcoming features.</p> <p>The following values are supported:</p> Version Minimal Lance Version Maximum Lance Version Description 0.1 Any 0.34 (write) This is the initial Lance format. It is no longer writable. 2.0 0.16.0 Any Rework of the Lance file format that removed row groups and introduced null support for lists, fixed size lists, and primitives 2.1 (unstable) None Any Enhances integer and string compression, adds support for nulls in struct fields, and improves random access performance with nested fields. legacy N/A N/A Alias for 0.1 stable N/A N/A Alias for the latest stable version (currently 2.0) next N/A N/A Alias for the latest unstable version (currently 2.1)"},{"location":"format/namespace/","title":"Lance Namespace Spec","text":"<p>Lance Namespace is an open specification on top of the storage-based Lance table and file format to standardize access to a collection of Lance tables (a.k.a. Lance datasets). It describes how a metadata service like Apache Hive MetaStore (HMS), Apache Gravitino, Unity Catalog, etc. should store and use Lance tables,  as well as how ML/AI tools and analytics compute engines should integrate with Lance tables.</p>"},{"location":"format/namespace/#why-namespace-not-catalog","title":"Why Namespace not Catalog?","text":"<p>The specification is called \"Namespace\" rather than \"Catalog\" because there are already many open catalog specifications and catalog services in the market today. The goal of Lance Namespace is not to create yet another catalog spec, but to provide a consistent abstraction that adapts to all of them easily, so that users can choose to use any catalog to store and use Lance tables. Namespace can mean catalog, schema, metastore, database, metalake, or any other organizational concept. Lance Namespace provides a consistent interface to convert among different catalog specifications and map them into the connector interfaces of various compute engines.</p>"},{"location":"format/namespace/concepts/","title":"Namespace Concepts","text":""},{"location":"format/namespace/concepts/#namespace-definition","title":"Namespace Definition","text":"<p>A Lance namespace is a centralized repository for discovering, organizing, and managing Lance tables. It can either contain a collection of tables, or a collection of Lance namespaces recursively. It is designed to encapsulates concepts including namespace, metastore, database, schema, etc. that frequently appear in other similar data systems to allow easy integration with any system of any type of object hierarchy.</p> <p>Here is an example layout of a Lance namespace:</p> <p></p>"},{"location":"format/namespace/concepts/#parent-child","title":"Parent &amp; Child","text":"<p>We use the term parent and child to describe relationship between 2 objects. If namespace A directly contains B, then A is the parent namespace of B, i.e. B is a child of A. For examples:</p> <ul> <li>Namespace <code>ns1</code> contains a child namespace <code>ns4</code>. i.e. <code>ns1</code> is the parent namespace of <code>ns4</code>.</li> <li>Namespace <code>ns2</code> contains a child table <code>t2</code>, i.e. <code>t2</code> belongs to parent namespace <code>ns2</code>.</li> </ul>"},{"location":"format/namespace/concepts/#root-namespace","title":"Root Namespace","text":"<p>A root namespace is a namespace that has no parent. The root namespace is assumed to always exist and is ready to be connected to by a tool to explore objects in the namespace. The lifecycle management (e.g. creation, deletion) of the root namespace is out of scope of this specification.</p>"},{"location":"format/namespace/concepts/#object-name","title":"Object Name","text":"<p>The name of an object is a string that uniquely identifies the object within the parent namespace it belongs to. The name of any object must be unique among all other objects that share the same parent namespace. For examples:</p> <ul> <li><code>cat2</code>, <code>cat3</code> and <code>cat4</code> are all unique names under the root namespace</li> <li><code>t3</code> and <code>t4</code> are both unique names under <code>cat4</code></li> </ul>"},{"location":"format/namespace/concepts/#object-identifier","title":"Object Identifier","text":"<p>The identifier of an object uniquely identifies the object within the root namespace it belongs to. The identifier of any object must be unique among all other objects that share the same root namespace.</p> <p>Based on the uniqueness property of an object name within its parent namespace, an object identifier is the list of object names starting from (not including) the root namespace to (including) the object itself. This is also called an list style identifier. For examples:</p> <ul> <li>the list style identifier of <code>cat5</code> is <code>[cat2, cat5]</code></li> <li>the list style identifier of <code>t1</code> is <code>[cat2, cat5, t1]</code></li> </ul> <p>The dollar (<code>$</code>) symbol is used as the default delimiter to join all the names to form an string style identifier, but other symbols could also be used if dot is used in the object name. For examples:</p> <ul> <li>the string style identifier of <code>cat5</code> is <code>cat2$cat5</code></li> <li>the string style identifier of <code>t1</code> is <code>cat2$cat5$t1</code></li> <li>the string style identifier of <code>t3</code> is <code>cat4#t3</code> when using delimiter <code>#</code></li> </ul>"},{"location":"format/namespace/concepts/#name-and-identifier-for-root-namespace","title":"Name and Identifier for Root Namespace","text":"<p>The root namespace itself has no name or identifier. When represented in code, its name and string style identifier is represented by an empty or null string, and its list style identifier is represented by an empty or null list.</p> <p>The actual name and identifier of the root namespace is typically assigned by users through some configuration when used in a tool. For example, a root namespace can be called <code>cat1</code> in Ray, but called <code>cat2</code> in Apache Spark, and they are both configured to connect to the same root namespace.</p>"},{"location":"format/namespace/concepts/#object-level","title":"Object Level","text":"<p>The root namespace is always at level 0. This means if an object has list style identifier with list size <code>N</code>, the object is at the <code>N</code>th level in the entire namespace hierarchy. We also say the object identifier has <code>N</code> levels. For examples, a namespace <code>[ns1, ns2]</code> is at level 2, the identifier <code>ns1$ns2</code> has 2 levels. A table <code>[catalog1, database2, table3]</code> is at level 3, the identifier <code>catalog1$database2$table3</code> has 3 levels.</p>"},{"location":"format/namespace/concepts/#leveled-namespace","title":"Leveled Namespace","text":"<p>If every table in the root namespace are at the same level <code>N</code>, the namespace is called leveled, and we say this namespace is a <code>N</code>-level namespace. For example, a directory namespace is a 1-level namespace, and a Hive 2.x namespace is a 2-level namespace. The example above is not leveled because <code>t1</code> has 2 namespaces <code>ns1</code> and <code>ns4</code> before root, whereas <code>t2</code> has 1 namespace <code>ns2</code> before root.</p>"},{"location":"format/namespace/impls/","title":"Namespace Implementations","text":"<p>A Lance Namespace Implementation is an implementation of the Lance namespace specification, more specifically:</p> <ol> <li>It satisfies all the Lance namespace definitions and concepts.</li> <li>It declares and implements a list of supported Lance namespace operations.</li> </ol>"},{"location":"format/namespace/impls/#implementation-and-storage","title":"Implementation and Storage","text":"<p>Except for any storage-only implementation (e.g. directory namespace), a Lance table exists both in the storage and the implementation. For example, a Lance table exists both in HMS and storage for the Hive namespace. There are 2 possible ways to manage a Lance table under such setting. A Lance namespace implementation can choose to support one or both:</p>"},{"location":"format/namespace/impls/#implementation-managed-table","title":"Implementation Managed Table","text":"<p>A implementation managed Lance table is a table that is fully managed by the Lance namespace implementation. The implementation must maintain information about the latest version of the Lance table. Any modifications to the table must happen through the implementation. If a user directly modifies the underlying table in the storage bypassing the implementation, the implementation must not reflect the changes in the table to the namespace users.</p> <p>This mode ensures the namespace service is aware of all activities in the table, and can thus fully enforce any governance and management features for the table.</p>"},{"location":"format/namespace/impls/#storage-managed-table","title":"Storage Managed Table","text":"<p>A storage managed Lance table is a table that is fully managed by the storage with a metadata definition in the Lance namespace implementation. The implementation only contains information about the table directory location. It is expected that a tool finds the latest version of the Lance table based on the contents in the table directory according to the Lance format specification. A modification to the table can happen either directly against the storage, or happen as a request to the implementation, where the implementation is responsible for applying the corresponding change to the underlying storage according to the Lance format specification.</p> <p>This mode is more flexible for real world ML/AI workflows but the implementation loses full visibility and control over the actions performed against the table, so it will be harder to enforce any governance and management features for storage managed tables.</p>"},{"location":"format/namespace/impls/#native-implementations","title":"Native Implementations","text":"<p>A native Lance namespace implementation is a Lance Namespace implementation  that is maintained in this <code>lance-namespace</code> repository. Any implementation that is outside the repository is considered as a third-party implementation.</p>"},{"location":"format/namespace/impls/dataproc/","title":"Lance Dataproc Namespace","text":"<p>Google Dataproc Metastore is a fully managed,  highly available, autohealing, serverless metastore that runs on Google Cloud.</p> <p>To use Google Dataproc Metastore with Lance, you can leverage Dataproc's Hive metastore, which exposes a Hive MetaStore-compatible interface.</p> <p>Simply configure your Lance Hive namespace to connect to Dataproc's Hive MetaStore endpoint. All the features and configurations of the Lance Hive Namespace apply when using Dataproc Metastore.</p>"},{"location":"format/namespace/impls/dir/","title":"Lance Directory Namespace","text":"<p>Lance directory namespace is a lightweight and simple 1-level Lance namespace that only contains a list of tables. People can easily get started with creating and using Lance tables directly on top of any local or remote storage system with a Lance directory namespace.</p> <p>A directory namespace maps to a directory on storage, we call such directory a namespace directory. A Lance table corresponds to a subdirectory in the namespace directory that has the format <code>&lt;table_name&gt;.lance</code>. We call such a subdirectory table directory. Consider the following example namespace directory layout:</p> <pre><code>.\n\u2514\u2500\u2500 /my/dir1/\n    \u251c\u2500\u2500 table1.lance/\n    \u2502   \u251c\u2500\u2500 data/\n    \u2502   \u2502   \u251c\u2500\u2500 0aa36d91-8293-406b-958c-faf9e7547938.lance\n    \u2502   \u2502   \u2514\u2500\u2500 ed7af55d-b064-4442-bcb5-47b524e98d0e.lance\n    \u2502   \u251c\u2500\u2500 _versions/\n    \u2502   \u2502   \u2514\u2500\u2500 9223372036854775707.manifest\n    \u2502   \u2514\u2500\u2500 _indices/\n    \u2502       \u2514\u2500\u2500 85814508-ed9a-41f2-b939-2050bb7a0ed5-fts/\n    \u2502           \u2514\u2500\u2500 index.idx \n    \u251c\u2500\u2500 table2.lance\n    \u2514\u2500\u2500 table3.lance\n</code></pre> <p>This describes a Lance directory namespace with the namespace directory at <code>/my/dir1/</code>. It contains tables <code>table1</code>, <code>table2</code>, <code>table3</code> sitting at table directories <code>/my/dirs/table1.lance</code>, <code>/my/dirs/table2.lance</code>, <code>/my/dirs/table3.lance</code> respectively.</p>"},{"location":"format/namespace/impls/dir/#configuration","title":"Configuration","text":"<p>The Lance directory namespace accepts the following configuration properties:</p> Property Required Description Default Example <code>root</code> No The root directory of the namespace where tables are stored Current working directory <code>/my/dir</code>, <code>s3://bucket/prefix</code> <code>storage.*</code> No Storage-specific configuration options <code>storage.region=us-west-2</code>"},{"location":"format/namespace/impls/dir/#root-path","title":"Root Path","text":"<p>There are 3 ways to specify a root path:</p> <ol> <li>URI: a URI that follows the RFC 3986 specification, e.g. <code>s3://mu-bucket/prefix</code>.</li> <li>Absolute POSIX storage path: an absolute file path in a POSIX standard storage, e.g. <code>/my/dir</code>.</li> <li>Relative POSIX storage path: a relative file path in a POSIX standard storage, e.g. <code>my/dir2</code>, <code>./my/dir3</code>.    The absolute path of the root should be derived from the current working directory.</li> </ol>"},{"location":"format/namespace/impls/dir/#storage-options","title":"Storage Options","text":"<p>Properties with the <code>storage.</code> prefix are passed directly to the underlying OpenDAL storage system after removing the prefix. For example, <code>storage.region</code> becomes <code>region</code> when passed to the storage layer. Please visit Apache OpenDAL for more details.</p>"},{"location":"format/namespace/impls/dir/#table-existence","title":"Table Existence","text":"<p>A table exists in a Lance directory namespace if a table directory of the specific name exists and contains a non-empty <code>_versions</code> subdirectory.</p> <p>When checking if a specific table exists or deciding if a table should be listed,  the operation must list objects with the <code>_versions/</code> prefix and checks if any objects exist. If the directory exists but there is no file in the directory, it should be treated as non-existent.</p>"},{"location":"format/namespace/impls/glue/","title":"Lance Glue Namespace","text":"<p>Lance Glue Namespace is an implementation using AWS Glue Data Catalog. For more details about AWS Glue, please read the AWS Glue Data Catalog Documentation.</p>"},{"location":"format/namespace/impls/glue/#configuration","title":"Configuration","text":"<p>The Lance Glue namespace accepts the following configuration properties:</p> Property Required Description Default Example <code>catalog_id</code> No The Catalog ID of the Glue catalog (defaults to AWS account ID) <code>123456789012</code> <code>endpoint</code> No Custom Glue service endpoint for API compatible metastores <code>https://glue.example.com</code> <code>region</code> No AWS region for all Glue operations <code>us-west-2</code> <code>access_key_id</code> No AWS access key ID for static credentials <code>secret_access_key</code> No AWS secret access key for static credentials <code>session_token</code> No AWS session token for temporary credentials <code>root</code> No Storage root location of the lakehouse on Glue catalog Current working directory <code>/my/dir</code>, <code>s3://bucket/prefix</code> <code>storage.*</code> No Additional storage configurations to access table <code>storage.region=us-west-2</code>"},{"location":"format/namespace/impls/glue/#authentication","title":"Authentication","text":"<p>The Glue namespace supports multiple authentication methods:</p> <ol> <li>Default AWS credential provider chain: When no explicit credentials are provided, the client uses the default AWS credential provider chain</li> <li>Static credentials: Set <code>access_key_id</code> and <code>secret_access_key</code> for basic AWS credentials</li> <li>Session credentials: Additionally provide <code>session_token</code> for temporary AWS credentials</li> </ol>"},{"location":"format/namespace/impls/glue/#namespace-mapping","title":"Namespace Mapping","text":"<p>An AWS Glue Data Catalog can be viewed as the root Lance namespace. A database in Glue maps to the first level Lance namespace, to form a 2-level Lance namespace as a whole.</p>"},{"location":"format/namespace/impls/glue/#table-definition","title":"Table Definition","text":"<p>When fully implemented, a Lance table should appear as a Table  object in AWS Glue with the following requirements:</p> <ol> <li>the <code>TableType</code> must be set to <code>EXTERNAL_TABLE</code> to indicate this is not a Glue managed table</li> <li>the <code>StorageDescriptor.Location</code> must point to the root location of the Lance table</li> <li>the <code>Parameters</code> must follow:<ol> <li>there is a key <code>table_type</code> set to <code>lance</code> (case insensitive)</li> <li>there is a key <code>managed_by</code> set to either <code>storage</code> or <code>impl</code> (case insensitive). If not set, default to <code>storage</code></li> <li>there is a key <code>version</code> set to the latest numeric version number of the table. This field will only be respected if <code>managed_by=impl</code></li> </ol> </li> </ol>"},{"location":"format/namespace/impls/glue/#requirement-for-implementation-managed-table","title":"Requirement for Implementation Managed Table","text":"<p>Updates to implementation-managed Lance tables must use AWS Glue\u2019s <code>VersionId</code> for conditional updates through the UpdateTable API. If the <code>VersionId</code> does not  match the expected version, the update fails to prevent concurrent modification conflicts.</p>"},{"location":"format/namespace/impls/gravitino/","title":"Lance Gravitino Namespace","text":"<p>Apache Gravitino is a high-performance, geo-distributed, and federated metadata lake.  It manages metadata directly in different sources, types, and regions, providing unified metadata access for data and AI assets.</p> <p>Lance at this moment does not provide a dedicated Gravitino namespace implementation.  However, Gravitino provides a Hive MetaStore-compatible endpoint,  which allows you to use Gravitino through the Lance Hive Namespace.</p>"},{"location":"format/namespace/impls/gravitino/#using-gravitino-with-lance","title":"Using Gravitino with Lance","text":"<p>To use Apache Gravitino with Lance, you can leverage Gravitino's Apache Hive Catalog,  which exposes a Hive MetaStore-compatible interface.</p> <p>Simply configure your Lance Hive namespace to connect to Gravitino's Hive MetaStore endpoint.  All the features and configurations of the Lance Hive Namespace apply when using Gravitino.</p>"},{"location":"format/namespace/impls/hive/","title":"Lance Hive Namespace","text":"<p>Lance Hive Namespace is an implementation using Apache Hive MetaStore (HMS). For more details about HMS, please read HMS AdminManual 2.x  and HMS AdminManual 3.x.</p>"},{"location":"format/namespace/impls/hive/#configuration","title":"Configuration","text":"<p>The Lance Hive namespace accepts the following configuration properties:</p> Property Required Description Default Example <code>client.pool-size</code> No Size of the HMS client connection pool 3 <code>5</code> <code>root</code> No Storage root location of the lakehouse on Hive catalog Current working directory <code>/my/dir</code>, <code>s3://bucket/prefix</code> <code>storage.*</code> No Additional storage configurations to access table <code>storage.region=us-west-2</code>"},{"location":"format/namespace/impls/hive/#version-specific-configuration","title":"Version-specific Configuration","text":"<p>Hive 2.x: Uses the configuration properties listed above.</p> <p>Hive 3.x: Uses the same configuration properties as Hive 2.x, with an additional catalog name constant (<code>hive</code>) for catalog-level operations. </p>"},{"location":"format/namespace/impls/hive/#namespace-mapping","title":"Namespace Mapping","text":"<p>A HMS server can be viewed as the root Lance namespace.</p> <p>For HMS 2.x and below, a database in HMS maps to the first level Lance namespace to form a 2-level Lance namespace as a whole.</p> <p>For HMS 3.x and above, a catalog in HMS maps to the first level Lance namespace, and a database in HMS maps to the second level Lance namespace to form a 3-level Lance namespace as a whole.</p>"},{"location":"format/namespace/impls/hive/#table-definition","title":"Table Definition","text":"<p>A Lance table should appear as a Table object  in HMS with the following requirements:</p> <ol> <li>the <code>tableType</code> must be set as <code>EXTERNAL_TABLE</code> to indicate this is not a managed Hive table</li> <li>the <code>location</code> in <code>storageDescriptor</code> must point to the root location of the Lance table</li> <li>the <code>parameters</code> must follow:<ol> <li>there is a key <code>table_type</code> set to <code>lance</code> (case insensitive)</li> <li>there is a key <code>managed_by</code> set to either <code>storage</code> or <code>impl</code> (case insensitive). If not set, default to <code>storage</code></li> <li>there is a key <code>version</code> set to the latest numeric version number of the table. This field will only be respected if <code>managed_by=impl</code> </li> </ol> </li> </ol>"},{"location":"format/namespace/impls/hive/#requirement-for-implementation-managed-table","title":"Requirement for Implementation Managed Table","text":"<p>An update to the implementation managed table must use Hive's atomic update feature (HIVE-26882) and use the <code>version</code> parameter value to perform conditional update through alter_table_with_environment_context</p>"},{"location":"format/namespace/impls/onelake/","title":"Lance OneLake Namespace","text":"<p>Microsoft OneLake is a unified, logical data lake for Microsoft Fabric that provides a single SaaS experience and a tenant-wide store for data that serves both professional and citizen data integration needs.</p> <p>Lance at this moment does not provide a dedicated OneLake namespace implementation. However, OneLake provides a Unity Catalog-compatible endpoint through its Table APIs, which allows you to use OneLake through the Lance Unity Namespace.</p>"},{"location":"format/namespace/impls/onelake/#using-onelake-with-lance","title":"Using OneLake with Lance","text":"<p>To use Microsoft OneLake with Lance, you can leverage OneLake's Table APIs, which expose a Unity Catalog-compatible interface.</p> <p>Simply configure your Lance Unity namespace to connect to OneLake's Unity Catalog endpoint at:</p> <pre><code>https://onelake.table.fabric.microsoft.com/delta\n</code></pre> <p>All the features and configurations of the Lance Unity Namespace apply when using OneLake.</p>"},{"location":"format/namespace/impls/polaris/","title":"Lance Polaris Namespace","text":"<p>Lance Polaris Namespace is an implementation using Polaris Catalog's Generic Table API. The Polaris namespace uses the following API endpoints:</p> <ul> <li>Namespace operations: Standard Iceberg REST API endpoints (<code>/namespaces</code>)</li> <li>Table operations: Generic Table API endpoints (<code>/namespaces/{namespace}/generic-tables</code>)</li> </ul> <p>For more details about Polaris Catalog, please read the Polaris Catalog Documentation.</p>"},{"location":"format/namespace/impls/polaris/#configuration","title":"Configuration","text":"<p>The Lance Polaris namespace accepts the following configuration properties:</p> Property Required Description Default Example <code>endpoint</code> Yes Polaris server endpoint URL <code>http://localhost:8182</code> <code>auth_token</code> No Bearer token for authentication <code>your-auth-token</code> <code>connect_timeout</code> No Connection timeout in milliseconds 10000 <code>30000</code> <code>read_timeout</code> No Read timeout in milliseconds 30000 <code>60000</code> <code>max_retries</code> No Maximum number of retries for failed requests 3 <code>5</code>"},{"location":"format/namespace/impls/polaris/#authentication","title":"Authentication","text":"<p>The Polaris namespace supports bearer token authentication:</p> <ol> <li>Bearer Token: Set <code>auth_token</code> with a valid Polaris access token</li> <li>No Authentication: For local or unsecured Polaris deployments</li> </ol>"},{"location":"format/namespace/impls/polaris/#namespace-mapping","title":"Namespace Mapping","text":"<p>Polaris provides a flexible namespace hierarchy:</p> <ul> <li>A catalog in Polaris maps to the first level Lance namespace</li> <li>Nested namespaces in Polaris map to subsequent Lance namespace levels</li> <li>Polaris supports arbitrary nesting depth, allowing flexible namespace organization</li> </ul>"},{"location":"format/namespace/impls/polaris/#table-definition","title":"Table Definition","text":"<p>A Lance table appears as a Generic Table object in Polaris with the following requirements:</p> <ol> <li>the <code>format</code> must be set to <code>lance</code> to indicate this is a Lance table</li> <li>the <code>base-location</code> must point to the root location of the Lance table</li> <li>the <code>doc</code> field can be used to store an optional table description</li> <li>the <code>properties</code> must follow:</li> <li>there is a key <code>table_type</code> set to <code>lance</code> (case insensitive)</li> <li>there is a key <code>managed_by</code> set to either <code>storage</code> or <code>impl</code> (case insensitive). If not set, default to <code>storage</code></li> <li>there is a key <code>version</code> set to the latest numeric version number of the table. This field will only be respected if <code>managed_by=impl</code></li> </ol>"},{"location":"format/namespace/impls/polaris/#requirement-for-implementation-managed-table","title":"Requirement for Implementation Managed Table","text":"<p>Updates to implementation-managed Lance tables must use Polaris's table versioning mechanism for conditional updates through the UpdateTable API. The <code>version</code> property must be updated atomically to prevent concurrent modification conflicts.</p>"},{"location":"format/namespace/impls/unity/","title":"Lance Unity Namespace","text":"<p>Lance Unity Namespace is an implementation using Unity Catalog. For more details about Unity Catalog, please read the Unity Catalog Documentation.</p>"},{"location":"format/namespace/impls/unity/#configuration","title":"Configuration","text":"<p>The Lance Unity namespace accepts the following configuration properties:</p> Property Required Description Default Example <code>endpoint</code> Yes Unity Catalog REST API endpoint <code>http://localhost:8080</code> <code>api_path</code> No API path prefix <code>/api/2.1/unity-catalog</code> <code>/api/2.1/unity-catalog</code> <code>auth_token</code> No Bearer token for authentication <code>dapi123456789abcdef</code> <code>catalog</code> Yes Unity Catalog name to use <code>main</code> <code>connect_timeout</code> No HTTP connection timeout in seconds 10 <code>30</code> <code>read_timeout</code> No HTTP read timeout in seconds 60 <code>120</code> <code>max_retries</code> No Maximum number of retries for failed requests 3 <code>5</code> <code>root</code> No Storage root location of the lakehouse on Unity catalog Current working directory <code>/my/dir</code>, <code>s3://bucket/prefix</code> <code>storage.*</code> No Additional storage configurations to access table <code>storage.region=us-west-2</code>"},{"location":"format/namespace/impls/unity/#authentication","title":"Authentication","text":"<p>The Unity namespace supports the following authentication methods:</p> <ol> <li>Bearer Token: Set <code>auth_token</code> with a valid Unity Catalog access token</li> <li>No Authentication: For local or unsecured Unity Catalog deployments</li> </ol>"},{"location":"format/namespace/impls/unity/#namespace-mapping","title":"Namespace Mapping","text":"<p>A Unity Catalog server provides a 3-level namespace hierarchy.</p> <ul> <li>A catalog in Unity Catalog maps to the first level Lance namespace</li> <li>A schema (database) in Unity Catalog maps to the second level Lance namespace</li> <li>Together they form a 3-level Lance namespace matching Unity's structure</li> </ul>"},{"location":"format/namespace/impls/unity/#table-definition","title":"Table Definition","text":"<p>A Lance table appears as a Table  object in Unity Catalog with the following requirements:</p> <ol> <li>the <code>table_type</code> must be set to <code>EXTERNAL</code> to indicate this is not a Unity managed table</li> <li>the <code>data_source_format</code> should be <code>TEXT</code> (Unity Catalog doesn't recognize <code>LANCE</code> format, so we use <code>TEXT</code> as a generic format for external tables)</li> <li>the <code>storage_location</code> must point to the root location of the Lance table</li> <li>the <code>columns</code> must be provided with the table schema converted from Lance's Arrow schema to Unity's column format</li> <li>the <code>properties</code> must follow:<ol> <li>there is a key <code>table_type</code> set to <code>lance</code> (case insensitive)</li> <li>there is a key <code>managed_by</code> set to either <code>storage</code> or <code>impl</code> (case insensitive). If not set, default to <code>storage</code></li> <li>there is a key <code>version</code> set to the latest numeric version number of the table. This field will only be respected if <code>managed_by=impl</code></li> </ol> </li> </ol>"},{"location":"format/namespace/impls/unity/#requirement-for-implementation-managed-table","title":"Requirement for Implementation Managed Table","text":"<p>Updates to implementation-managed Lance tables must use Unity Catalog's table versioning mechanism for conditional updates through the UpdateTable API. The <code>version</code> property must be updated atomically to prevent concurrent modification conflicts.</p>"},{"location":"format/namespace/impls/rest/","title":"Lance REST Namespace","text":"<p>In an enterprise environment, typically there is a requirement to store tables in a metadata service for more advanced governance features around access control, auditing, lineage tracking, etc. Lance REST Namespace is an OpenAPI protocol that enables reading, writing and managing Lance tables by connecting those metadata services or building a custom metadata server in a standardized way. The REST server definition can be found in the OpenAPI specification.</p>"},{"location":"format/namespace/impls/rest/#configuration","title":"Configuration","text":"<p>The Lance REST namespace accepts the following configuration properties:</p> Property Required Description Default Example <code>uri</code> Yes The URI endpoint for the REST API <code>https://api.example.com/lance</code> <code>delimiter</code> No The delimiter used to parse object string identifiers in REST routes <code>.</code> <code>.</code>, <code>/</code>, <code>::</code>, <code>#</code> <code>headers.*</code> No Additional headers to send with every request <code>headers.Authorization=Bearer...</code>"},{"location":"format/namespace/impls/rest/#headers","title":"Headers","text":"<p>Properties with the <code>headers.</code> prefix are passed as HTTP headers with every request to the REST server after removing the prefix. For example, <code>headers.Authorization</code> becomes the <code>Authorization</code> header.</p> <p>Common header configurations include: - <code>headers.Authorization</code>: Authentication tokens (Bearer, Basic, etc.) - <code>headers.X-API-Key</code>: API key authentication - <code>headers.X-Request-ID</code>: Request tracking</p>"},{"location":"format/namespace/impls/rest/#rest-routes","title":"REST Routes","text":"<p>The REST route for an operation typically follows the pattern of <code>POST /&lt;version&gt;/&lt;object&gt;/{id}/&lt;action&gt;</code>, for example <code>POST /v1/namespace/{id}/list</code> for <code>ListNamespace</code>. The request and response schemas are used as the actual request and response of the route. </p> <p>The key design principle of the REST route is that all the necessary information for a reverse proxy  (e.g. load balancing, authN, authZ) should be available for access without the need to deserialize request body.</p> <p>For routes that involve multiple objects, all related objects should be present in the route. For example, the route for <code>RenameTable</code> is thus <code>POST /v1/table/{from_id}/rename/to/{to_id}</code>.</p>"},{"location":"format/namespace/impls/rest/#standard-operations","title":"Standard Operations","text":"<p>Standard operations should take the same request and return the same response as any other implementation.</p> <p>The information in the route could also present in the request body. When the information in the route and request body both present but do not match, the server must throw a 400 Bad Request error. When the information in the request body is missing, the server must use the information in the route instead.</p>"},{"location":"format/namespace/impls/rest/#non-standard-operations","title":"Non-Standard Operations","text":"<p>For request and response that cannot be simply described as a JSON object  the REST server needs to perform special handling to describe equivalent information through path parameters,  query parameters and headers. The specific handling is described in the OpenAPI spec. Here are the non-standard operations:</p> <ul> <li>ListNamespaces</li> <li>ListTables</li> <li>ListTableTags</li> <li>CreateTable</li> <li>InsertIntoTable</li> <li>MergeInsertIntoTable</li> </ul>"},{"location":"format/namespace/impls/rest/#namespace-server-and-adapter","title":"Namespace Server and Adapter","text":"<p>Any REST HTTP server that implements this OpenAPI protocol is called a Lance Namespace server. If you are a metadata service provider that is building a custom implementation of Lance namespace, building a REST server gives you standardized integration to Lance without the need to worry about tool support and continuously distribute newer library versions compared to using an implementation.</p> <p>If the main purpose of this server is to be a proxy on top of an existing metadata service, converting back and forth between Lance REST API models and native API models of the metadata service, then this Lance namespace server is called a Lance Namespace adapter.</p>"},{"location":"format/namespace/impls/rest/#choosing-between-an-adapter-vs-an-implementation","title":"Choosing between an Adapter vs an Implementation","text":"<p>Any adapter can always be directly a Lance namespace implementation bypassing the REST server, and vise versa. In fact, an implementation is basically the backend of an adapter. For example, we natively support a Lance HMS Namespace implementation, as well as a Lance namespace adapter for HMS by using the HMS Namespace implementation to fulfill requests in the Lance REST server.</p> <p>If you are considering between a Lance namespace adapter vs implementation to build or use in your environment, here are some criteria to consider:</p> <ol> <li>Multi-Language Feasibility &amp; Maintenance Cost: If you want a single strategy that works across all Lance language bindings, an adapter is preferred.    Sometimes it is not even possible for an integration to go with the implementation approach since it cannot support all the languages.    Sometimes an integration is popular or important enough that it is viable to build an implementation and maintain one library per language.</li> <li>Tooling Support: each tool needs to declare the Lance namespace implementations it supports.    That means there will be a preference for tools to always support a REST namespace,    but it might not always support a specific implementation. This favors the adapter approach.</li> <li>Security: if you have security concerns about the adapter being a man-in-the-middle, you should choose an implementation</li> <li>Performance: after all, adapter adds one layer of indirection and is thus not the most performant solution.    If you are performance sensitive, you should choose an implementation</li> </ol>"},{"location":"format/namespace/impls/rest/create-table/","title":"REST Implementation Non-Standard Operation - CreateTable","text":""},{"location":"format/namespace/impls/rest/create-table/#rest-route-definition","title":"REST Route Definition","text":"<pre><code>  /v1/table/{id}/create:\n    parameters:\n      - $ref: '#/components/parameters/id'\n      - $ref: '#/components/parameters/delimiter'\n      - name: \"mode\"\n        in: query\n        required: false\n        schema:\n          type: string\n      - name: \"x-lance-table-location\"\n        in: header\n        required: false\n        schema:\n          type: string\n        description: URI pointing to root location to create the table at\n      - name: \"x-lance-table-properties\"\n        in: header\n        required: false\n        schema:\n          type: string\n        description: |\n          JSON-encoded string map (e.g. { \"owner\": \"jack\" })\n    post:\n      tags:\n        - Table\n        - Data\n      summary: Create a table with the given name\n      operationId: CreateTable\n      description: |\n        Create table `id` in the namespace with the given data in Arrow IPC stream.\n\n        The schema of the Arrow IPC stream is used as the table schema.    \n        If the stream is empty, the API creates a new empty table.\n\n        REST NAMESPACE ONLY\n        REST namespace uses Arrow IPC stream as the request body.\n        It passes in the `CreateTableRequest` information in the following way:\n        - `id`: pass through path parameter of the same name\n        - `mode`: pass through query parameter of the same name\n        - `location`: pass through header `x-lance-table-location`\n        - `properties`: pass through header `x-lance-table-properties`\n      requestBody:\n        description: Arrow IPC data\n        content:\n          application/vnd.apache.arrow.stream:\n            schema:\n              type: string\n              format: binary\n        required: true\n      responses:\n        200:\n          $ref: '#/components/responses/CreateTableResponse'\n        400:\n          $ref: '#/components/responses/BadRequestErrorResponse'\n        401:\n          $ref: '#/components/responses/UnauthorizedErrorResponse'\n        403:\n          $ref: '#/components/responses/ForbiddenErrorResponse'\n        404:\n          $ref: '#/components/responses/NotFoundErrorResponse'\n        503:\n          $ref: '#/components/responses/ServiceUnavailableErrorResponse'\n        5XX:\n          $ref: '#/components/responses/ServerErrorResponse'\n</code></pre>"},{"location":"format/namespace/impls/rest/insert-into-table/","title":"REST Implementation Non-Standard Operation - InsertIntoTable","text":""},{"location":"format/namespace/impls/rest/insert-into-table/#rest-route-definition","title":"REST Route Definition","text":"<pre><code>  /v1/table/{id}/insert:\n    parameters:\n      - $ref: '#/components/parameters/id'\n      - $ref: '#/components/parameters/delimiter'\n      - name: mode\n        in: query\n        description: |\n          How the insert should behave:\n          - append (default): insert data to the existing table\n          - overwrite: remove all data in the table and then insert data to it\n        required: false\n        schema:\n          type: string\n          enum:\n            - append\n            - overwrite\n          default: append\n    post:\n      tags:\n        - Table\n        - Data\n      summary: Insert records into a table\n      operationId: InsertIntoTable\n      description: |\n        Insert new records into table `id`.\n\n        REST NAMESPACE ONLY\n        REST namespace uses Arrow IPC stream as the request body.\n        It passes in the `InsertIntoTableRequest` information in the following way:\n        - `id`: pass through path parameter of the same name\n        - `mode`: pass through query parameter of the same name\n      requestBody:\n        description: Arrow IPC stream containing the records to insert\n        content:\n          application/vnd.apache.arrow.stream:\n            schema:\n              type: string\n              format: binary\n        required: true\n      responses:\n        200:\n          $ref: '#/components/responses/InsertIntoTableResponse'\n        400:\n          $ref: '#/components/responses/BadRequestErrorResponse'\n        401:\n          $ref: '#/components/responses/UnauthorizedErrorResponse'\n        403:\n          $ref: '#/components/responses/ForbiddenErrorResponse'\n        404:\n          $ref: '#/components/responses/NotFoundErrorResponse'\n        503:\n          $ref: '#/components/responses/ServiceUnavailableErrorResponse'\n        5XX:\n          $ref: '#/components/responses/ServerErrorResponse'\n</code></pre>"},{"location":"format/namespace/impls/rest/list-namespaces/","title":"REST Implementation Non-Standard Operation - ListNamespaces","text":""},{"location":"format/namespace/impls/rest/list-namespaces/#rest-route-definition","title":"REST Route Definition","text":"<pre><code>  /v1/namespace/{id}/list:\n    parameters:\n      - $ref: '#/components/parameters/id'\n      - $ref: '#/components/parameters/delimiter'\n      - name: 'page_token'\n        in: query\n        schema:\n          $ref: '#/components/schemas/PageToken'\n      - name: 'limit'\n        in: query\n        schema:\n          $ref: '#/components/schemas/PageLimit'\n    get:\n      tags:\n        - Namespace\n        - Metadata\n      summary: List namespaces\n      operationId: ListNamespaces\n      description: |\n        List all child namespace names of the parent namespace `id`.\n\n        REST NAMESPACE ONLY\n        REST namespace uses GET to perform this operation without a request body.\n        It passes in the `ListNamespacesRequest` information in the following way:\n        - `id`: pass through path parameter of the same name\n        - `page_token`: pass through query parameter of the same name\n        - `limit`: pass through query parameter of the same name\n      responses:\n        200:\n          $ref: '#/components/responses/ListNamespacesResponse'\n        400:\n          $ref: '#/components/responses/BadRequestErrorResponse'\n        401:\n          $ref: '#/components/responses/UnauthorizedErrorResponse'\n        403:\n          $ref: '#/components/responses/ForbiddenErrorResponse'\n        404:\n          $ref: '#/components/responses/NotFoundErrorResponse'\n        406:\n          $ref: '#/components/responses/UnsupportedOperationErrorResponse'\n        503:\n          $ref: '#/components/responses/ServiceUnavailableErrorResponse'\n        5XX:\n          $ref: '#/components/responses/ServerErrorResponse'\n</code></pre>"},{"location":"format/namespace/impls/rest/list-table-tags/","title":"REST Implementation Non-Standard Operation - ListTableTags","text":""},{"location":"format/namespace/impls/rest/list-table-tags/#rest-route-definition","title":"REST Route Definition","text":"<pre><code>  /v1/table/{id}/tags/list:\n    parameters:\n      - $ref: '#/components/parameters/id'\n      - $ref: '#/components/parameters/delimiter'\n      - name: 'page_token'\n        in: query\n        schema:\n          $ref: '#/components/schemas/PageToken'\n      - name: 'limit'\n        in: query\n        schema:\n          $ref: '#/components/schemas/PageLimit'\n    get:\n      tags:\n        - Table\n        - Tag\n        - Metadata\n      summary: List all tags for a table\n      operationId: ListTableTags\n      description: |\n        List all tags that have been created for table `id`.\n        Returns a map of tag names to their corresponding version numbers and metadata.\n\n        REST NAMESPACE ONLY\n        REST namespace uses GET to perform this operation without a request body.\n        It passes in the `ListTableTagsRequest` information in the following way:\n        - `id`: pass through path parameter of the same name\n        - `page_token`: pass through query parameter of the same name\n        - `limit`: pass through query parameter of the same name\n      responses:\n        200:\n          $ref: '#/components/responses/ListTableTagsResponse'\n        400:\n          $ref: '#/components/responses/BadRequestErrorResponse'\n        401:\n          $ref: '#/components/responses/UnauthorizedErrorResponse'\n        403:\n          $ref: '#/components/responses/ForbiddenErrorResponse'\n        404:\n          $ref: '#/components/responses/NotFoundErrorResponse'\n        503:\n          $ref: '#/components/responses/ServiceUnavailableErrorResponse'\n        5XX:\n          $ref: '#/components/responses/ServerErrorResponse'\n</code></pre>"},{"location":"format/namespace/impls/rest/list-tables/","title":"REST Implementation Non-Standard Operation - ListTables","text":""},{"location":"format/namespace/impls/rest/list-tables/#rest-route-definition","title":"REST Route Definition","text":"<pre><code>  /v1/namespace/{id}/table/list:\n    parameters:\n      - $ref: '#/components/parameters/id'\n      - $ref: '#/components/parameters/delimiter'\n      - name: 'page_token'\n        in: query\n        schema:\n          $ref: '#/components/schemas/PageToken'\n      - name: 'limit'\n        in: query\n        schema:\n          $ref: '#/components/schemas/PageLimit'\n    get:\n      tags:\n        - Namespace\n        - Table\n        - Metadata\n      summary: List tables in a namespace\n      operationId: ListTables\n      description: |\n        List all child table names of the parent namespace `id`.\n\n        REST NAMESPACE ONLY\n        REST namespace uses GET to perform this operation without a request body.\n        It passes in the `ListTablesRequest` information in the following way:\n        - `id`: pass through path parameter of the same name\n        - `page_token`: pass through query parameter of the same name\n        - `limit`: pass through query parameter of the same name\n      responses:\n        200:\n          $ref: '#/components/responses/ListTablesResponse'\n        400:\n          $ref: '#/components/responses/BadRequestErrorResponse'\n        401:\n          $ref: '#/components/responses/UnauthorizedErrorResponse'\n        403:\n          $ref: '#/components/responses/ForbiddenErrorResponse'\n        404:\n          $ref: '#/components/responses/NotFoundErrorResponse'\n        406:\n          $ref: '#/components/responses/UnsupportedOperationErrorResponse'\n        503:\n          $ref: '#/components/responses/ServiceUnavailableErrorResponse'\n        5XX:\n          $ref: '#/components/responses/ServerErrorResponse'\n</code></pre>"},{"location":"format/namespace/impls/rest/merge-insert-into-table/","title":"REST Implementation Non-Standard Operation - MergeInsertIntoTable","text":""},{"location":"format/namespace/impls/rest/merge-insert-into-table/#rest-route-definition","title":"REST Route Definition","text":"<pre><code>  /v1/table/{id}/merge_insert:\n    parameters:\n      - $ref: '#/components/parameters/id'\n      - $ref: '#/components/parameters/delimiter'\n      - name: \"on\"\n        in: query\n        description: Column name to use for matching rows (required)\n        required: true\n        schema:\n          type: string\n      - name: \"when_matched_update_all\"\n        in: query\n        description: Update all columns when rows match\n        required: false\n        schema:\n          type: boolean\n          default: false\n      - name: \"when_matched_update_all_filt\"\n        in: query\n        description: The row is updated (similar to UpdateAll) only for rows where the SQL expression evaluates to true\n        required: false\n        schema:\n          type: string\n      - name: \"when_not_matched_insert_all\"\n        in: query\n        description: Insert all columns when rows don't match\n        required: false\n        schema:\n          type: boolean\n          default: false\n      - name: \"when_not_matched_by_source_delete\"\n        in: query\n        description: Delete all rows from target table that don't match a row in the source table\n        required: false\n        schema:\n          type: boolean\n          default: false\n      - name: \"when_not_matched_by_source_delete_filt\"\n        in: query\n        description: Delete rows from the target table if there is no match AND the SQL expression evaluates to true\n        schema:\n          type: string\n    post:\n      tags:\n        - Table\n        - Data\n      summary: Merge insert (upsert) records into a table\n      operationId: MergeInsertIntoTable\n      description: |\n        Performs a merge insert (upsert) operation on table `id`.\n        This operation updates existing rows\n        based on a matching column and inserts new rows that don't match.\n        It returns the number of rows inserted and updated.\n\n        REST NAMESPACE ONLY\n        REST namespace uses Arrow IPC stream as the request body.\n        It passes in the `MergeInsertIntoTableRequest` information in the following way:\n        - `id`: pass through path parameter of the same name\n        - `on`: pass through query parameter of the same name\n        - `when_matched_update_all`: pass through query parameter of the same name\n        - `when_matched_update_all_filt`: pass through query parameter of the same name\n        - `when_not_matched_insert_all`: pass through query parameter of the same name\n        - `when_not_matched_by_source_delete`: pass through query parameter of the same name\n        - `when_not_matched_by_source_delete_filt`: pass through query parameter of the same name\n      requestBody:\n        description: Arrow IPC stream containing the records to merge\n        content:\n          application/vnd.apache.arrow.stream:\n            schema:\n              type: string\n              format: binary\n        required: true\n      responses:\n        200:\n          $ref: '#/components/responses/MergeInsertIntoTableResponse'\n        400:\n          $ref: '#/components/responses/BadRequestErrorResponse'\n        401:\n          $ref: '#/components/responses/UnauthorizedErrorResponse'\n        403:\n          $ref: '#/components/responses/ForbiddenErrorResponse'\n        404:\n          $ref: '#/components/responses/NotFoundErrorResponse'\n        503:\n          $ref: '#/components/responses/ServiceUnavailableErrorResponse'\n        5XX:\n          $ref: '#/components/responses/ServerErrorResponse'\n</code></pre>"},{"location":"format/namespace/operations/","title":"Namespace Operations","text":"<p>The Lance Namespace Specification defines a list of operations that can be performed against any Lance namespace:</p> Operation ID Current Version Namespace Table Index Metadata Data Transaction CreateNamespace 1 \u2713 \u2713 ListNamespaces 1 \u2713 \u2713 DescribeNamespace 1 \u2713 \u2713 DropNamespace 1 \u2713 \u2713 NamespaceExists 1 \u2713 \u2713 ListTables 1 \u2713 \u2713 \u2713 RegisterTable 1 \u2713 \u2713 DescribeTable 1 \u2713 \u2713 TableExists 1 \u2713 \u2713 DropTable 1 \u2713 \u2713 DeregisterTable 1 \u2713 \u2713 InsertIntoTable 1 \u2713 \u2713 MergeInsertIntoTable 1 \u2713 \u2713 UpdateTable 1 \u2713 \u2713 DeleteFromTable 1 \u2713 \u2713 QueryTable 1 \u2713 \u2713 CountTableRows 1 \u2713 \u2713 CreateTable 1 \u2713 \u2713 CreateTableIndex 1 \u2713 \u2713 \u2713 ListTableIndices 1 \u2713 \u2713 \u2713 DescribeTableIndexStats 1 \u2713 \u2713 \u2713 RestoreTable 1 \u2713 \u2713 ListTableVersions 1 \u2713 \u2713 ExplainTableQueryPlan 1 \u2713 \u2713 AnalyzeTableQueryPlan 1 \u2713 \u2713 AlterTableAddColumns 1 \u2713 \u2713 AlterTableAlterColumns 1 \u2713 \u2713 AlterTableDropColumns 1 \u2713 \u2713 GetTableStats 1 \u2713 \u2713 ListTableTags 1 \u2713 \u2713 GetTableTagVersion 1 \u2713 \u2713 CreateTableTag 1 \u2713 \u2713 DeleteTableTag 1 \u2713 \u2713 UpdateTableTag 1 \u2713 \u2713 DropTableIndex 1 \u2713 \u2713 \u2713 DescribeTransaction 1 \u2713 \u2713 AlterTransaction 1 \u2713 \u2713"},{"location":"format/namespace/operations/#recommended-basic-operations","title":"Recommended Basic Operations","text":"<p>To have a functional basic namespace implementation, the following metadata operations are recommended as a minimum:</p> <p>Namespace Metadata Operations:</p> <ul> <li>CreateNamespace - Create a new namespace</li> <li>ListNamespaces - List available namespaces</li> <li>DescribeNamespace - Get namespace details</li> <li>DropNamespace - Remove a namespace</li> </ul> <p>Table Metadata Operations:</p> <ul> <li>CreateEmptyTable - Create an empty table (metadata only)</li> <li>ListTables - List tables in a namespace</li> <li>DescribeTable - Get table details</li> <li>DropTable - Remove a table</li> </ul> <p>These operations provide the foundational metadata management capabilities needed for namespace and table administration without requiring data or index operation support. With the namespace able to provide basic information about the table, the Lance SDK can be used to fulfill the other operations.</p>"},{"location":"format/namespace/operations/#operation-versioning","title":"Operation Versioning","text":"<p>When backwards incompatible change is introduced, a new operation version needs to be created, with a naming convention of <code>&lt;operationId&gt;V&lt;version&gt;</code>, for example <code>ListNamespacesV2</code>, <code>DescribeTableV3</code>, etc.</p>"},{"location":"format/namespace/operations/#operation-request-and-response-schema","title":"Operation Request and Response Schema","text":"<p>In general, each operation has a request and response. The request and response schema is defined using JSON schema in the  <code>components/schemas</code> section of the OpenAPI specification.</p> <p>Note</p> <p>For exceptions to this rule, see the Notes section of the operations</p>"},{"location":"format/namespace/operations/#error-response-model","title":"Error Response Model","text":"<p>All error responses follow the JSON error response model based on RFC-7807:</p> <pre><code>    ErrorResponse:\n      type: object\n      description: Common JSON error response model\n      properties:\n        error:\n          type: string\n          description: a brief, human-readable message about the error\n          example: Incorrect username or password\n        code:\n          type: integer\n          minimum: 400\n          maximum: 600\n          description: |\n            HTTP style response code, where 4XX represents client side errors \n            and 5XX represents server side errors.\n\n            For implementations that uses HTTP (e.g. REST namespace),\n            this field can be optional in favor of the HTTP response status code.\n            In case both values exist and do not match, the HTTP response status code should be used.\n          example: 404\n        type:\n          type: string\n          description: |\n            An optional type identifier string for the error.\n            This allows the implementation to specify their internal error type,\n            which could be more detailed than the HTTP standard status code.\n          example: /errors/incorrect-user-pass\n        detail:\n          type: string\n          description: |\n            an optional human-readable explanation of the error.\n            This can be used to record information such as stack trace.\n          example: Authentication failed due to incorrect username or password\n        instance:\n          type: string\n          description: |\n            a string that identifies the specific occurrence of the error.\n            This can be a URI, a request or response ID, \n            or anything that the implementation can recognize to trace specific occurrence of the error.\n          example: /login/log/abc123\n</code></pre>"},{"location":"format/namespace/operations/#http-status-codes-and-responses","title":"HTTP Status Codes and Responses","text":""},{"location":"format/namespace/operations/#400-bad-request-error-response","title":"400 - Bad Request Error Response","text":"<pre><code>    BadRequestErrorResponse:\n      description:\n        Indicates a bad request error. It could be caused by an unexpected request\n        body format or other forms of request validation failure, such as invalid json.\n        Usually serves application/json content, although in some cases simple text/plain content might\n        be returned by the server's middleware.\n      content:\n        application/json:\n          schema:\n            $ref: '#/components/schemas/ErrorResponse'\n          example: {\n            \"type\": \"/errors/bad-request\",\n            \"title\": \"Malformed request\",\n            \"status\": 400,\n            \"detail\": \"\",\n            \"instance\": \"/v1/namespaces\"\n          }\n</code></pre>"},{"location":"format/namespace/operations/#401-unauthorized-error-response","title":"401 - Unauthorized Error Response","text":"<pre><code>    UnauthorizedErrorResponse:\n      description: Unauthorized. The request lacks valid authentication credentials for the operation.\n      content:\n        application/json:\n          schema:\n            $ref: '#/components/schemas/ErrorResponse'\n          example: {\n            \"type\": \"/errors/unauthorized-request\",\n            \"title\": \"No valid authentication credentials for the operation\",\n            \"status\": 401,\n            \"detail\": \"\",\n            \"instance\": \"/v1/namespaces\"\n          }\n</code></pre>"},{"location":"format/namespace/operations/#403-forbidden-error-response","title":"403 - Forbidden Error Response","text":"<pre><code>    ForbiddenErrorResponse:\n      description: Forbidden. Authenticated user does not have the necessary permissions.\n      content:\n        application/json:\n          schema:\n            $ref: '#/components/schemas/ErrorResponse'\n          example: {\n            \"type\": \"/errors/forbidden-request\",\n            \"title\": \"Not authorized to make this request\",\n            \"status\": 403,\n            \"detail\": \"\",\n            \"instance\": \"/v1/namespaces\"\n          }\n</code></pre>"},{"location":"format/namespace/operations/#404-not-found-error-response","title":"404 - Not Found Error Response","text":"<pre><code>    NotFoundErrorResponse:\n      description:\n        A server-side problem that means can not find the specified resource.\n      content:\n        application/json:\n          schema:\n            $ref: '#/components/schemas/ErrorResponse'\n          example: {\n            \"type\": \"/errors/not-found-error\",\n            \"title\": \"Not found Error\",\n            \"status\": 404,\n            \"detail\": \"\",\n            \"instance\": \"/v1/namespaces/{ns}\"\n          }\n</code></pre>"},{"location":"format/namespace/operations/#406-unsupported-operation-error-response","title":"406 - Unsupported Operation Error Response","text":"<pre><code>    UnsupportedOperationErrorResponse:\n      description: Not Acceptable / Unsupported Operation. The server does not support this operation.\n      content:\n        application/json:\n          schema:\n            $ref: '#/components/schemas/ErrorResponse'\n          example: {\n            \"type\": \"/errors/unsupported-operation\",\n            \"title\": \"The server does not support this operation\",\n            \"status\": 406,\n            \"detail\": \"\",\n            \"instance\": \"/v1/namespaces\"\n          }\n</code></pre>"},{"location":"format/namespace/operations/#409-conflict-error-response","title":"409 - Conflict Error Response","text":"<pre><code>    ConflictErrorResponse:\n      description: The request conflicts with the current state of the target resource.\n      content:\n        application/json:\n          schema:\n            $ref: '#/components/schemas/ErrorResponse'\n          example: {\n            \"type\": \"/errors/conflict\",\n            \"title\": \"The namespace has been concurrently modified\",\n            \"status\": 409,\n            \"detail\": \"\",\n            \"instance\": \"/v1/namespaces/{ns}\"\n          }\n</code></pre>"},{"location":"format/namespace/operations/#503-service-unavailable-error-response","title":"503 - Service Unavailable Error Response","text":"<pre><code>    ServiceUnavailableErrorResponse:\n      description:\n        The service is not ready to handle the request. The client should wait and retry.\n        The service may additionally send a Retry-After header to indicate when to retry.\n      content:\n        application/json:\n          schema:\n            $ref: '#/components/schemas/ErrorResponse'\n          example: {\n            \"type\": \"/errors/service-unavailable\",\n            \"title\": \"Slow down\",\n            \"status\": 503,\n            \"detail\": \"\",\n            \"instance\": \"/v1/namespaces\"\n          }\n</code></pre>"},{"location":"format/namespace/operations/#5xx-server-error-response","title":"5XX - Server Error Response","text":"<pre><code>    ServerErrorResponse:\n      description:\n        A server-side problem that might not be addressable from the client\n        side. Used for server 5xx errors without more specific documentation in\n        individual routes.\n      content:\n        application/json:\n          schema:\n            $ref: '#/components/schemas/ErrorResponse'\n          example: {\n            \"type\": \"/errors/server-error\",\n            \"title\": \"Internal Server Error\",\n            \"status\": 500,\n            \"detail\": \"\",\n            \"instance\": \"/v1/namespaces\"\n          }\n</code></pre>"},{"location":"format/namespace/operations/alter-table-add-columns/","title":"AlterTableAddColumns","text":""},{"location":"format/namespace/operations/alter-table-add-columns/#description","title":"Description","text":"<pre><code>      description: |\n        Add new columns to table `id` using SQL expressions or default values.\n</code></pre>"},{"location":"format/namespace/operations/alter-table-add-columns/#request-schema","title":"Request Schema","text":"<pre><code>    AlterTableAddColumnsRequest:\n      type: object\n      required:\n        - new_columns\n      properties:\n        id:\n          type: array\n          items:\n            type: string\n        new_columns:\n          type: array\n          items:\n            $ref: '#/components/schemas/NewColumnTransform'\n          description: List of new columns to add\n</code></pre>"},{"location":"format/namespace/operations/alter-table-add-columns/#response-schema","title":"Response Schema","text":"<pre><code>    AlterTableAddColumnsResponse:\n      type: object\n      required:\n        - version\n      properties:\n        version:\n          type: integer\n          format: int64\n          minimum: 0\n          description: Version of the table after adding columns\n</code></pre>"},{"location":"format/namespace/operations/alter-table-add-columns/#related-components-schema","title":"Related Components Schema","text":""},{"location":"format/namespace/operations/alter-table-add-columns/#new-column-transform","title":"New Column Transform","text":"<pre><code>    NewColumnTransform:\n      type: object\n      required:\n        - name\n        - expression\n      properties:\n        name:\n          type: string\n          description: Name of the new column\n        expression:\n          type: string\n          description: SQL expression to compute the column value\n</code></pre>"},{"location":"format/namespace/operations/alter-table-alter-columns/","title":"AlterTableAlterColumns","text":""},{"location":"format/namespace/operations/alter-table-alter-columns/#description","title":"Description","text":"<pre><code>      description: |\n        Modify existing columns in table `id`, such as renaming or changing data types.\n</code></pre>"},{"location":"format/namespace/operations/alter-table-alter-columns/#request-schema","title":"Request Schema","text":"<pre><code>    AlterTableAlterColumnsRequest:\n      type: object\n      required:\n        - alterations\n      properties:\n        id:\n          type: array\n          items:\n            type: string\n        alterations:\n          type: array\n          items:\n            $ref: '#/components/schemas/ColumnAlteration'\n          description: List of column alterations to perform\n</code></pre>"},{"location":"format/namespace/operations/alter-table-alter-columns/#response-schema","title":"Response Schema","text":"<pre><code>    AlterTableAlterColumnsResponse:\n      type: object\n      required:\n        - version\n      properties:\n        version:\n          type: integer\n          format: int64\n          minimum: 0\n          description: Version of the table after altering columns\n</code></pre>"},{"location":"format/namespace/operations/alter-table-alter-columns/#related-components-schema","title":"Related Components Schema","text":""},{"location":"format/namespace/operations/alter-table-alter-columns/#column-alteration","title":"Column Alteration","text":"<pre><code>    ColumnAlteration:\n      type: object\n      required:\n        - column\n      properties:\n        column:\n          type: string\n          description: Name of the column to alter\n        rename:\n          type: string\n          description: New name for the column (optional)\n        cast_to:\n          type: string\n          description: New data type to cast the column to (optional)\n</code></pre>"},{"location":"format/namespace/operations/alter-table-drop-columns/","title":"AlterTableDropColumns","text":""},{"location":"format/namespace/operations/alter-table-drop-columns/#description","title":"Description","text":"<pre><code>      description: |\n        Remove specified columns from table `id`.\n</code></pre>"},{"location":"format/namespace/operations/alter-table-drop-columns/#request-schema","title":"Request Schema","text":"<pre><code>    AlterTableDropColumnsRequest:\n      type: object\n      required:\n        - columns\n      properties:\n        id:\n          type: array\n          items:\n            type: string\n        columns:\n          type: array\n          items:\n            type: string\n          description: Names of columns to drop\n</code></pre>"},{"location":"format/namespace/operations/alter-table-drop-columns/#response-schema","title":"Response Schema","text":"<pre><code>    AlterTableDropColumnsResponse:\n      type: object\n      required:\n        - version\n      properties:\n        version:\n          type: integer\n          format: int64\n          minimum: 0\n          description: Version of the table after dropping columns\n</code></pre>"},{"location":"format/namespace/operations/alter-transaction/","title":"AlterTransaction","text":""},{"location":"format/namespace/operations/alter-transaction/#description","title":"Description","text":"<pre><code>      description: |\n        Alter a transaction with a list of actions such as setting status or properties.\n        The server should either succeed and apply all actions, or fail and apply no action.\n</code></pre>"},{"location":"format/namespace/operations/alter-transaction/#request-schema","title":"Request Schema","text":"<p><pre><code>    AlterTransactionRequest:\n      type: object\n      description: |\n        Alter a transaction with a list of actions.\n        The server should either succeed and apply all actions, or fail and apply no action.\n      required:\n        - actions\n      properties:\n        id:\n          type: array\n          items:\n            type: string\n        actions:\n          type: array\n          minItems: 1\n          items:\n            $ref: '#/components/schemas/AlterTransactionAction'\n</code></pre> Supporting action schemas:</p> <p>Set Status Action: <pre><code>      minimum: 0\n\n    InsertIntoTableRequest:\n      type: object\n      description: |\n        Request for inserting records into a table, excluding the Arrow IPC stream.\n</code></pre> Set Property Action: <pre><code>      properties:\n        id:\n          type: array\n          items:\n            type: string\n        mode:\n          type: string\n          enum:\n            - append\n            - overwrite\n</code></pre> Unset Property Action: <pre><code>    MergeInsertIntoTableRequest:\n      type: object\n      description: |\n        Request for merging or inserting records into a table, excluding the Arrow IPC stream.\n      properties:\n        id:\n          type: array\n          items:\n</code></pre></p>"},{"location":"format/namespace/operations/alter-transaction/#response-schema","title":"Response Schema","text":"<pre><code>    AlterTransactionResponse:\n      type: object\n      required:\n        - status\n      properties:\n        status:\n          $ref: '#/components/schemas/TransactionStatus'\n        properties:\n          type: object\n          additionalProperties:\n            type: string\n</code></pre>"},{"location":"format/namespace/operations/alter-transaction/#related-components-schema","title":"Related Components Schema","text":""},{"location":"format/namespace/operations/alter-transaction/#set-status-action","title":"Set Status Action","text":"<pre><code>    AlterTransactionSetStatus:\n      type: object\n      properties:\n        status:\n          $ref: '#/components/schemas/TransactionStatus'\n</code></pre>"},{"location":"format/namespace/operations/alter-transaction/#set-property-action","title":"Set Property Action","text":"<pre><code>    AlterTransactionSetProperty:\n      type: object\n      properties:\n        key:\n          type: string\n        value:\n          type: string\n        mode:\n          $ref: '#/components/schemas/SetPropertyMode'\n</code></pre>"},{"location":"format/namespace/operations/alter-transaction/#unset-property-action","title":"Unset Property Action","text":"<pre><code>    AlterTransactionUnsetProperty:\n      type: object\n      properties:\n        key:\n          type: string\n        mode:\n          $ref: '#/components/schemas/UnsetPropertyMode'\n</code></pre>"},{"location":"format/namespace/operations/analyze-table-query-plan/","title":"AnalyzeTableQueryPlan","text":""},{"location":"format/namespace/operations/analyze-table-query-plan/#description","title":"Description","text":"<pre><code>      description: |\n        Analyze the query execution plan for a query against table `id`.\n        Returns detailed statistics and analysis of the query execution plan.\n</code></pre>"},{"location":"format/namespace/operations/analyze-table-query-plan/#request-schema","title":"Request Schema","text":"<pre><code>    AnalyzeTableQueryPlanRequest:\n      type: object\n      required:\n        - query\n      properties:\n        id:\n          type: array\n          items:\n            type: string\n        query:\n          $ref: '#/components/schemas/QueryTableRequest'\n</code></pre>"},{"location":"format/namespace/operations/analyze-table-query-plan/#response-schema","title":"Response Schema","text":"<pre><code>    AnalyzeTableQueryPlanResponse:\n      type: object\n      required:\n        - analysis\n      properties:\n        analysis:\n          type: string\n          description: Detailed analysis of the query execution plan\n</code></pre>"},{"location":"format/namespace/operations/count-table-rows/","title":"CountTableRows","text":""},{"location":"format/namespace/operations/count-table-rows/#description","title":"Description","text":"<pre><code>      description: |\n        Count the number of rows in table `id`\n</code></pre>"},{"location":"format/namespace/operations/count-table-rows/#request-schema","title":"Request Schema","text":"<pre><code>    CountTableRowsRequest:\n      type: object\n      properties:\n        id:\n          type: array\n          items:\n            type: string\n        version:\n          description: |\n            Version of the table to describe.\n            If not specified, server should resolve it to the latest version.\n          type: integer\n          format: int64\n          minimum: 0\n        filter:\n          description: |\n            SQL filter expression to be applied\n          type: string\n</code></pre>"},{"location":"format/namespace/operations/count-table-rows/#response-schema","title":"Response Schema","text":"<pre><code>    CountTableRowsResponse:\n      type: integer\n      format: int64\n      description: |\n        Response containing the count of rows. \n        Serializes transparently as just the number for backward compatibility.\n      minimum: 0\n</code></pre>"},{"location":"format/namespace/operations/create-empty-table/","title":"CreateEmptyTable","text":"<p>Create an empty table (metadata only) without touching storage.</p>"},{"location":"format/namespace/operations/create-empty-table/#description","title":"Description","text":"<pre><code>      description: |\n        Create an empty table with the given name without touching storage.\n        This is a metadata-only operation that records the table existence and sets up aspects like access control.\n\n        For DirectoryNamespace implementation, this creates a `.lance-reserved` file in the table directory\n        to mark the table's existence without creating actual Lance data files.\n</code></pre>"},{"location":"format/namespace/operations/create-empty-table/#request-schema","title":"Request Schema","text":"<pre><code>    CreateEmptyTableRequest:\n      type: object\n      description: |\n        Request for creating an empty table.\n      properties:\n        id:\n          type: array\n          items:\n            type: string\n        location:\n          type: string\n          description: |\n            Optional storage location for the table.\n            If not provided, the namespace implementation should determine the table location.\n        properties:\n          type: object\n          additionalProperties:\n            type: string\n</code></pre>"},{"location":"format/namespace/operations/create-empty-table/#response-schema","title":"Response Schema","text":"<pre><code>    CreateEmptyTableResponse:\n      type: object\n      description: |\n        Response for creating an empty table.\n      properties:\n        location:\n          type: string\n        properties:\n          type: object\n          additionalProperties:\n            type: string\n        storage_options:\n          type: object\n          description: |\n            Configuration options to be used to access storage. The available\n            options depend on the type of storage in use. These will be\n            passed directly to Lance to initialize storage access.\n          additionalProperties:\n            type: string\n</code></pre>"},{"location":"format/namespace/operations/create-namespace/","title":"CreateNamespace","text":""},{"location":"format/namespace/operations/create-namespace/#description","title":"Description","text":"<pre><code>      description: |\n        Create new namespace `id`.\n\n        During the creation process, the implementation may modify user-provided `properties`, \n        such as adding additional properties like `created_at` to user-provided properties, \n        omitting any specific property, or performing actions based on any property value.\n</code></pre>"},{"location":"format/namespace/operations/create-namespace/#request-schema","title":"Request Schema","text":"<pre><code>    CreateNamespaceRequest:\n      type: object\n      properties:\n        id:\n          type: array\n          items:\n            type: string\n        mode:\n          type: string\n          description: |\n            There are three modes when trying to create a namespace,\n            to differentiate the behavior when a namespace of the same name already exists:\n              * create: the operation fails with 409.\n              * exist_ok: the operation succeeds and the existing namespace is kept.\n              * overwrite: the existing namespace is dropped and a new empty namespace with this name is created.\n          enum:\n            - create\n            - exist_ok\n            - overwrite\n        properties:\n          type: object\n          additionalProperties:\n            type: string\n</code></pre>"},{"location":"format/namespace/operations/create-namespace/#response-schema","title":"Response Schema","text":"<pre><code>    CreateNamespaceResponse:\n      type: object\n      properties:\n        properties:\n          description: |\n            Properties after the namespace is created.\n\n            If the server does not support namespace properties, it should return null for this field.\n            If namespace properties are supported, but none are set, it should return an empty object.\n          type: object\n          additionalProperties:\n            type: string\n</code></pre>"},{"location":"format/namespace/operations/create-table-index/","title":"CreateTableIndex","text":""},{"location":"format/namespace/operations/create-table-index/#description","title":"Description","text":"<pre><code>      description: |\n        Create an index on a table column for faster search operations.\n        Supports vector indexes (IVF_FLAT, IVF_HNSW_SQ, IVF_PQ, etc.) and scalar indexes (BTREE, BITMAP, FTS, etc.).\n        Index creation is handled asynchronously. \n        Use the `ListTableIndices` and `DescribeTableIndexStats` operations to monitor index creation progress.\n</code></pre>"},{"location":"format/namespace/operations/create-table-index/#request-schema","title":"Request Schema","text":"<pre><code>    CreateTableIndexRequest:\n      type: object\n      required:\n        - column\n        - index_type\n      properties:\n        id:\n          type: array\n          items:\n            type: string\n        column:\n          type: string\n          description: Name of the column to create index on\n        index_type:\n          type: string\n          enum: \n            - BTREE\n            - BITMAP\n            - LABEL_LIST\n            - IVF_FLAT\n            - IVF_PQ\n            - IVF_HNSW_SQ\n            - FTS\n          description: Type of index to create\n        metric_type:\n          type: string\n          enum: \n            - l2\n            - cosine\n            - dot\n          nullable: true\n          description: Distance metric type for vector indexes\n        with_position:\n          type: boolean\n          nullable: true\n          description: Optional FTS parameter for position tracking\n        base_tokenizer:\n          type: string\n          nullable: true\n          description: Optional FTS parameter for base tokenizer\n        language:\n          type: string\n          nullable: true\n          description: Optional FTS parameter for language\n        max_token_length:\n          type: integer\n          nullable: true\n          minimum: 0\n          description: Optional FTS parameter for maximum token length\n        lower_case:\n          type: boolean\n          nullable: true\n          description: Optional FTS parameter for lowercase conversion\n        stem:\n          type: boolean\n          nullable: true\n          description: Optional FTS parameter for stemming\n        remove_stop_words:\n          type: boolean\n          nullable: true\n          description: Optional FTS parameter for stop word removal\n        ascii_folding:\n          type: boolean\n          nullable: true\n          description: Optional FTS parameter for ASCII folding\n</code></pre>"},{"location":"format/namespace/operations/create-table-index/#response-schema","title":"Response Schema","text":"<pre><code>    CreateTableIndexResponse:\n      type: object\n      required:\n        - location\n      properties:\n        id:\n          type: array\n          items:\n            type: string\n        location:\n          type: string\n          description: Table location (usually empty)\n        properties:\n          type: object\n          additionalProperties:\n            type: string\n          description: Additional properties (usually empty)\n</code></pre>"},{"location":"format/namespace/operations/create-table-tag/","title":"CreateTableTag","text":""},{"location":"format/namespace/operations/create-table-tag/#description","title":"Description","text":"<pre><code>      description: |\n        Create a new tag for table `id` that points to a specific version.\n</code></pre>"},{"location":"format/namespace/operations/create-table-tag/#request-schema","title":"Request Schema","text":"<pre><code>    CreateTableTagRequest:\n      type: object\n      required:\n        - tag\n        - version\n      properties:\n        id:\n          type: array\n          items:\n            type: string\n        tag:\n          type: string\n          description: Name of the tag to create\n        version:\n          type: integer\n          format: int64\n          minimum: 0\n          description: Version number for the tag to point to\n</code></pre>"},{"location":"format/namespace/operations/create-table/","title":"CreateTable","text":""},{"location":"format/namespace/operations/create-table/#description","title":"Description","text":"<pre><code>      description: |\n        Create table `id` in the namespace with the given data in Arrow IPC stream.\n\n        The schema of the Arrow IPC stream is used as the table schema.    \n        If the stream is empty, the API creates a new empty table.\n</code></pre>"},{"location":"format/namespace/operations/create-table/#request-schema","title":"Request Schema","text":"<pre><code>    CreateTableRequest:\n      type: object\n      description: |\n        Request for creating a table, excluding the Arrow IPC stream.\n      properties:\n        id:\n          type: array\n          items:\n            type: string\n        location:\n          type: string\n        mode:\n          type: string\n          description: |\n            There are three modes when trying to create a table,\n            to differentiate the behavior when a table of the same name already exists:\n              * create: the operation fails with 409.\n              * exist_ok: the operation succeeds and the existing table is kept.\n              * overwrite: the existing table is dropped and a new table with this name is created.\n          enum:\n            - create\n            - exist_ok\n            - overwrite\n        properties:\n          type: object\n          additionalProperties:\n            type: string\n</code></pre>"},{"location":"format/namespace/operations/create-table/#response-schema","title":"Response Schema","text":"<pre><code>    CreateTableResponse:\n      type: object\n      properties:\n        location:\n          type: string\n        version:\n          type: integer\n          format: int64\n          minimum: 0\n        properties:\n          type: object\n          additionalProperties:\n            type: string\n        storage_options:\n          type: object\n          description: |\n            Configuration options to be used to access storage. The available\n            options depend on the type of storage in use. These will be\n            passed directly to Lance to initialize storage access.\n          additionalProperties:\n            type: string\n</code></pre>"},{"location":"format/namespace/operations/delete-from-table/","title":"DeleteFromTable","text":""},{"location":"format/namespace/operations/delete-from-table/#description","title":"Description","text":"<pre><code>      description: |\n        Delete rows from table `id`.\n</code></pre>"},{"location":"format/namespace/operations/delete-from-table/#request-schema","title":"Request Schema","text":"<pre><code>    DeleteFromTableRequest:\n      type: object\n      description: |\n        Delete data from table based on a SQL predicate.\n        Returns the number of rows that were deleted.\n      required:\n        - predicate\n      properties:\n        id:\n          type: array\n          items:\n            type: string\n          description: The namespace identifier\n        predicate:\n          type: string\n          description: SQL predicate to filter rows for deletion\n</code></pre>"},{"location":"format/namespace/operations/delete-from-table/#response-schema","title":"Response Schema","text":"<pre><code>    DeleteFromTableResponse:\n      type: object\n      required:\n        - version\n      properties:\n        version:\n          type: integer\n          format: int64\n          description: The commit version associated with the operation\n          minimum: 0\n</code></pre>"},{"location":"format/namespace/operations/delete-table-tag/","title":"DeleteTableTag","text":""},{"location":"format/namespace/operations/delete-table-tag/#description","title":"Description","text":"<pre><code>      description: |\n        Delete an existing tag from table `id`.\n</code></pre>"},{"location":"format/namespace/operations/delete-table-tag/#request-schema","title":"Request Schema","text":"<pre><code>    DeleteTableTagRequest:\n      type: object\n      required:\n        - tag\n      properties:\n        id:\n          type: array\n          items:\n            type: string\n        tag:\n          type: string\n          description: Name of the tag to delete\n</code></pre>"},{"location":"format/namespace/operations/deregister-table/","title":"DeregisterTable","text":""},{"location":"format/namespace/operations/deregister-table/#description","title":"Description","text":"<pre><code>      description: |\n        Deregister table `id` from its namespace.\n</code></pre>"},{"location":"format/namespace/operations/deregister-table/#request-schema","title":"Request Schema","text":"<pre><code>    DeregisterTableRequest:\n      type: object\n      description: |\n         The table content remains available in the storage.\n      properties:\n        id:\n          type: array\n          items:\n            type: string\n</code></pre>"},{"location":"format/namespace/operations/deregister-table/#response-schema","title":"Response Schema","text":"<pre><code>    DeregisterTableResponse:\n      type: object\n      properties:\n        id:\n          type: array\n          items:\n            type: string\n        location:\n          type: string\n        properties:\n          type: object\n          additionalProperties:\n            type: string\n</code></pre>"},{"location":"format/namespace/operations/describe-namespace/","title":"DescribeNamespace","text":""},{"location":"format/namespace/operations/describe-namespace/#description","title":"Description","text":"<pre><code>      description: |\n        Describe the detailed information for namespace `id`.\n</code></pre>"},{"location":"format/namespace/operations/describe-namespace/#request-schema","title":"Request Schema","text":"<pre><code>    DescribeNamespaceRequest:\n      type: object\n      properties:\n        id:\n          type: array\n          items:\n            type: string\n</code></pre>"},{"location":"format/namespace/operations/describe-namespace/#response-schema","title":"Response Schema","text":"<pre><code>    DescribeNamespaceResponse:\n      type: object\n      properties:\n        properties:\n          type: object\n          description:\n            Properties stored on the namespace, if supported by the server.\n            If the server does not support namespace properties, it should return null for this field.\n            If namespace properties are supported, but none are set, it should return an empty object.\n          additionalProperties:\n            type: string\n          example: { \"owner\": \"Ralph\", 'created_at': '1452120468' }\n          default: { }\n          nullable: true\n</code></pre>"},{"location":"format/namespace/operations/describe-table-index-stats/","title":"DescribeTableIndexStats","text":""},{"location":"format/namespace/operations/describe-table-index-stats/#description","title":"Description","text":"<pre><code>      description: |\n        Get statistics for a specific index on a table. Returns information about\n        the index type, distance type (for vector indices), and row counts.\n</code></pre>"},{"location":"format/namespace/operations/describe-table-index-stats/#request-schema","title":"Request Schema","text":"<pre><code>    DescribeTableIndexStatsRequest:\n      type: object\n      properties:\n        id:\n          type: array\n          items:\n            type: string\n        version:\n          type: integer\n          format: int64\n          minimum: 0\n          nullable: true\n          description: Optional table version to get stats for\n        index_name:\n          type: string\n          description: Name of the index\n</code></pre>"},{"location":"format/namespace/operations/describe-table-index-stats/#response-schema","title":"Response Schema","text":"<pre><code>    DescribeTableIndexStatsResponse:\n      type: object\n      properties:\n        distance_type:\n          type: string\n          nullable: true\n          description: Distance type for vector indexes\n        index_type:\n          type: string\n          nullable: true\n          description: Type of the index\n        num_indexed_rows:\n          type: integer\n          format: int64\n          minimum: 0\n          nullable: true\n          description: Number of indexed rows\n        num_unindexed_rows:\n          type: integer\n          format: int64\n          minimum: 0\n          nullable: true\n          description: Number of unindexed rows\n</code></pre>"},{"location":"format/namespace/operations/describe-table/","title":"DescribeTable","text":""},{"location":"format/namespace/operations/describe-table/#description","title":"Description","text":"<pre><code>      description: |\n        Describe the detailed information for table `id`.\n</code></pre>"},{"location":"format/namespace/operations/describe-table/#request-schema","title":"Request Schema","text":"<pre><code>    DescribeTableRequest:\n      type: object\n      properties:\n        id:\n          type: array\n          items:\n            type: string\n        version:\n          description: |\n            Version of the table to describe.\n            If not specified, server should resolve it to the latest version.\n          type: integer\n          format: int64\n          minimum: 0\n</code></pre>"},{"location":"format/namespace/operations/describe-table/#response-schema","title":"Response Schema","text":"<pre><code>    DescribeTableResponse:\n      type: object\n      properties:\n        version:\n          type: integer\n          format: int64\n          minimum: 0\n        location:\n          type: string\n        schema:\n          $ref: '#/components/schemas/JsonArrowSchema'\n        properties:\n          type: object\n          additionalProperties:\n            type: string\n        storage_options:\n          type: object\n          description: |\n            Configuration options to be used to access storage. The available\n            options depend on the type of storage in use. These will be\n            passed directly to Lance to initialize storage access.\n          additionalProperties:\n            type: string\n</code></pre>"},{"location":"format/namespace/operations/describe-table/#related-components-schema","title":"Related Components Schema","text":""},{"location":"format/namespace/operations/describe-table/#json-arrow-schema","title":"Json Arrow Schema","text":"<pre><code>    JsonArrowSchema:\n      type: object\n      description: |\n        JSON representation of a Apache Arrow schema.\n      required:\n      - fields\n      properties:\n        fields:\n          type: array\n          items:\n            $ref: '#/components/schemas/JsonArrowField'\n        metadata:\n          type: object\n          additionalProperties:\n            type: string\n          propertyNames:\n            type: string\n</code></pre>"},{"location":"format/namespace/operations/describe-table/#json-arrow-schema_1","title":"Json Arrow Schema","text":"<pre><code>    JsonArrowField:\n      type: object\n      description: |\n        JSON representation of an Apache Arrow field.\n      required:\n      - name\n      - type\n      - nullable\n      properties:\n        metadata:\n          type: object\n          additionalProperties:\n            type: string\n          propertyNames:\n            type: string\n        name:\n          type: string\n        nullable:\n          type: boolean\n        type:\n          $ref: '#/components/schemas/JsonArrowDataType'\n</code></pre>"},{"location":"format/namespace/operations/describe-transaction/","title":"DescribeTransaction","text":""},{"location":"format/namespace/operations/describe-transaction/#description","title":"Description","text":"<pre><code>      description: |\n        Return a detailed information for a given transaction\n</code></pre>"},{"location":"format/namespace/operations/describe-transaction/#request-schema","title":"Request Schema","text":"<pre><code>    DescribeTransactionRequest:\n      type: object\n      properties:\n        id:\n          type: array\n          items:\n            type: string\n</code></pre>"},{"location":"format/namespace/operations/describe-transaction/#response-schema","title":"Response Schema","text":"<pre><code>    DescribeTransactionResponse:\n      type: object\n      required:\n        - status\n      properties:\n        status:\n          $ref: '#/components/schemas/TransactionStatus'\n        properties:\n          type: object\n          additionalProperties:\n            type: string\n</code></pre>"},{"location":"format/namespace/operations/drop-namespace/","title":"DropNamespace","text":""},{"location":"format/namespace/operations/drop-namespace/#description","title":"Description","text":"<pre><code>      description: |\n        Drop namespace `id` from its parent namespace.\n</code></pre>"},{"location":"format/namespace/operations/drop-namespace/#request-schema","title":"Request Schema","text":"<pre><code>    DropNamespaceRequest:\n      type: object\n      properties:\n        id:\n          type: array\n          items:\n            type: string\n        mode:\n          type: string\n          description: |\n            The mode for dropping a namespace, deciding the server behavior when the namespace to drop is not found.\n            - FAIL (default): the server must return 400 indicating the namespace to drop does not exist.\n            - SKIP: the server must return 204 indicating the drop operation has succeeded.\n          enum:\n            - SKIP\n            - FAIL\n        behavior:\n          type: string\n          description: |\n            The behavior for dropping a namespace.\n            - RESTRICT (default): the namespace should not contain any table or child namespace when drop is initiated.\n                If tables are found, the server should return error and not drop the namespace.\n            - CASCADE: all tables and child namespaces in the namespace are dropped before the namespace is dropped.\n          enum:\n            - RESTRICT\n            - CASCADE\n</code></pre>"},{"location":"format/namespace/operations/drop-namespace/#response-schema","title":"Response Schema","text":"<pre><code>    DropNamespaceResponse:\n      type: object\n      properties:\n        properties:\n          type: object\n          additionalProperties:\n            type: string\n        transactionId:\n          description: |\n            If present, indicating the operation is long running and should be tracked using GetTransaction\n          type: array\n          items:\n            type: string\n</code></pre>"},{"location":"format/namespace/operations/drop-table-index/","title":"DropTableIndex","text":""},{"location":"format/namespace/operations/drop-table-index/#description","title":"Description","text":"<pre><code>      description: |\n        Drop the specified index from table `id`.\n</code></pre>"},{"location":"format/namespace/operations/drop-table-index/#request-schema","title":"Request Schema","text":"<pre><code>    DropTableIndexRequest:\n      type: object\n      required:\n        - id\n        - index_name\n      properties:\n        id:\n          type: array\n          items:\n            type: string\n        index_name:\n          type: string\n          description: Name of the index to drop\n</code></pre>"},{"location":"format/namespace/operations/drop-table-index/#response-schema","title":"Response Schema","text":"<pre><code>    DropTableIndexResponse:\n      type: object\n      properties:\n        version:\n          type: integer\n          format: int64\n          minimum: 0\n          description: Version of the table after dropping the index\n</code></pre>"},{"location":"format/namespace/operations/drop-table/","title":"DropTable","text":""},{"location":"format/namespace/operations/drop-table/#description","title":"Description","text":"<pre><code>      description: |\n        Drop table `id` and delete its data.\n</code></pre>"},{"location":"format/namespace/operations/drop-table/#request-schema","title":"Request Schema","text":"<pre><code>    DropTableRequest:\n      type: object\n      description: |\n        If the table and its data can be immediately deleted, return information of the deleted table.\n        Otherwise, return a transaction ID that client can use to track deletion progress.\n      properties:\n        id:\n          type: array\n          items:\n            type: string\n</code></pre>"},{"location":"format/namespace/operations/drop-table/#response-schema","title":"Response Schema","text":"<pre><code>    DropTableResponse:\n      type: object\n      properties:\n        id:\n          type: array\n          items:\n            type: string\n        location:\n          type: string\n        properties:\n          type: object\n          additionalProperties:\n            type: string\n        transactionId:\n          type: array\n          description: |\n            If present, indicating the operation is long running and should be tracked using GetTransaction\n          items:\n            type: string\n</code></pre>"},{"location":"format/namespace/operations/explain-table-query-plan/","title":"ExplainTableQueryPlan","text":""},{"location":"format/namespace/operations/explain-table-query-plan/#description","title":"Description","text":"<pre><code>      description: |\n        Get the query execution plan for a query against table `id`.\n        Returns a human-readable explanation of how the query will be executed.\n</code></pre>"},{"location":"format/namespace/operations/explain-table-query-plan/#request-schema","title":"Request Schema","text":"<pre><code>    ExplainTableQueryPlanRequest:\n      type: object\n      required:\n        - query\n      properties:\n        id:\n          type: array\n          items:\n            type: string\n        query:\n          $ref: '#/components/schemas/QueryTableRequest'\n        verbose:\n          type: boolean\n          default: false\n          description: Whether to return verbose explanation\n</code></pre>"},{"location":"format/namespace/operations/explain-table-query-plan/#response-schema","title":"Response Schema","text":"<pre><code>    ExplainTableQueryPlanResponse:\n      type: object\n      required:\n        - plan\n      properties:\n        plan:\n          type: string\n          description: Human-readable query execution plan\n</code></pre>"},{"location":"format/namespace/operations/get-table-stats/","title":"GetTableStats","text":""},{"location":"format/namespace/operations/get-table-stats/#description","title":"Description","text":"<pre><code>      description: |\n        Get statistics for table `id`, including row counts, data sizes, and column statistics.\n</code></pre>"},{"location":"format/namespace/operations/get-table-stats/#request-schema","title":"Request Schema","text":"<pre><code>    GetTableStatsRequest:\n      type: object\n      properties:\n        id:\n          type: array\n          items:\n            type: string\n</code></pre>"},{"location":"format/namespace/operations/get-table-stats/#response-schema","title":"Response Schema","text":"<pre><code>    GetTableStatsResponse:\n      type: object\n      required:\n        - num_rows\n        - size_bytes\n      properties:\n        num_rows:\n          type: integer\n          format: int64\n          minimum: 0\n          description: Total number of rows in the table\n        size_bytes:\n          type: integer\n          format: int64\n          minimum: 0\n          description: Total size of the table in bytes\n        num_fragments:\n          type: integer\n          format: int64\n          minimum: 0\n          description: Number of data fragments\n</code></pre>"},{"location":"format/namespace/operations/get-table-tag-version/","title":"GetTableTagVersion","text":""},{"location":"format/namespace/operations/get-table-tag-version/#description","title":"Description","text":"<pre><code>      description: |\n        Get the version number that a specific tag points to for table `id`.\n</code></pre>"},{"location":"format/namespace/operations/get-table-tag-version/#request-schema","title":"Request Schema","text":"<pre><code>    GetTableTagVersionRequest:\n      type: object\n      required:\n        - tag\n      properties:\n        id:\n          type: array\n          items:\n            type: string\n        tag:\n          type: string\n          description: Name of the tag to get version for\n</code></pre>"},{"location":"format/namespace/operations/get-table-tag-version/#response-schema","title":"Response Schema","text":"<pre><code>    GetTableTagVersionResponse:\n      type: object\n      required:\n        - version\n      properties:\n        version:\n          type: integer\n          format: int64\n          minimum: 0\n          description: version number that the tag points to\n</code></pre>"},{"location":"format/namespace/operations/insert-into-table/","title":"InsertIntoTable","text":""},{"location":"format/namespace/operations/insert-into-table/#description","title":"Description","text":"<pre><code>      description: |\n        Insert new records into table `id`.\n</code></pre>"},{"location":"format/namespace/operations/insert-into-table/#request-schema","title":"Request Schema","text":"<pre><code>    InsertIntoTableRequest:\n      type: object\n      description: |\n        Request for inserting records into a table, excluding the Arrow IPC stream.\n      properties:\n        id:\n          type: array\n          items:\n            type: string\n        mode:\n          type: string\n          enum:\n            - append\n            - overwrite\n          default: append\n</code></pre>"},{"location":"format/namespace/operations/insert-into-table/#response-schema","title":"Response Schema","text":"<pre><code>    InsertIntoTableResponse:\n      type: object\n      description: Response from inserting records into a table\n      properties:\n        version:\n          type: integer\n          format: int64\n          description: The version of the table after the insert\n          minimum: 0\n</code></pre>"},{"location":"format/namespace/operations/list-namespaces/","title":"ListNamespaces","text":""},{"location":"format/namespace/operations/list-namespaces/#description","title":"Description","text":"<pre><code>      description: |\n        List all child namespace names of the parent namespace `id`.\n</code></pre>"},{"location":"format/namespace/operations/list-namespaces/#request-schema","title":"Request Schema","text":"<pre><code>    ListNamespacesRequest:\n      type: object\n      properties:\n        id:\n          type: array\n          items:\n            type: string\n        page_token:\n          $ref: \"#/components/schemas/PageToken\"\n        limit:\n          $ref: \"#/components/schemas/PageLimit\"\n</code></pre>"},{"location":"format/namespace/operations/list-namespaces/#response-schema","title":"Response Schema","text":"<pre><code>    ListNamespacesResponse:\n      type: object\n      required:\n        - namespaces\n      properties:\n        namespaces:\n          type: array\n          uniqueItems: true\n          description: |\n            The list of names of the child namespaces relative to the parent namespace `id` in the request.\n          items:\n            type: string\n        page_token:\n          $ref: \"#/components/schemas/PageToken\"\n</code></pre>"},{"location":"format/namespace/operations/list-namespaces/#related-components-schema","title":"Related Components Schema","text":""},{"location":"format/namespace/operations/list-namespaces/#page-token","title":"Page Token","text":"<pre><code>    PageToken:\n      description: |\n        An opaque token that allows pagination for list operations (e.g. ListNamespaces).\n\n        For an initial request of a list operation, \n        if the implementation cannot return all items in one response,\n        or if there are more items than the page limit specified in the request,\n        the implementation must return a page token in the response,\n        indicating there are more results available.\n\n        After the initial request, \n        the value of the page token from each response must be used\n        as the page token value for the next request.\n\n        Caller must interpret either `null`, \n        missing value or empty string value of the page token from\n        the implementation's response as the end of the listing results.\n      type: string\n      nullable: true\n</code></pre>"},{"location":"format/namespace/operations/list-namespaces/#page-limit","title":"Page Limit","text":"<pre><code>    PageLimit:\n      description: |\n        An inclusive upper bound of the \n        number of results that a caller will receive.\n      type: integer\n      nullable: true\n</code></pre>"},{"location":"format/namespace/operations/list-table-indices/","title":"ListTableIndices","text":""},{"location":"format/namespace/operations/list-table-indices/#description","title":"Description","text":"<pre><code>      description: |\n        List all indices created on a table. Returns information about each index\n        including name, columns, status, and UUID.\n</code></pre>"},{"location":"format/namespace/operations/list-table-indices/#request-schema","title":"Request Schema","text":"<pre><code>    ListTableIndicesRequest:\n      type: object\n      properties:\n        id:\n          type: array\n          items:\n            type: string\n          description: The namespace identifier\n        version:\n          type: integer\n          format: int64\n          minimum: 0\n          nullable: true\n          description: Optional table version to list indexes from\n        page_token:\n          $ref: '#/components/schemas/PageToken'\n        limit:\n          $ref: '#/components/schemas/PageLimit'\n</code></pre>"},{"location":"format/namespace/operations/list-table-indices/#response-schema","title":"Response Schema","text":"<pre><code>    ListTableIndicesResponse:\n      type: object\n      required:\n        - indexes\n      properties:\n        indexes:\n          type: array\n          items:\n            $ref: '#/components/schemas/IndexContent'\n          description: List of indexes on the table\n        page_token:\n          $ref: '#/components/schemas/PageToken'\n</code></pre>"},{"location":"format/namespace/operations/list-table-tags/","title":"ListTableTags","text":""},{"location":"format/namespace/operations/list-table-tags/#description","title":"Description","text":"<pre><code>      description: |\n        List all tags that have been created for table `id`.\n        Returns a map of tag names to their corresponding version numbers and metadata.\n</code></pre>"},{"location":"format/namespace/operations/list-table-tags/#response-schema","title":"Response Schema","text":"<pre><code>    ListTableTagsResponse:\n      type: object\n      required:\n        - tags\n      properties:\n        tags:\n          type: object\n          additionalProperties:\n            $ref: '#/components/schemas/TagContents'\n          description: Map of tag names to their contents\n</code></pre>"},{"location":"format/namespace/operations/list-table-versions/","title":"ListTableVersions","text":""},{"location":"format/namespace/operations/list-table-versions/#description","title":"Description","text":"<pre><code>      description: |\n        List all versions (commits) of table `id` with their metadata.\n</code></pre>"},{"location":"format/namespace/operations/list-table-versions/#request-schema","title":"Request Schema","text":"<pre><code>    ListTableVersionsRequest:\n      type: object\n      properties:\n        id:\n          type: array\n          items:\n            type: string\n        page_token:\n          $ref: '#/components/schemas/PageToken'\n        limit:\n          $ref: '#/components/schemas/PageLimit'\n</code></pre>"},{"location":"format/namespace/operations/list-table-versions/#response-schema","title":"Response Schema","text":"<pre><code>    ListTableVersionsResponse:\n      type: object\n      required:\n        - versions\n      properties:\n        versions:\n          type: array\n          items:\n            $ref: '#/components/schemas/TableVersion'\n          description: List of table versions\n        page_token:\n          $ref: '#/components/schemas/PageToken'\n</code></pre>"},{"location":"format/namespace/operations/list-table-versions/#related-components-schema","title":"Related Components Schema","text":""},{"location":"format/namespace/operations/list-table-versions/#table-version","title":"Table Version","text":"<pre><code>    TableVersion:\n      type: object\n      required:\n        - version\n        - timestamp\n      properties:\n        version:\n          type: integer\n          format: int64\n          minimum: 0\n          description: Version number\n        timestamp:\n          type: string\n          format: date-time\n          description: Timestamp when the version was created\n</code></pre>"},{"location":"format/namespace/operations/list-tables/","title":"ListTables","text":""},{"location":"format/namespace/operations/list-tables/#description","title":"Description","text":"<pre><code>      description: |\n        List all child table names of the parent namespace `id`.\n</code></pre>"},{"location":"format/namespace/operations/list-tables/#request-schema","title":"Request Schema","text":"<pre><code>    ListTablesRequest:\n      type: object\n      properties:\n        id:\n          type: array\n          items:\n            type: string\n        page_token:\n          $ref: \"#/components/schemas/PageToken\"\n        limit:\n          $ref: \"#/components/schemas/PageLimit\"\n</code></pre>"},{"location":"format/namespace/operations/list-tables/#response-schema","title":"Response Schema","text":"<pre><code>    ListTablesResponse:\n      type: object\n      required:\n        - tables\n      properties:\n        tables:\n          type: array\n          uniqueItems: true\n          description: |\n            The list of names of the tables relative to the parent namespace `id` in the request.\n          items:\n            type: string\n        page_token:\n          $ref: \"#/components/schemas/PageToken\"\n</code></pre>"},{"location":"format/namespace/operations/list-tables/#related-components-schema","title":"Related Components Schema","text":""},{"location":"format/namespace/operations/list-tables/#page-token","title":"Page Token","text":"<pre><code>    PageToken:\n      description: |\n        An opaque token that allows pagination for list operations (e.g. ListNamespaces).\n\n        For an initial request of a list operation, \n        if the implementation cannot return all items in one response,\n        or if there are more items than the page limit specified in the request,\n        the implementation must return a page token in the response,\n        indicating there are more results available.\n\n        After the initial request, \n        the value of the page token from each response must be used\n        as the page token value for the next request.\n\n        Caller must interpret either `null`, \n        missing value or empty string value of the page token from\n        the implementation's response as the end of the listing results.\n      type: string\n      nullable: true\n</code></pre>"},{"location":"format/namespace/operations/list-tables/#page-limit","title":"Page Limit","text":"<pre><code>    PageLimit:\n      description: |\n        An inclusive upper bound of the \n        number of results that a caller will receive.\n      type: integer\n      nullable: true\n</code></pre>"},{"location":"format/namespace/operations/merge-insert-into-table/","title":"MergeInsertIntoTable","text":""},{"location":"format/namespace/operations/merge-insert-into-table/#description","title":"Description","text":"<pre><code>      description: |\n        Performs a merge insert (upsert) operation on table `id`.\n        This operation updates existing rows\n        based on a matching column and inserts new rows that don't match.\n        It returns the number of rows inserted and updated.\n</code></pre>"},{"location":"format/namespace/operations/merge-insert-into-table/#request-schema","title":"Request Schema","text":"<pre><code>    MergeInsertIntoTableRequest:\n      type: object\n      description: |\n        Request for merging or inserting records into a table, excluding the Arrow IPC stream.\n      properties:\n        id:\n          type: array\n          items:\n            type: string\n        \"on\":\n          description: Column name to use for matching rows (required)\n          type: string\n        when_matched_update_all:\n          description: Update all columns when rows match\n          type: boolean\n          default: false\n        when_matched_update_all_filt:\n          description: The row is updated (similar to UpdateAll) only for rows where the SQL expression evaluates to true\n          type: string\n        when_not_matched_insert_all:\n          description: Insert all columns when rows don't match\n          type: boolean\n          default: false\n        when_not_matched_by_source_delete:\n          description: Delete all rows from target table that don't match a row in the source table\n          type: boolean\n          default: false\n        when_not_matched_by_source_delete_filt:\n          description: Delete rows from the target table if there is no match AND the SQL expression evaluates to true\n          type: string\n</code></pre>"},{"location":"format/namespace/operations/merge-insert-into-table/#response-schema","title":"Response Schema","text":"<pre><code>    MergeInsertIntoTableResponse:\n      type: object\n      description: Response from merge insert operation\n      properties:\n        num_updated_rows:\n          type: integer\n          format: int64\n          description: Number of rows updated\n          minimum: 0\n        num_inserted_rows:\n          type: integer\n          format: int64\n          description: Number of rows inserted\n          minimum: 0\n        num_deleted_rows:\n          type: integer\n          format: int64\n          description: Number of rows deleted (typically 0 for merge insert)\n          minimum: 0\n        version:\n          type: integer\n          format: int64\n          description: The commit version associated with the operation\n          minimum: 0\n</code></pre>"},{"location":"format/namespace/operations/namespace-exists/","title":"NamespaceExists","text":""},{"location":"format/namespace/operations/namespace-exists/#description","title":"Description","text":"<pre><code>      description: |\n        Check if namespace `id` exists.\n\n        This operation must behave exactly like the DescribeNamespace API, \n        except it does not contain a response body.\n</code></pre>"},{"location":"format/namespace/operations/namespace-exists/#request-schema","title":"Request Schema","text":"<pre><code>    NamespaceExistsRequest:\n      type: object\n      properties:\n        id:\n          type: array\n          items:\n            type: string\n</code></pre>"},{"location":"format/namespace/operations/namespace-exists/#response","title":"Response","text":"<p>This operation returns success if the namespace exists,  or an error if it does not exist. No response body is included in successful responses.</p>"},{"location":"format/namespace/operations/query-table/","title":"QueryTable","text":""},{"location":"format/namespace/operations/query-table/#description","title":"Description","text":"<pre><code>      description: |\n        Query table `id` with vector search, full text search and optional SQL filtering.\n        Returns results in Arrow IPC file or stream format.\n</code></pre>"},{"location":"format/namespace/operations/query-table/#request-schema","title":"Request Schema","text":"<pre><code>    QueryTableRequest:\n      type: object\n      required:\n        - vector\n        - k\n      properties:\n        id:\n          type: array\n          items:\n            type: string\n        bypass_vector_index:\n          type: boolean\n          description: Whether to bypass vector index\n        columns:\n          type: array\n          nullable: true\n          items:\n            type: string\n          description: Optional list of columns to return\n        distance_type:\n          type: string\n          description: Distance metric to use\n        ef:\n          type: integer\n          minimum: 0\n          description: Search effort parameter for HNSW index\n        fast_search:\n          type: boolean\n          description: Whether to use fast search\n        filter:\n          type: string\n          description: Optional SQL filter expression\n        full_text_query:\n          type: object\n          nullable: true\n          description: Optional full-text search query. Provide either string_query or structured_query, not both.\n          properties:\n            string_query:\n              $ref: '#/components/schemas/StringFtsQuery'\n            structured_query:\n              $ref: '#/components/schemas/StructuredFtsQuery'\n        k:\n          type: integer\n          minimum: 0\n          description: Number of results to return\n        lower_bound:\n          type: number\n          format: float\n          description: Lower bound for search\n        nprobes:\n          type: integer\n          minimum: 0\n          description: Number of probes for IVF index\n        offset:\n          type: integer\n          minimum: 0\n          description: Number of results to skip\n        prefilter:\n          type: boolean\n          description: Whether to apply filtering before vector search\n        refine_factor:\n          type: integer\n          format: int32\n          minimum: 0\n          description: Refine factor for search\n        upper_bound:\n          type: number\n          format: float\n          description: Upper bound for search\n        vector:\n          type: object\n          nullable: true\n          description: Query vector(s) for similarity search. Provide either single_vector or multi_vector, not both.\n          properties:\n            single_vector:\n              type: array\n              items:\n                type: number\n                format: float\n              description: Single query vector\n            multi_vector:\n              type: array\n              items:\n                type: array\n                items:\n                  type: number\n                  format: float\n              description: Multiple query vectors for batch search\n        vector_column:\n          type: string\n          description: Name of the vector column to search\n        version:\n          type: integer\n          format: int64\n          minimum: 0\n          description: Table version to query\n        with_row_id:\n          type: boolean\n          description: If true, return the row id as a column called `_rowid`\n</code></pre>"},{"location":"format/namespace/operations/query-table/#response","title":"Response","text":"<p>The response returns query results in Arrow IPC file or stream format  containing the matching records based on the search criteria.</p>"},{"location":"format/namespace/operations/register-table/","title":"RegisterTable","text":""},{"location":"format/namespace/operations/register-table/#description","title":"Description","text":"<pre><code>      description: |\n        Register an existing table at a given storage location as `id`.\n</code></pre>"},{"location":"format/namespace/operations/register-table/#request-schema","title":"Request Schema","text":"<pre><code>    RegisterTableRequest:\n      type: object\n      required:\n        - location\n      properties:\n        id:\n          type: array\n          items:\n            type: string\n        location:\n          type: string\n        mode:\n          type: string\n          description: |\n            There are two modes when trying to register a table,\n            to differentiate the behavior when a table of the same name already exists:\n              * CREATE (default): the operation fails with 409.\n              * OVERWRITE: the existing table registration is replaced with the new registration.\n          enum:\n            - CREATE\n            - OVERWRITE\n        properties:\n          type: object\n          additionalProperties:\n            type: string\n</code></pre>"},{"location":"format/namespace/operations/register-table/#response-schema","title":"Response Schema","text":"<pre><code>    RegisterTableResponse:\n      type: object\n      required:\n        - location\n      properties:\n        location:\n          type: string\n        properties:\n          type: object\n          additionalProperties:\n            type: string\n</code></pre>"},{"location":"format/namespace/operations/restore-table/","title":"RestoreTable","text":""},{"location":"format/namespace/operations/restore-table/#description","title":"Description","text":"<pre><code>      description: |\n        Restore table `id` to a specific version.\n</code></pre>"},{"location":"format/namespace/operations/restore-table/#request-schema","title":"Request Schema","text":"<pre><code>    RestoreTableRequest:\n      type: object\n      properties:\n        id:\n          type: array\n          items:\n            type: string\n        version:\n          type: integer\n          format: int64\n          minimum: 0\n          description: Version to restore to (if not specified, restores to current version)\n</code></pre>"},{"location":"format/namespace/operations/restore-table/#response-schema","title":"Response Schema","text":"<pre><code>    RestoreTableResponse:\n      type: object\n      properties:\n        version:\n          type: integer\n          format: int64\n          minimum: 0\n          description: Version of the table after restore operation\n</code></pre>"},{"location":"format/namespace/operations/table-exists/","title":"TableExists","text":""},{"location":"format/namespace/operations/table-exists/#description","title":"Description","text":"<pre><code>      description: |\n        Check if table `id` exists.\n\n        This operation should behave exactly like DescribeTable, \n        except it does not contain a response body.\n\n        For DirectoryNamespace implementation, a table exists if either:\n        - The table has Lance data versions (regular table created with CreateTable)\n        - A `.lance-reserved` file exists in the table directory (empty table created with CreateEmptyTable)\n</code></pre>"},{"location":"format/namespace/operations/table-exists/#request-schema","title":"Request Schema","text":"<pre><code>    TableExistsRequest:\n      type: object\n      properties:\n        id:\n          type: array\n          items:\n            type: string\n        version:\n          description: |\n            Version of the table to check existence.\n            If not specified, server should resolve it to the latest version.\n          type: integer\n          format: int64\n          minimum: 0\n</code></pre>"},{"location":"format/namespace/operations/table-exists/#response","title":"Response","text":"<p>This operation returns success if the table exists, or an error if it does not exist.  No response body is included in successful responses.</p>"},{"location":"format/namespace/operations/update-table-tag/","title":"UpdateTableTag","text":""},{"location":"format/namespace/operations/update-table-tag/#description","title":"Description","text":"<pre><code>      description: |\n        Update an existing tag for table `id` to point to a different version.\n</code></pre>"},{"location":"format/namespace/operations/update-table-tag/#request-schema","title":"Request Schema","text":"<pre><code>    UpdateTableTagRequest:\n      type: object\n      required:\n        - tag\n        - version\n      properties:\n        id:\n          type: array\n          items:\n            type: string\n        tag:\n          type: string\n          description: Name of the tag to update\n        version:\n          type: integer\n          format: int64\n          minimum: 0\n          description: New version number for the tag to point to\n</code></pre>"},{"location":"format/namespace/operations/update-table/","title":"UpdateTable","text":""},{"location":"format/namespace/operations/update-table/#description","title":"Description","text":"<pre><code>      description: |\n        Update existing rows in table `id`.\n</code></pre>"},{"location":"format/namespace/operations/update-table/#request-schema","title":"Request Schema","text":"<pre><code>    UpdateTableRequest:\n      type: object\n      description: |\n        Each update consists of a column name and an SQL expression that will be\n        evaluated against the current row's value. Optionally, a predicate can be\n        provided to filter which rows to update.\n      required:\n        - updates\n      properties:\n        id:\n          type: array\n          items:\n            type: string\n        predicate:\n          type: string\n          nullable: true\n          description: Optional SQL predicate to filter rows for update\n        updates:\n          type: array\n          items:\n            type: array\n            minItems: 2\n            maxItems: 2\n            items:\n              type: string\n          description: List of column updates as [column_name, expression] pairs\n</code></pre>"},{"location":"format/namespace/operations/update-table/#response-schema","title":"Response Schema","text":"<pre><code>    UpdateTableResponse:\n      type: object\n      required:\n        - updated_rows\n        - version\n      properties:\n        updated_rows:\n          type: integer\n          format: int64\n          description: Number of rows updated\n          minimum: 0\n        version:\n          type: integer\n          format: int64\n          description: The commit version associated with the operation\n          minimum: 0\n</code></pre>"},{"location":"format/table/","title":"Lance Table Format","text":""},{"location":"format/table/#overview","title":"Overview","text":"<p>The Lance table format organizes datasets as versioned collections of fragments and indices. Each version is described by an immutable manifest file that references data files, deletion files, transaction file and indices. The format supports ACID transactions, schema evolution, and efficient incremental updates through Multi-Version Concurrency Control (MVCC).</p>"},{"location":"format/table/#manifest","title":"Manifest","text":"<p>A manifest describes a single version of the dataset. It contains the complete schema definition including nested fields, the list of data fragments comprising this version,  a monotonically increasing version number, and an optional reference to the index section that describes a list of index metadata.</p> Manifest protobuf message <pre><code>message Manifest {\n  // All fields of the dataset, including the nested fields.\n  repeated lance.file.Field fields = 1;\n\n  // Schema metadata.\n  map&lt;string, bytes&gt; schema_metadata = 5;\n\n  // Fragments of the dataset.\n  repeated DataFragment fragments = 2;\n\n  // Snapshot version number.\n  uint64 version = 3;\n\n  // The file position of the version auxiliary data.\n  //  * It is not inheritable between versions.\n  //  * It is not loaded by default during query.\n  uint64 version_aux_data = 4;\n\n  message WriterVersion {\n    // The name of the library that created this file.\n    string library = 1;\n    // The version of the library that created this file. Because we cannot assume\n    // that the library is semantically versioned, this is a string. However, if it\n    // is semantically versioned, it should be a valid semver string without any 'v'\n    // prefix. For example: `2.0.0`, `2.0.0-rc.1`.\n    //\n    // For forward compatibility with older readers, when writing new manifests this\n    // field should contain only the core version (major.minor.patch) without any\n    // prerelease or build metadata. The prerelease/build info should be stored in\n    // the separate prerelease and build_metadata fields instead.\n    string version = 2;\n    // Optional semver prerelease identifier.\n    //\n    // This field stores the prerelease portion of a semantic version separately\n    // from the core version number. For example, if the full version is \"2.0.0-rc.1\",\n    // the version field would contain \"2.0.0\" and prerelease would contain \"rc.1\".\n    //\n    // This separation ensures forward compatibility: older readers can parse the\n    // clean version field without errors, while newer readers can reconstruct the\n    // full semantic version by combining version, prerelease, and build_metadata.\n    //\n    // If absent, the version field is used as-is.\n    optional string prerelease = 3;\n    // Optional semver build metadata.\n    //\n    // This field stores the build metadata portion of a semantic version separately\n    // from the core version number. For example, if the full version is\n    // \"2.0.0-rc.1+build.123\", the version field would contain \"2.0.0\", prerelease\n    // would contain \"rc.1\", and build_metadata would contain \"build.123\".\n    //\n    // If absent, no build metadata is present.\n    optional string build_metadata = 4;\n  }\n\n  // The version of the writer that created this file.\n  //\n  // This information may be used to detect whether the file may have known bugs\n  // associated with that writer.\n  WriterVersion writer_version = 13;\n\n  // If present, the file position of the index metadata.\n  optional uint64 index_section = 6;\n\n  // Version creation Timestamp, UTC timezone\n  google.protobuf.Timestamp timestamp = 7;\n\n  // Optional version tag\n  string tag = 8;\n\n  // Feature flags for readers.\n  //\n  // A bitmap of flags that indicate which features are required to be able to\n  // read the table. If a reader does not recognize a flag that is set, it\n  // should not attempt to read the dataset.\n  //\n  // Known flags:\n  // * 1: deletion files are present\n  // * 2: row ids are stable and stored as part of the fragment metadata.\n  // * 4: use v2 format (deprecated)\n  // * 8: table config is present\n  uint64 reader_feature_flags = 9;\n\n  // Feature flags for writers.\n  //\n  // A bitmap of flags that indicate which features must be used when writing to the\n  // dataset. If a writer does not recognize a flag that is set, it should not attempt to\n  // write to the dataset.\n  //\n  // The flag identities are the same as for reader_feature_flags, but the values of\n  // reader_feature_flags and writer_feature_flags are not required to be identical.\n  uint64 writer_feature_flags = 10;\n\n  // The highest fragment ID that has been used so far.\n  //\n  // This ID is not guaranteed to be present in the current version, but it may\n  // have been used in previous versions.\n  //\n  // For a single fragment, will be zero. For no fragments, will be absent.\n  optional uint32 max_fragment_id = 11;\n\n  // Path to the transaction file, relative to `{root}/_transactions`. The file at that\n  // location contains a wire-format serialized Transaction message representing the\n  // transaction that created this version.\n  //\n  // This string field \"transaction_file\" may be empty if no transaction file was written.\n  //\n  // The path format is \"{read_version}-{uuid}.txn\" where {read_version} is the version of\n  // the table the transaction read from (serialized to decimal with no padding digits),\n  // and {uuid} is a hyphen-separated UUID.\n  string transaction_file = 12;\n\n  // The file position of the transaction content. None if transaction is empty\n  // This transaction content begins with the transaction content length as u32\n  // If the transaction proto message has a length of `len`, the message ends at `len` + 4\n  optional uint64 transaction_section = 21;\n\n  // The next unused row id. If zero, then the table does not have any rows.\n  //\n  // This is only used if the \"stable_row_ids\" feature flag is set.\n  uint64 next_row_id = 14;\n\n  message DataStorageFormat {\n    // The format of the data files (e.g. \"lance\")\n    string file_format = 1;\n    // The max format version of the data files. The format of the version can vary by\n    // file_format and is not required to follow semver.\n    //\n    // Every file in this version of the dataset has the same file_format version.\n    string version = 2;\n  }\n\n  // The data storage format\n  //\n  // This specifies what format is used to store the data files.\n  DataStorageFormat data_format = 15;\n\n  // Table config.\n  //\n  // Keys with the prefix \"lance.\" are reserved for the Lance library. Other\n  // libraries may wish to similarly prefix their configuration keys\n  // appropriately.\n  map&lt;string, string&gt; config = 16;\n\n  // Metadata associated with the table.\n  //\n  // This is a key-value map that can be used to store arbitrary metadata\n  // associated with the table.\n  //\n  // This is different than configuration, which is used to tell libraries how\n  // to read, write, or manage the table.\n  //\n  // This is different than schema metadata, which is used to describe the\n  // data itself and is attached to the output schema of scans.\n  map&lt;string, string&gt; table_metadata = 19;\n\n  // Field number 17 (`blob_dataset_version`) was used for a secondary blob dataset.\n  reserved 17;\n  reserved \"blob_dataset_version\";\n\n  // The base paths of data files.\n  //\n  // This is used to determine the base path of a data file. In common cases data file paths are under current dataset base path.\n  // But for shallow cloning, importing file and other multi-tier storage cases, the actual data files could be outside of the current dataset.\n  // This field is used with the `base_id` in `lance.file.File` and `lance.file.DeletionFile`.\n  //\n  // For example, if we have a dataset with base path `s3://bucket/dataset`, we have a DataFile with base_id 0, we get the actual data file path by:\n  // base_paths[id = 0] + /data/ + file.path\n  // the key(a.k.a index) starts from 0, increased by 1 for each new base path.\n  repeated BasePath base_paths = 18;\n\n  // The branch of the dataset. None means main branch.\n  optional string branch = 20;\n\n}\n</code></pre>"},{"location":"format/table/#schema-fields","title":"Schema &amp; Fields","text":"<p>The schema of the table is written as a series of fields, plus a schema metadata map.  The data types generally have a 1-1 correspondence with the Apache Arrow data types. Each field, including nested fields, have a unique integer id. At initial table creation time, fields are assigned ids in depth-first order. Afterwards, field IDs are assigned incrementally for newly added fields.</p> <p>Column encoding configurations are specified through field metadata using the <code>lance-encoding:</code> prefix. See File Format Encoding Specification for details on available encodings, compression schemes, and configuration options.</p> Field protobuf message <pre><code>message Field {\n  enum Type {\n    PARENT = 0;\n    REPEATED = 1;\n    LEAF = 2;\n  }\n  Type type = 1;\n\n  // Fully qualified name.\n  string name = 2;\n  /// Field Id.\n  ///\n  /// See the comment in `DataFile.fields` for how field ids are assigned.\n  int32 id = 3;\n  /// Parent Field ID. If not set, this is a top-level column.\n  int32 parent_id = 4;\n\n  // Logical types, support parameterized Arrow Type.\n  //\n  // PARENT types will always have logical type \"struct\".\n  //\n  // REPEATED types may have logical types:\n  // * \"list\"\n  // * \"large_list\"\n  // * \"list.struct\"\n  // * \"large_list.struct\"\n  // The final two are used if the list values are structs, and therefore the\n  // field is both implicitly REPEATED and PARENT.\n  //\n  // LEAF types may have logical types:\n  // * \"null\"\n  // * \"bool\"\n  // * \"int8\" / \"uint8\"\n  // * \"int16\" / \"uint16\"\n  // * \"int32\" / \"uint32\"\n  // * \"int64\" / \"uint64\"\n  // * \"halffloat\" / \"float\" / \"double\"\n  // * \"string\" / \"large_string\"\n  // * \"binary\" / \"large_binary\"\n  // * \"date32:day\"\n  // * \"date64:ms\"\n  // * \"decimal:128:{precision}:{scale}\" / \"decimal:256:{precision}:{scale}\"\n  // * \"time:{unit}\" / \"timestamp:{unit}\" / \"duration:{unit}\", where unit is\n  // \"s\", \"ms\", \"us\", \"ns\"\n  // * \"dict:{value_type}:{index_type}:false\"\n  string logical_type = 5;\n  // If this field is nullable.\n  bool nullable = 6;\n\n  // optional field metadata (e.g. extension type name/parameters)\n  map&lt;string, bytes&gt; metadata = 10;  \n\n  bool unenforced_primary_key = 12;\n\n  // DEPRECATED ----------------------------------------------------------------\n\n  // Deprecated: Only used in V1 file format. V2 uses variable encodings defined\n  // per page.\n  //\n  // The global encoding to use for this field.\n  Encoding encoding = 7;\n\n  // Deprecated: Only used in V1 file format. V2 dynamically chooses when to\n  // do dictionary encoding and keeps the dictionary in the data files.\n  //\n  // The file offset for storing the dictionary value.\n  // It is only valid if encoding is DICTIONARY.\n  //\n  // The logic type presents the value type of the column, i.e., string value.\n  Dictionary dictionary = 8;\n\n  // Deprecated: optional extension type name, use metadata field\n  // ARROW:extension:name\n  string extension_name = 9;\n\n  // Field number 11 was previously `string storage_class`.\n  // Keep it reserved so older manifests remain compatible while new writers\n  // avoid reusing the slot.\n  reserved 11;\n  reserved \"storage_class\";\n\n}\n</code></pre>"},{"location":"format/table/#fragments","title":"Fragments","text":"<p>A fragment represents a horizontal partition of the dataset containing a subset of rows. Each fragment has a unique <code>uint32</code> identifier assigned incrementally based on the dataset's maximum fragment ID. Each fragment consists of one or more data files storing columns, plus an optional deletion file. If present, the deletion file stores the positions (0-based) of the rows that have been deleted from the fragment. The fragment tracks the total row count including deleted rows in its physical rows field. Column subsets can be read without accessing all data files, and each data file is independently compressed and encoded.</p> DataFragment protobuf message <pre><code>message DataFragment {\n  // The ID of a DataFragment is unique within a dataset.\n  uint64 id = 1;\n\n  repeated DataFile files = 2;\n\n  // File that indicates which rows, if any, should be considered deleted.\n  DeletionFile deletion_file = 3;\n\n  // TODO: What's the simplest way we can allow an inline tombstone bitmap?\n\n  // A serialized RowIdSequence message (see rowids.proto).\n  //\n  // These are the row ids for the fragment, in order of the rows as they appear.\n  // That is, if a fragment has 3 rows, and the row ids are [1, 42, 3], then the\n  // first row is row 1, the second row is row 42, and the third row is row 3.\n  oneof row_id_sequence {\n    // If small (&lt; 200KB), the row ids are stored inline.\n    bytes inline_row_ids = 5;\n    // Otherwise, stored as part of a file.\n    ExternalFile external_row_ids = 6;\n  } // row_id_sequence\n\n  oneof last_updated_at_version_sequence {\n    // If small (&lt; 200KB), the row latest updated versions are stored inline.\n    bytes inline_last_updated_at_versions = 7;\n    // Otherwise, stored as part of a file.\n    ExternalFile external_last_updated_at_versions = 8;\n  } // last_updated_at_version_sequence\n\n  oneof created_at_version_sequence {\n    // If small (&lt; 200KB), the row created at versions are stored inline.\n    bytes inline_created_at_versions = 9;\n    // Otherwise, stored as part of a file.\n    ExternalFile external_created_at_versions = 10;\n  } // created_at_version_sequence\n\n  // Number of original rows in the fragment, this includes rows that are now marked with\n  // deletion tombstones. To compute the current number of rows, subtract\n  // `deletion_file.num_deleted_rows` from this value.\n  uint64 physical_rows = 4;\n\n}\n</code></pre>"},{"location":"format/table/#data-evolution","title":"Data Evolution","text":"<p>This fragment design enables a new concept called data evolution, which means efficient schema evolution (add column, update column, drop column) with backfill. For example, when adding a new column, new column data are added by appending new data files to each fragment, with values computed for all existing rows in the fragment. There is no need to rewrite the entire table to just add data for a single column. This enables efficient feature engineering and embedding updates for ML/AI workloads.</p> <p>Each data file should contain a distinct set of field ids.  It is not required that all field ids in the dataset schema are found in one of the data files.  If there is no corresponding data file, that column should be read as entirely <code>NULL</code>.</p> <p>Field ids might be replaced with <code>-2</code>, a tombstone value.  In this case that column should be ignored. This used, for example, when rewriting a column:  The old data file replaces the field id with <code>-2</code> to ignore the old data, and a new data file is appended to the fragment.</p>"},{"location":"format/table/#data-files","title":"Data Files","text":"<p>Data files store column data for a fragment using the Lance file format. Each data file stores a subset of the columns in the fragment. Field IDs are assigned either sequentially based on schema position (for Lance file format v1)  or independently of column indices due to variable encoding widths (for Lance file format v2).</p> DataFile protobuf message <pre><code>message DataFile {\n  // Path to the root relative to the dataset's URI.\n  string path = 1;\n  // The ids of the fields/columns in this file.\n  //\n  // When a DataFile object is created in memory, every value in fields is assigned -1 by\n  // default. An object with a value in fields of -1 must not be stored to disk. -2 is\n  // used for \"tombstoned\", meaning a field that is no longer in use. This is often\n  // because the original field id was reassigned to a different data file.\n  //\n  // In Lance v1 IDs are assigned based on position in the file, offset by the max\n  // existing field id in the table (if any already). So when a fragment is first created\n  // with one file of N columns, the field ids will be 1, 2, ..., N. If a second fragment\n  // is created with M columns, the field ids will be N+1, N+2, ..., N+M.\n  //\n  // In Lance v1 there is one field for each field in the input schema, this includes\n  // nested fields (both struct and list).  Fixed size list fields have only a single\n  // field id (these are not considered nested fields in Lance v1).\n  //\n  // This allows column indices to be calculated from field IDs and the input schema.\n  //\n  // In Lance v2 the field IDs generally follow the same pattern but there is no\n  // way to calculate the column index from the field ID.  This is because a given\n  // field could be encoded in many different ways, some of which occupy a different\n  // number of columns.  For example, a struct field could be encoded into N + 1 columns\n  // or it could be encoded into a single packed column.  To determine column indices\n  // the column_indices property should be used instead.\n  //\n  // In Lance v1 these ids must be sorted but might not always be contiguous.\n  repeated int32 fields = 2;\n  // The top-level column indices for each field in the file.\n  //\n  // If the data file is version 1 then this property will be empty\n  //\n  // Otherwise there must be one entry for each field in `fields`.\n  //\n  // Some fields may not correspond to a top-level column in the file.  In these cases\n  // the index will -1.\n  //\n  // For example, consider the schema:\n  //\n  // - dimension: packed-struct (0):\n  //   - x: u32 (1)\n  //   - y: u32 (2)\n  // - path: list&lt;u32&gt; (3)\n  // - embedding: fsl&lt;768&gt; (4)\n  //   - fp64\n  // - borders: fsl&lt;4&gt; (5)\n  //   - simple-struct (6)\n  //     - margin: fp64 (7)\n  //     - padding: fp64 (8)\n  //\n  // One possible column indices array could be:\n  // [0, -1, -1, 1, 3, 4, 5, 6, 7]\n  //\n  // This reflects quite a few phenomenon:\n  // - The packed struct is encoded into a single column and there is no top-level column\n  //   for the x or y fields\n  // - The variable sized list is encoded into two columns\n  // - The embedding is encoded into a single column (common for FSL of primitive) and there\n  //   is not \"FSL column\"\n  // - The borders field actually does have an \"FSL column\"\n  //\n  // The column indices table may not have duplicates (other than -1)\n  repeated int32 column_indices = 3;\n  // The major file version used to create the file\n  uint32 file_major_version = 4;\n  // The minor file version used to create the file\n  //\n  // If both `file_major_version` and `file_minor_version` are set to 0,\n  // then this is a version 0.1 or version 0.2 file.\n  uint32 file_minor_version = 5;\n\n  // The known size of the file on disk in bytes.\n  //\n  // This is used to quickly find the footer of the file.\n  //\n  // When this is zero, it should be interpreted as \"unknown\".\n  uint64 file_size_bytes = 6;\n\n  // The base path index of the data file. Used when the file is imported or referred from another dataset.\n  // Lance use it as key of the base_paths field in Manifest to determine the actual base path of the data file.\n  optional uint32 base_id = 7;\n\n}\n</code></pre>"},{"location":"format/table/#deletion-files","title":"Deletion Files","text":"<p>Deletion files (a.k.a. deletion vectors) track deleted rows without rewriting data files. Each fragment can have at most one deletion file per version.</p> <p>Deletion files support two storage formats. The Arrow IPC format (<code>.arrow</code> extension) stores a flat Int32Array of deleted row offsets and is efficient for sparse deletions. The Roaring Bitmap format (<code>.bin</code> extension) stores a compressed roaring bitmap and is efficient for dense deletions. Readers must filter rows whose offsets appear in the deletion file for the fragment.</p> <p>Deletions can be materialized by rewriting data files with deleted rows removed. However, this invalidates row addresses and requires rebuilding indices, which can be expensive.</p> DeletionFile protobuf message <pre><code>message DeletionFile {\n  // Type of deletion file, intended as a way to increase efficiency of the storage of deleted row\n  // offsets. If there are sparsely deleted rows, then ARROW_ARRAY is the most efficient. If there\n  // are densely deleted rows, then BITMAP is the most efficient.\n  enum DeletionFileType {\n    // A single Int32Array of deleted row offsets, stored as an Arrow IPC file with one batch and\n    // one column. Has a .arrow extension.\n    ARROW_ARRAY = 0;\n    // A Roaring Bitmap of deleted row offsets. Has a .bin extension.\n    BITMAP = 1;\n  }\n\n  // Type of deletion file.\n  DeletionFileType file_type = 1;\n  // The version of the dataset this deletion file was built from.\n  uint64 read_version = 2;\n  // An opaque id used to differentiate this file from others written by concurrent\n  // writers.\n  uint64 id = 3;\n  // The number of rows that are marked as deleted.\n  uint64 num_deleted_rows = 4;\n  // The base path index of the deletion file. Used when the file is imported or referred from another\n  // dataset. Lance uses it as key of the base_paths field in Manifest to determine the actual base\n  // path of the deletion file.\n  optional uint32 base_id = 7;\n\n}\n</code></pre>"},{"location":"format/table/#related-specifications","title":"Related Specifications","text":""},{"location":"format/table/#storage-layout","title":"Storage Layout","text":"<p>File organization, base path system, and multi-location storage.</p> <p>See Storage Layout Specification</p>"},{"location":"format/table/#transactions","title":"Transactions","text":"<p>MVCC, commit protocol, transaction types, and conflict resolution.</p> <p>See Transaction Specification</p>"},{"location":"format/table/#row-lineage","title":"Row Lineage","text":"<p>Row address, Stable row ID, row version tracking, and change data feed.</p> <p>See Row ID &amp; Lineage Specification</p>"},{"location":"format/table/#indices","title":"Indices","text":"<p>Vector indices, scalar indices, full-text search, and index management.</p> <p>See Index Specification</p>"},{"location":"format/table/#versioning","title":"Versioning","text":"<p>Feature flags and format version compatibility.</p> <p>See Format Versioning Specification</p>"},{"location":"format/table/branch_tag/","title":"Branch and Tag Specification","text":""},{"location":"format/table/branch_tag/#overview","title":"Overview","text":"<p>Lance supports branching and tagging for managing multiple independent version histories and creating named references to specific versions. Branches enable parallel development workflows, while tags provide stable named references for important versions.</p>"},{"location":"format/table/branch_tag/#branching","title":"Branching","text":""},{"location":"format/table/branch_tag/#branch-name","title":"Branch Name","text":"<p>Branch names must follow these validation rules:</p> <ol> <li>Cannot be empty</li> <li>Cannot start or end with <code>/</code></li> <li>Cannot contain consecutive <code>//</code></li> <li>Cannot contain <code>..</code> or <code>\\</code></li> <li>Segments must contain only alphanumeric characters, <code>.</code>, <code>-</code>, <code>_</code></li> <li>Cannot end with <code>.lock</code></li> <li>Cannot be named <code>main</code> (reserved for main branch)</li> </ol>"},{"location":"format/table/branch_tag/#branch-metadata-path","title":"Branch Metadata Path","text":"<p>Branch metadata is stored at <code>_refs/branches/{branch-name}.json</code> in the dataset root. Since branch names support hierarchical naming with <code>/</code> characters, the <code>/</code> is URL-encoded as <code>%2F</code> in the filename to distinguish it from directory separators (e.g., <code>bugfix/issue-123</code> becomes <code>bugfix%2Fissue-123.json</code>):</p> <pre><code>{dataset_root}/\n    _refs/\n        branches/\n            feature-a.json\n            bugfix%2Fissue-123.json  # Note: '/' encoded as '%2F'\n</code></pre>"},{"location":"format/table/branch_tag/#branch-metadata-file-format","title":"Branch Metadata File Format","text":"<p>Each branch metadata file is a JSON file with the following fields:</p> JSON Key Type Optional Description <code>parent_branch</code> string Yes Name of the branch this was created from. <code>null</code> indicates branched from main. <code>parent_version</code> number Version number of the parent branch at the time this branch was created. <code>create_at</code> number Unix timestamp (seconds since epoch) when the branch was created. <code>manifest_size</code> number Size of the initial manifest file in bytes."},{"location":"format/table/branch_tag/#branch-dataset-layout","title":"Branch Dataset Layout","text":"<p>Each branch dataset is technically a shallow clone of the source dataset. Branch datasets are organized using the <code>tree/</code> directory at the dataset root:</p> <pre><code>{dataset_root}/\n    tree/\n        {branch_name}/\n            _versions/\n                *.manifest\n            _transactions/\n                *.txn\n            _deletions/\n                *.arrow\n                *.bin\n            _indices/\n                {UUID}/\n                    index.idx\n</code></pre> <p>Named branches store their version-specific files under <code>tree/{branch_name}/</code>, resembling the GitHub branch path convention. It uses the branch name as is to form the path,  which means <code>/</code> would create a logical subdirectory (e.g., <code>bugfix/issue-123</code>, <code>feature/user-auth</code>):</p> <pre><code>{dataset_root}/\n    tree/\n        feature-a/\n            _versions/\n                1.manifest\n                2.manifest\n        bugfix/\n            issue-123/\n                _versions/\n                    1.manifest\n</code></pre>"},{"location":"format/table/branch_tag/#tagging","title":"Tagging","text":""},{"location":"format/table/branch_tag/#tag-name","title":"Tag Name","text":"<p>Tag names must follow these validation rules:</p> <ol> <li>Cannot be empty</li> <li>Must contain only alphanumeric characters, <code>.</code>, <code>-</code>, <code>_</code></li> <li>Cannot start or end with <code>.</code></li> <li>Cannot end with <code>.lock</code></li> <li>Cannot contain consecutive <code>..</code></li> </ol> <p>Note that tag names do not support <code>/</code> characters, unlike branch names.</p>"},{"location":"format/table/branch_tag/#tag-storage","title":"Tag Storage","text":"<p>Tags are stored as JSON files under <code>_refs/tags/</code> at the dataset root:</p> <pre><code>{dataset_root}/\n    _refs/\n        tags/\n            v1.0.0.json\n            v1.1.0.json\n            production.json\n</code></pre> <p>Tags are always stored at the root dataset level, regardless of which branch they reference.</p>"},{"location":"format/table/branch_tag/#tag-file-format","title":"Tag File Format","text":"<p>Each tag file is a JSON file with the following fields:</p> JSON Key Type Optional Description <code>branch</code> string Yes Branch name being tagged. <code>null</code> or absent indicates main branch. <code>version</code> number Version number being tagged within that branch. <code>manifest_size</code> number Size of the manifest file in bytes. Used for efficient manifest loading."},{"location":"format/table/layout/","title":"Storage Layout Specification","text":""},{"location":"format/table/layout/#overview","title":"Overview","text":"<p>This specification defines how Lance datasets are organized on object storage. The layout design emphasizes portability, allowing datasets to be relocated or referenced across multiple storage systems with minimal metadata changes.</p>"},{"location":"format/table/layout/#dataset-root","title":"Dataset Root","text":"<p>The dataset root is the location where the dataset was initially created. Every Lance dataset has exactly one dataset root, which serves as the primary storage location for the dataset's files. The dataset root contains the standard subdirectory structure (<code>data/</code>, <code>_versions/</code>, <code>_deletions/</code>, <code>_indices/</code>, <code>_refs/</code>, <code>tree/</code>) that organizes the dataset's files.</p>"},{"location":"format/table/layout/#basic-layout","title":"Basic Layout","text":"<p>A Lance dataset in its basic form stores all files within the dataset root directory structure:</p> <pre><code>{dataset_root}/\n    data/\n        *.lance           -- Data files containing column data\n    _versions/\n        *.manifest        -- Manifest files (one per version)\n    _transactions/\n        *.txn             -- Transaction files for commit coordination\n    _deletions/\n        *.arrow           -- Deletion vector files (arrow format)\n        *.bin             -- Deletion vector files (bitmap format)\n    _indices/\n        {UUID}/\n            ...           -- Index content (different for each index type)\n    _refs/\n        tags/\n            *.json        -- Tag metadata\n        branches/\n            *.json        -- Branch metadata\n    tree/\n        {branch_name}/\n            ...           -- Branch dataset\n</code></pre>"},{"location":"format/table/layout/#base-path-system","title":"Base Path System","text":""},{"location":"format/table/layout/#basepath-message","title":"BasePath Message","text":"<p>The manifest's <code>base_paths</code> field contains an array of <code>BasePath</code> entries that define alternative storage locations for dataset files. Each base path entry has a unique numeric identifier that file metadata can reference to indicate where files are located. The <code>path</code> field specifies an absolute path interpretable by the object store. The <code>is_dataset_root</code> field determines how the path is interpreted: when true, the path points to a dataset root with standard subdirectories (<code>data/</code>, <code>_deletions/</code>, <code>_indices/</code>); when false, the path points directly to a file directory without subdirectories. An optional <code>name</code> field provides a human-readable alias, which is particularly useful for referencing tags in shallow clones.</p> BasePath protobuf message <pre><code>message BasePath {\n  uint32 id = 1;\n  optional string name = 2;\n  bool is_dataset_root = 3;\n  string path = 4;\n}\n</code></pre>"},{"location":"format/table/layout/#file-metadata-base-references","title":"File Metadata Base References","text":"<p>Three types of files can specify alternative base paths: data files, deletion files, and index metadata. Each of these file types includes an optional <code>base_id</code> field in their metadata that references a base path entry by its numeric identifier. When a file's <code>base_id</code> is absent, the file is located relative to the dataset root. When a file's <code>base_id</code> is present, readers must look up the corresponding base path entry in the manifest's <code>base_paths</code> array to determine where the file is stored.</p> <p>At read time, path resolution follows a two-step process. First, the reader determines the base path: if <code>base_id</code> is absent, the base path is the dataset root; otherwise, the reader looks up the base path entry using the <code>base_id</code> to obtain the path and its <code>is_dataset_root</code> flag. Second, the reader constructs the full file path based on whether the base path represents a dataset root. For dataset roots (when <code>is_dataset_root</code> is true), the full path includes standard subdirectories: data files are located under <code>data/</code>, deletion files under <code>_deletions/</code>, and indices under <code>_indices/</code>. For non-root base paths (when <code>is_dataset_root</code> is false), the base path points directly to the file directory, and the file path is appended directly without subdirectory prefixes.</p>"},{"location":"format/table/layout/#example-complex-layout-scenarios","title":"Example Complex Layout Scenarios","text":""},{"location":"format/table/layout/#hotcold-tiering","title":"Hot/Cold Tiering","text":"<pre><code>Manifest base_paths:\n[\n  { id: 0, is_dataset_root: true, path: \"s3://hot-bucket/dataset\" },\n  { id: 1, is_dataset_root: true, path: \"s3://cold-bucket/dataset-archive\" }\n]\n\nFragment 0 (recent data):\n  DataFile { path: \"fragment-0.lance\", base_id: 0 }\n  \u2192 resolves to: s3://hot-bucket/dataset/data/fragment-0.lance\n\nFragment 100 (historical data):\n  DataFile { path: \"fragment-100.lance\", base_id: 1 }\n  \u2192 resolves to: s3://cold-bucket/dataset-archive/data/fragment-100.lance\n</code></pre> <p>This allows seamless querying across storage tiers without data movement.</p>"},{"location":"format/table/layout/#multi-region-distribution","title":"Multi-Region Distribution","text":"<pre><code>Manifest base_paths:\n[\n  { id: 0, is_dataset_root: true, path: \"s3://us-east-bucket/dataset\" },\n  { id: 1, is_dataset_root: true, path: \"s3://eu-west-bucket/dataset\" },\n  { id: 2, is_dataset_root: true, path: \"s3://ap-south-bucket/dataset\" }\n]\n\nFragments distributed by data locality:\n  Fragment 0 (US users): base_id: 0\n  Fragment 1 (EU users): base_id: 1\n  Fragment 2 (Asia users): base_id: 2\n</code></pre> <p>Compute jobs can read data from the nearest region without data transfer.</p>"},{"location":"format/table/layout/#shallow-clone","title":"Shallow Clone","text":"<p>Shallow clones create a new dataset that references data files from a source dataset without copying:</p> <p>Example: Shallow Clone</p> <pre><code>Source dataset: s3://production/main-dataset\nClone dataset:  s3://experiments/test-variant\n\nClone manifest base_paths:\n[\n  { id: 0, is_dataset_root: true, path: \"s3://experiments/test-variant\" },\n  { id: 1, is_dataset_root: true, path: \"s3://production/main-dataset\",\n    name: \"v1.0\" }\n]\n\nOriginal fragments (inherited):\n  DataFile { path: \"fragment-0.lance\", base_id: 1 }\n  \u2192 resolves to: s3://production/main-dataset/data/fragment-0.lance\n\nNew fragments (clone-specific):\n  DataFile { path: \"fragment-new.lance\", base_id: 0 }\n  \u2192 resolves to: s3://experiments/test-variant/data/fragment-new.lance\n</code></pre> <p>The clone can append new data, modify schemas, or delete rows without affecting the source dataset. Only the manifest and new data files are stored in the clone location.</p> <p>Workflow:</p> <ol> <li>Clone transaction creates new manifest in target location</li> <li>Manifest includes base path pointing to source dataset</li> <li>Original fragments reference source via <code>base_id: 1</code></li> <li>Subsequent writes reference clone location via <code>base_id: 0</code></li> <li>Source dataset remains immutable and can be garbage collected independently</li> </ol>"},{"location":"format/table/layout/#dataset-portability","title":"Dataset Portability","text":"<p>The base path system combined with relative file references provides strong portability guarantees for Lance datasets. All file paths within Lance files are stored relative to their containing directory, enabling datasets to be relocated without file modifications.</p> <p>To port a dataset to a new location, simply copy all contents from the dataset root directory. The copied dataset will function immediately at the new location without any manifest updates, as all file references within the dataset root resolve through relative paths.</p> <p>When a dataset uses multiple base paths (such as in shallow clones or multi-bucket configurations), users have flexibility in how to port the dataset. The simplest approach is to copy only the dataset root, which preserves references to the original base path locations. Alternatively, users can copy additional base paths to the new location and update the manifest's <code>base_paths</code> array to reflect the new base paths. Since only the <code>base_paths</code> field in the manifest requires modification, this remains a lightweight metadata operation that does not require rewriting additional metadata or data files.</p>"},{"location":"format/table/layout/#file-naming-conventions","title":"File Naming Conventions","text":""},{"location":"format/table/layout/#data-files","title":"Data Files","text":"<p>Pattern: <code>data/{uuid-based-filename}.lance</code></p> <p>Data files use UUID-based filenames optimized for S3 throughput. The filename is generated from a UUID (16 bytes) by converting the first 3 bytes to a 24-character binary string and the remaining 13 bytes to a 26-character hex string, resulting in a 50-character filename. The binary prefix (rather than hex) provides maximum entropy per character, allowing S3's internal partitioning to quickly recognize access patterns and scale appropriately, minimizing throttling.</p> <p>Example: <code>data/101100101101010011010110a1b2c3d4e5f6g7h8i9j0.lance</code></p>"},{"location":"format/table/layout/#deletion-files","title":"Deletion Files","text":"<p>Pattern: <code>_deletions/{fragment_id}-{read_version}-{id}.{extension}</code></p> <p>Deletion files use two extensions: <code>.arrow</code> for Arrow IPC format (sparse deletions) and <code>.bin</code> for Roaring bitmap format (dense deletions).</p> <p>Example: <code>_deletions/42-10-a1b2c3d4.arrow</code></p>"},{"location":"format/table/layout/#transaction-files","title":"Transaction Files","text":"<p>Pattern: <code>_transactions/{read_version}-{uuid}.txn</code></p> <p>Where <code>read_version</code> is the table version the transaction was built from.</p> <p>Example: <code>_transactions/5-550e8400-e29b-41d4-a716-446655440000.txn</code></p>"},{"location":"format/table/layout/#manifest-files","title":"Manifest Files","text":"<p>Manifest files are stored in the <code>_versions/</code> directory with naming schemes that support atomic commits.</p> <p>See Manifest Naming Schemes for details on the V1 and V2 patterns and their implications for version discovery.</p>"},{"location":"format/table/row_id_lineage/","title":"Row ID and Lineage Specification","text":""},{"location":"format/table/row_id_lineage/#overview","title":"Overview","text":"<p>Lance provides row identification and lineage tracking capabilities. Row addressing enables efficient random access to rows within the table through a physical location encoding. Stable row IDs provide persistent identifiers that remain constant throughout a row's lifetime, even as its physical location changes. Row version tracking records when rows were created and last modified, enabling incremental processing, change data capture, and time-travel queries.</p>"},{"location":"format/table/row_id_lineage/#row-id-styles","title":"Row ID Styles","text":"<p>Lance uses two different styles of row IDs:</p>"},{"location":"format/table/row_id_lineage/#row-address","title":"Row Address","text":"<p>Row address is the physical location of a row in the table, represented as a 64-bit identifier composed of two 32-bit values:</p> <pre><code>row_address = (fragment_id &lt;&lt; 32) | local_row_offset\n</code></pre> <p>This addressing scheme enables efficient random access: given a row address, the fragment and offset are extracted with bit operations. Row addresses change when data is reorganized through compaction or updates.</p> <p>Row address is currently the primary form of identifier used for indexing purposes. Secondary indices (vector indices, scalar indices, full-text search indices) reference rows by their row addresses.</p> <p>Note</p> <p>Work to support stable row IDs in indices is in progress.</p>"},{"location":"format/table/row_id_lineage/#stable-row-id","title":"Stable Row ID","text":"<p>Stable Row ID is a unique auto-incrementing u64 identifier assigned to each row that remains constant throughout the row's lifetime,  even when the row's physical location (row address) changes. See the next section for more details.</p> <p>Warning</p> <p>Historically, \"row ID\" was used to mean row address interchangeably.    With the introduction of stable row IDs,    there could be places in code and documentation that mix the terms \"row ID\" and \"row address\" or \"row ID\" and \"stable row ID\".   Please raise a PR if you find any place incorrect or confusing.</p>"},{"location":"format/table/row_id_lineage/#stable-row-id_1","title":"Stable Row ID","text":""},{"location":"format/table/row_id_lineage/#row-id-assignment","title":"Row ID Assignment","text":"<p>Row IDs are assigned using a monotonically increasing <code>next_row_id</code> counter stored in the manifest.</p> <p>Assignment Protocol:</p> <ol> <li>Writer reads the current <code>next_row_id</code> from the manifest at the read version</li> <li>Writer assigns row IDs sequentially starting from <code>next_row_id</code> for new rows</li> <li>Writer updates <code>next_row_id</code> in the new manifest to <code>next_row_id + num_new_rows</code></li> <li>If commit fails due to conflict, writer rebases:</li> <li>Re-reads the new <code>next_row_id</code> from the latest version</li> <li>Reassigns row IDs to new rows using the updated counter</li> <li>Retries commit</li> </ol> <p>This protocol mirrors fragment ID assignment and ensures row IDs are unique across all table versions.</p>"},{"location":"format/table/row_id_lineage/#row-id-behavior-on-updates","title":"Row ID Behavior on Updates","text":"<p>When a row is updated, it is typically assigned a new row ID rather than reusing the old one. This avoids the complexity of updating secondary indices that may reference the old values.</p> <p>Update Workflow:</p> <ol> <li>Original row with ID <code>R</code> exists at address <code>(F1, O1)</code></li> <li>Update operation creates new row with ID <code>R'</code> at address <code>(F2, O2)</code></li> <li>Deletion vector marks row ID <code>R</code> as deleted in fragment <code>F1</code></li> <li>Secondary indices referencing old row ID <code>R</code> are invalidated through fragment bitmap updates</li> <li>New row ID <code>R'</code> requires index rebuild for affected columns</li> </ol> <p>This approach ensures secondary indices do not reference stale data.</p>"},{"location":"format/table/row_id_lineage/#row-id-sequences","title":"Row ID Sequences","text":""},{"location":"format/table/row_id_lineage/#storage-format","title":"Storage Format","text":"<p>Row ID sequences are stored using the <code>RowIdSequence</code> protobuf message. The sequence is partitioned into segments, each encoded optimally based on the data pattern.</p> RowIdSequence protobuf message <pre><code>message RowIdSequence {\n    repeated U64Segment segments = 1;\n\n}\n</code></pre>"},{"location":"format/table/row_id_lineage/#segment-encodings","title":"Segment Encodings","text":"<p>Each segment uses one of five encodings optimized for different data patterns:</p>"},{"location":"format/table/row_id_lineage/#range-contiguous-values","title":"Range (Contiguous Values)","text":"<p>For sorted, contiguous values with no gaps. Example: Row IDs <code>[100, 101, 102, 103, 104]</code> \u2192 <code>Range{start: 100, end: 105}</code>. Used for new fragments where row IDs are assigned sequentially.</p> Range protobuf message <pre><code>message Range {\n    /// The start of the range, inclusive.\n    uint64 start = 1;\n    /// The end of the range, exclusive.\n    uint64 end = 2;\n}\n</code></pre>"},{"location":"format/table/row_id_lineage/#range-with-holes-sparse-deletions","title":"Range with Holes (Sparse Deletions)","text":"<p>For sorted values with few gaps. Example: Row IDs <code>[100, 101, 103, 104]</code> (missing 102) \u2192 <code>RangeWithHoles{start: 100, end: 105, holes: [102]}</code>. Used for fragments with sparse deletions where maintaining the range is efficient.</p> RangeWithHoles protobuf message <pre><code>message RangeWithHoles {\n    /// The start of the range, inclusive.\n    uint64 start = 1;\n    /// The end of the range, exclusive.\n    uint64 end = 2;\n    /// The holes in the range, as a sorted array of values;\n    /// Binary search can be used to check whether a value is a hole and should\n    /// be skipped. This can also be used to count the number of holes before a\n    /// given value, if you need to find the logical offset of a value in the\n    /// segment.\n    EncodedU64Array holes = 3;\n}\n</code></pre>"},{"location":"format/table/row_id_lineage/#range-with-bitmap-dense-deletions","title":"Range with Bitmap (Dense Deletions)","text":"<p>For sorted values with many gaps. The bitmap encodes 8 values per byte, with the most significant bit representing the first value. Used for fragments with dense deletion patterns.</p> RangeWithBitmap protobuf message <pre><code>message RangeWithBitmap {\n    /// The start of the range, inclusive.\n    uint64 start = 1;\n    /// The end of the range, exclusive.\n    uint64 end = 2;\n    /// A bitmap of the values in the range. The bitmap is a sequence of bytes,\n    /// where each byte represents 8 values. The first byte represents values\n    /// start to start + 7, the second byte represents values start + 8 to\n    /// start + 15, and so on. The most significant bit of each byte represents\n    /// the first value in the range, and the least significant bit represents\n    /// the last value in the range. If the bit is set, the value is in the\n    /// range; if it is not set, the value is not in the range.\n    bytes bitmap = 3;\n}\n</code></pre>"},{"location":"format/table/row_id_lineage/#sorted-array-sparse-values","title":"Sorted Array (Sparse Values)","text":"<p>For sorted but non-contiguous values, stored as an <code>EncodedU64Array</code>. Used for merged fragments or fragments after compaction.</p>"},{"location":"format/table/row_id_lineage/#unsorted-array-general-case","title":"Unsorted Array (General Case)","text":"<p>For unsorted values, stored as an <code>EncodedU64Array</code>. Rare; most operations maintain sorted order.</p>"},{"location":"format/table/row_id_lineage/#encoded-u64-arrays","title":"Encoded U64 Arrays","text":"<p>The <code>EncodedU64Array</code> message supports bitpacked encoding to minimize storage. The implementation selects the most compact encoding based on the value range, choosing between base + 16-bit offsets, base + 32-bit offsets, or full 64-bit values.</p> EncodedU64Array protobuf message <pre><code>message EncodedU64Array {\n    message U16Array {\n        uint64 base = 1;\n        /// The deltas are stored as 16-bit unsigned integers.\n        /// (protobuf doesn't support 16-bit integers, so we use bytes instead)\n        bytes offsets = 2;\n    }\n\n    message U32Array {\n        uint64 base = 1;\n        /// The deltas are stored as 32-bit unsigned integers.\n        /// (we use bytes instead of uint32 to avoid overhead of varint encoding)\n        bytes offsets = 2;\n    }\n\n    message U64Array {\n        /// (We use bytes instead of uint64 to avoid overhead of varint encoding)\n        bytes values = 2;\n    }\n\n    oneof array {\n        U16Array u16_array = 1;\n        U32Array u32_array = 2;\n        U64Array u64_array = 3;\n    }\n\n}\n</code></pre>"},{"location":"format/table/row_id_lineage/#inline-vs-external-storage","title":"Inline vs External Storage","text":"<p>Row ID sequences are stored either inline in the fragment metadata or in external files. Sequences smaller than ~200KB are stored inline to avoid additional I/O, while larger sequences are written to external files referenced by path and offset. This threshold balances manifest size against the overhead of separate file reads.</p> DataFragment row_id_sequence field <pre><code>message DataFragment {\n  oneof row_id_sequence {\n    bytes inline_row_ids = 5;\n    ExternalFile external_row_ids = 6;\n  }\n}\n</code></pre>"},{"location":"format/table/row_id_lineage/#row-id-index","title":"Row ID Index","text":""},{"location":"format/table/row_id_lineage/#construction","title":"Construction","text":"<p>The row ID index is built at table load time by aggregating row ID sequences from all fragments:</p> <pre><code>For each fragment F with ID f:\n  For each (position p, row_id r) in F.row_id_sequence:\n    index[r] = (f, p)\n</code></pre> <p>This creates a mapping from row ID to current row address.</p>"},{"location":"format/table/row_id_lineage/#index-invalidation-with-updates","title":"Index Invalidation with Updates","text":"<p>When rows are updated, the row ID index must account for stale mappings:</p> <p>Example Scenario:</p> <ol> <li>Initial state: Fragment 1 contains rows with IDs <code>[1, 2, 3]</code> at offsets <code>[0, 1, 2]</code></li> <li>Update operation modifies row 2:</li> <li>New fragment 2 created with row ID <code>4</code> (new ID assigned)</li> <li>Deletion vector marks row ID <code>2</code> as deleted in fragment 1</li> <li>Row ID index:</li> <li><code>1 \u2192 (1, 0)</code> \u2713 Valid</li> <li><code>2 \u2192 (1, 1)</code> \u2717 Invalid (deleted)</li> <li><code>3 \u2192 (1, 2)</code> \u2713 Valid</li> <li><code>4 \u2192 (2, 0)</code> \u2713 Valid (new row)</li> </ol>"},{"location":"format/table/row_id_lineage/#fragment-bitmaps-for-index-masking","title":"Fragment Bitmaps for Index Masking","text":"<p>Secondary indices use fragment bitmaps to track which row IDs remain valid:</p> <p>Without Row ID Updates:</p> <pre><code>String Index on column \"str\":\n  Fragment Bitmap: {1, 2}  (covers fragments 1 and 2)\n  All indexed row IDs are valid\n</code></pre> <p>With Row ID Updates:</p> <pre><code>Vector Index on column \"vec\":\n  Fragment Bitmap: {1}  (only fragment 1)\n  Row ID 2 was updated, so index entry for ID 2 is stale\n  Index query filters out ID 2 using deletion vectors\n</code></pre> <p>This bitmap-based approach allows indices to remain immutable while accounting for row modifications.</p>"},{"location":"format/table/row_id_lineage/#row-version-tracking","title":"Row Version Tracking","text":""},{"location":"format/table/row_id_lineage/#created-at-version","title":"Created At Version","text":"<p>Each row tracks the version at which it was created. The sequence uses run-length encoding for efficient storage, where each run specifies a span of consecutive rows and the version they were created in.</p> <p>Example: Fragment with 1000 rows created in version 5: <pre><code>RowDatasetVersionSequence {\n  runs: [\n    RowDatasetVersionRun { span: Range{start: 0, end: 1000}, version: 5 }\n  ]\n}\n</code></pre></p> DataFragment created_at_version_sequence field <pre><code>message DataFragment {\n  oneof created_at_version_sequence {\n    bytes inline_created_at_versions = 9;\n    ExternalFile external_created_at_versions = 10;\n  }\n}\n</code></pre> RowDatasetVersionSequence protobuf messages <pre><code>message RowDatasetVersionSequence {\n    repeated RowDatasetVersionRun runs = 1;\n\n}\n</code></pre>"},{"location":"format/table/row_id_lineage/#last-updated-at-version","title":"Last Updated At Version","text":"<p>Each row tracks the version at which it was last modified. When a row is created, <code>last_updated_at_version</code> equals <code>created_at_version</code>. When a row is updated, a new row is created with both <code>created_at_version</code> and <code>last_updated_at_version</code> set to the current version, and the old row is marked deleted.</p> <p>Example: Row created in version 3, updated in version 7: <pre><code>Old row (marked deleted):\n  created_at_version: 3\n  last_updated_at_version: 3\n\nNew row:\n  created_at_version: 7\n  last_updated_at_version: 7\n</code></pre></p> DataFragment last_updated_at_version_sequence field <pre><code>message DataFragment {\n  oneof last_updated_at_version_sequence {\n    bytes inline_last_updated_at_versions = 7;\n    ExternalFile external_last_updated_at_versions = 8;\n  }\n}\n</code></pre>"},{"location":"format/table/row_id_lineage/#change-data-feed","title":"Change Data Feed","text":"<p>Lance supports querying rows that changed between versions through version tracking columns. These queries can be expressed as standard SQL predicates on the <code>_row_created_at_version</code> and <code>_row_last_updated_at_version</code> columns.</p>"},{"location":"format/table/row_id_lineage/#inserted-rows","title":"Inserted Rows","text":"<p>Rows created between two versions can be retrieved by filtering on <code>_row_created_at_version</code>:</p> <pre><code>SELECT * FROM dataset\nWHERE _row_created_at_version &gt; {begin_version}\n  AND _row_created_at_version &lt;= {end_version}\n</code></pre> <p>This query returns all rows inserted in the specified version range, including the version metadata columns <code>_row_created_at_version</code>, <code>_row_last_updated_at_version</code>, and <code>_rowid</code>.</p>"},{"location":"format/table/row_id_lineage/#updated-rows","title":"Updated Rows","text":"<p>Rows modified (but not newly created) between two versions can be retrieved by combining filters on both version columns:</p> <pre><code>SELECT * FROM dataset\nWHERE _row_created_at_version &lt;= {begin_version}\n  AND _row_last_updated_at_version &gt; {begin_version}\n  AND _row_last_updated_at_version &lt;= {end_version}\n</code></pre> <p>This query excludes newly inserted rows by requiring <code>_row_created_at_version &lt;= {begin_version}</code>, ensuring only pre-existing rows that were subsequently updated are returned.</p>"},{"location":"format/table/transaction/","title":"Transaction Specification","text":""},{"location":"format/table/transaction/#transaction-overview","title":"Transaction Overview","text":"<p>Lance implements Multi-Version Concurrency Control (MVCC) to provide ACID transaction guarantees for concurrent readers and writers. Each commit creates a new immutable table version through atomic storage operations. All table versions form a serializable history, enabling features such as time travel and schema evolution.</p> <p>Transactions are the fundamental unit of change in Lance. A transaction describes a set of modifications to be applied atomically to create a new table version. The transaction model supports concurrent writes through optimistic concurrency control with automatic conflict resolution.</p>"},{"location":"format/table/transaction/#commit-protocol","title":"Commit Protocol","text":""},{"location":"format/table/transaction/#storage-primitives","title":"Storage Primitives","text":"<p>Lance commits rely on atomic write operations provided by the underlying object store:</p> <ul> <li>rename-if-not-exists: Atomically rename a file only if the target does not exist</li> <li>put-if-not-exists: Atomically write a file only if it does not already exist (also known as PUT-IF-NONE-MATCH or conditional PUT)</li> </ul> <p>These primitives guarantee that exactly one writer succeeds when multiple writers attempt to create the same manifest file concurrently.</p>"},{"location":"format/table/transaction/#manifest-naming-schemes","title":"Manifest Naming Schemes","text":"<p>Lance supports two manifest naming schemes:</p> <ul> <li>V1: <code>{version}.manifest</code> - Monotonically increasing version numbers (e.g., <code>1.manifest</code>, <code>2.manifest</code>)</li> <li>V2: <code>{u64::MAX - version:020}.manifest</code> - Reverse-sorted lexicographic ordering (e.g., <code>18446744073709551614.manifest</code> for version 1)</li> </ul> <p>The V2 scheme enables efficient discovery of the latest version through lexicographic object listing.</p>"},{"location":"format/table/transaction/#transaction-files","title":"Transaction Files","text":"<p>Transaction files store the serialized transaction protobuf message for each commit attempt. These files serve two purposes:</p> <ol> <li>Enable manifest reconstruction during commit retries when concurrent transactions have been committed</li> <li>Support conflict detection by describing the operation performed</li> </ol>"},{"location":"format/table/transaction/#commit-algorithm","title":"Commit Algorithm","text":"<p>The commit process attempts to atomically write a new manifest file using the storage primitives described above. When concurrent writers conflict, the system loads transaction files to detect conflicts and attempts to rebase the transaction if possible. If the atomic commit fails, the process retries with updated transaction state. For detailed conflict detection and resolution mechanisms, see the Conflict Resolution section.</p>"},{"location":"format/table/transaction/#transaction-types","title":"Transaction Types","text":"<p>The authoritative specification for transaction types is defined in <code>protos/transaction.proto</code>.</p> <p>Each transaction contains a <code>read_version</code> field indicating the table version from which the transaction was built, a <code>uuid</code> field uniquely identifying the transaction, and an <code>operation</code> field specifying one of the following transaction types:</p>"},{"location":"format/table/transaction/#append","title":"Append","text":"<p>Adds new fragments to the table without modifying existing data. Fragment IDs are not assigned at transaction creation time; they are assigned during manifest construction.</p> Append protobuf message <pre><code>message Append {\n  // The new fragments to append.\n  //\n  // Fragment IDs are not yet assigned.\n  repeated DataFragment fragments = 1;\n}\n</code></pre>"},{"location":"format/table/transaction/#delete","title":"Delete","text":"<p>Marks rows as deleted using deletion vectors. May update fragments (adding deletion vectors) or delete entire fragments. The <code>predicate</code> field stores the deletion condition, enabling conflict detection with concurrent transactions.</p> Delete protobuf message <pre><code>message Delete {\n  // The fragments to update\n  //\n  // The fragment IDs will match existing fragments in the dataset.\n  repeated DataFragment updated_fragments = 1;\n  // The fragments to delete entirely.\n  repeated uint64 deleted_fragment_ids = 2;\n  // The predicate that was evaluated\n  //\n  // This may be used to determine whether the delete would have affected \n  // files written by a concurrent transaction.\n  string predicate = 3;\n}\n</code></pre>"},{"location":"format/table/transaction/#overwrite","title":"Overwrite","text":"<p>Creates or completely overwrites the table with new data, schema, and configuration.</p> Overwrite protobuf message <pre><code>message Overwrite {\n  // The new fragments\n  //\n  // Fragment IDs are not yet assigned.\n  repeated DataFragment fragments = 1;\n  // The new schema\n  repeated lance.file.Field schema = 2;\n  // Schema metadata.\n  map&lt;string, bytes&gt; schema_metadata = 3;\n  // Key-value pairs to merge with existing config.\n  map&lt;string, string&gt; config_upsert_values = 4;\n  // The base paths to be added for the initial dataset creation\n  repeated BasePath initial_bases = 5;\n}\n</code></pre>"},{"location":"format/table/transaction/#createindex","title":"CreateIndex","text":"<p>Adds, replaces, or removes secondary indices (vector indices, scalar indices, full-text search indices).</p> CreateIndex protobuf message <pre><code>message CreateIndex {\n  repeated IndexMetadata new_indices = 1;\n  repeated IndexMetadata removed_indices = 2;\n}\n</code></pre>"},{"location":"format/table/transaction/#rewrite","title":"Rewrite","text":"<p>Reorganizes data without semantic modification. This includes operations such as compaction, defragmentation, and re-ordering. Rewrite operations change row addresses, requiring index updates. New fragment IDs must be reserved via <code>ReserveFragments</code> before executing a <code>Rewrite</code> transaction.</p> Rewrite protobuf message <pre><code>message Rewrite {\n  // The old fragments that are being replaced\n  //\n  // DEPRECATED: use groups instead.\n  //\n  // These should all have existing fragment IDs.\n  repeated DataFragment old_fragments = 1;\n  // The new fragments\n  //\n  // DEPRECATED: use groups instead.\n  //\n  // These fragments IDs are not yet assigned.\n  repeated DataFragment new_fragments = 2;\n\n  // During a rewrite an index may be rewritten.  We only serialize the UUID\n  // since a rewrite should not change the other index parameters.\n  message RewrittenIndex {\n    // The id of the index that will be replaced\n    UUID old_id = 1;\n    // the id of the new index\n    UUID new_id = 2;\n    // the new index details\n    google.protobuf.Any new_index_details = 3;\n    // the version of the new index\n    uint32 new_index_version = 4;\n  }\n\n  // A group of rewrite files that are all part of the same rewrite.\n  message RewriteGroup {\n    // The old fragment that is being replaced\n    //\n    // This should have an existing fragment ID.\n    repeated DataFragment old_fragments = 1;\n    // The new fragment\n    //\n    // The ID should have been reserved by an earlier\n    // reserve operation\n    repeated DataFragment new_fragments = 2;\n  }\n\n  // Groups of files that have been rewritten\n  repeated RewriteGroup groups = 3;\n  // Indices that have been rewritten\n  repeated RewrittenIndex rewritten_indices = 4;\n}\n</code></pre>"},{"location":"format/table/transaction/#merge","title":"Merge","text":"<p>Adds new columns to the table, modifying the schema. All fragments must be updated to include the new columns.</p> Merge protobuf message <pre><code>message Merge {\n  // The updated fragments\n  //\n  // These should all have existing fragment IDs.\n  repeated DataFragment fragments = 1;\n  // The new schema\n  repeated lance.file.Field schema = 2;\n  // Schema metadata.\n  map&lt;string, bytes&gt; schema_metadata = 3;\n}\n</code></pre>"},{"location":"format/table/transaction/#project","title":"Project","text":"<p>Removes columns from the table, modifying the schema. This is a metadata-only operation; data files are not modified.</p> Project protobuf message <pre><code>message Project {\n  // The new schema\n  repeated lance.file.Field schema = 1;\n}\n</code></pre>"},{"location":"format/table/transaction/#restore","title":"Restore","text":"<p>Reverts the table to a previous version.</p> Restore protobuf message <pre><code>message Restore {\n  // The version to restore to\n  uint64 version = 1;\n}\n</code></pre>"},{"location":"format/table/transaction/#reservefragments","title":"ReserveFragments","text":"<p>Pre-allocates fragment IDs for use in future <code>Rewrite</code> operations. This allows rewrite operations to reference fragment IDs before the rewrite transaction is committed.</p> ReserveFragments protobuf message <pre><code>message ReserveFragments {\n  uint32 num_fragments = 1;\n}\n</code></pre>"},{"location":"format/table/transaction/#clone","title":"Clone","text":"<p>Creates a shallow or deep copy of the table. Shallow clones are metadata-only copies that reference original data files through <code>base_paths</code>. Deep clones are full copies using object storage native copy operations (e.g., S3 CopyObject).</p> Clone protobuf message <pre><code>message Clone {\n  // - true:  Performs a metadata-only clone (copies manifest without data files).\n  //          The cloned dataset references original data through `base_paths`,\n  //          suitable for experimental scenarios or rapid metadata migration.\n  // - false: Performs a full deep clone using the underlying object storage's native\n  //          copy API (e.g., S3 CopyObject, GCS rewrite). This leverages server-side\n  //          bulk copy operations to bypass download/upload bottlenecks, achieving\n  //          near-linear speedup for large datasets (typically 3-10x faster than\n  //          manual file transfers). The operation maintains atomicity and data\n  //          integrity guarantees provided by the storage backend.\n  bool is_shallow = 1;\n  // the reference name in the source dataset\n  // in most cases it should be the the branch or tag name in the source dataset\n  optional string ref_name = 2;\n  // the version of the source dataset for cloning\n  uint64 ref_version = 3;\n  // the absolute base path of the source dataset for cloning\n  string ref_path = 4;\n  // if the target dataset is a branch, this is the branch name of the target dataset\n  optional string branch_name = 5;\n}\n</code></pre>"},{"location":"format/table/transaction/#update","title":"Update","text":"<p>Modifies row values without adding or removing rows. Supports two execution modes: REWRITE_ROWS deletes rows in current fragments and rewrites them in new fragments, which is optimal when the majority of columns are modified or only a small number of rows are affected; REWRITE_COLUMNS fully rewrites affected columns within fragments by tombstoning old column versions, which is optimal when most rows are affected but only a subset of columns are modified.</p> Update protobuf message <pre><code>message Update {\n  // The fragments that have been removed. These are fragments where all rows\n  // have been updated and moved to a new fragment.\n  repeated uint64 removed_fragment_ids = 1;\n  // The fragments that have been updated.\n  repeated DataFragment updated_fragments = 2;\n  // The new fragments where updated rows have been moved to.\n  repeated DataFragment new_fragments = 3;\n  // The ids of the fields that have been modified.\n  repeated uint32 fields_modified = 4;\n  /// The MemWAL (pre-image) that should be marked as merged after this transaction\n  MemWalIndexDetails.MemWal mem_wal_to_merge = 5;\n  /// The fields that used to judge whether to preserve the new frag's id into\n  /// the frag bitmap of the specified indices.\n  repeated uint32 fields_for_preserving_frag_bitmap = 6;\n  // The mode of update\n  UpdateMode update_mode = 7;\n}\n</code></pre>"},{"location":"format/table/transaction/#updateconfig","title":"UpdateConfig","text":"<p>Modifies table configuration, table metadata, schema metadata, or field metadata without changing data.</p> UpdateConfig protobuf message <pre><code>message UpdateConfig {\n  UpdateMap config_updates = 6;\n  UpdateMap table_metadata_updates = 7;\n  UpdateMap schema_metadata_updates = 8;\n  map&lt;int32, UpdateMap&gt; field_metadata_updates = 9;\n\n  // Deprecated -------------------------------\n  map&lt;string, string&gt; upsert_values = 1;\n  repeated string delete_keys = 2;\n  map&lt;string, string&gt; schema_metadata = 3;\n  map&lt;uint32, FieldMetadataUpdate&gt; field_metadata = 4;\n\n  message FieldMetadataUpdate {\n    map&lt;string, string&gt; metadata = 5;\n  }\n}\n</code></pre>"},{"location":"format/table/transaction/#datareplacement","title":"DataReplacement","text":"<p>Replaces data in specific column regions with new data files.</p> DataReplacement protobuf message <pre><code>message DataReplacement {\n  repeated DataReplacementGroup replacements = 1;\n}\n</code></pre>"},{"location":"format/table/transaction/#updatememwalstate","title":"UpdateMemWalState","text":"<p>Updates the state of MemWal indices (write-ahead log based indices).</p> UpdateMemWalState protobuf message <pre><code>message UpdateMemWalState {\n\n  repeated MemWalIndexDetails.MemWal added = 1;\n\n  repeated MemWalIndexDetails.MemWal updated = 2;\n\n  // If a MemWAL is updated, its pre-image should be in the removed list.\n  repeated MemWalIndexDetails.MemWal removed = 3;\n}\n</code></pre>"},{"location":"format/table/transaction/#updatebases","title":"UpdateBases","text":"<p>Adds new base paths to the table, enabling reference to data files in additional locations.</p> UpdateBases protobuf message <pre><code>message UpdateBases {\n  // The new base paths to add to the manifest.\n  repeated BasePath new_bases = 1;\n}\n</code></pre>"},{"location":"format/table/transaction/#conflict-resolution","title":"Conflict Resolution","text":""},{"location":"format/table/transaction/#terminology","title":"Terminology","text":"<p>When concurrent transactions attempt to commit against the same read version, Lance employs conflict resolution to determine whether the transactions can coexist. Three outcomes are possible:</p> <ul> <li> <p>Rebasable: The transaction can be modified to incorporate concurrent changes while preserving its semantic intent.   The transaction is transformed to account for the concurrent modification, then the commit is retried automatically within the commit layer.</p> </li> <li> <p>Retryable: The transaction cannot be rebased, but the operation can be re-executed at the application level with updated data.   The implementation returns a retryable conflict error, signaling that the application should re-read the data and retry the operation.   The retried operation is expected to produce semantically equivalent results.</p> </li> <li> <p>Incompatible: The transactions conflict in a fundamental way where retrying would violate the operation's assumptions or produce semantically different results than expected.   The commit fails with a non-retryable error.   Callers should proceed with extreme caution if they decide to retry, as the transaction may produce different output than originally intended.</p> </li> </ul>"},{"location":"format/table/transaction/#rebase-mechanism","title":"Rebase Mechanism","text":"<p>The <code>TransactionRebase</code> structure tracks the state necessary to rebase a transaction against concurrent commits:</p> <ol> <li>Fragment tracking: Maintains a map of fragments as they existed at the transaction's read version, marking which require rewriting</li> <li>Modification detection: Tracks the set of fragment IDs that have been modified or deleted</li> <li>Affected rows: For Delete and Update operations, stores the specific rows affected by the operation for fine-grained conflict detection</li> <li>Fragment reuse indices: Accumulates fragment reuse index metadata from concurrent Rewrite operations</li> </ol> <p>When a concurrent transaction is detected, the rebase process:</p> <ol> <li>Compares fragment modifications to determine if there is overlap</li> <li>For Delete/Update operations, compares <code>affected_rows</code> to detect whether the same rows were modified</li> <li>Merges deletion vectors when both transactions delete rows from the same fragment</li> <li>Accumulates fragment reuse index updates when concurrent Rewrites change fragment IDs</li> <li>Modifies the transaction if rebasable, or returns a retryable/incompatible conflict error</li> </ol>"},{"location":"format/table/transaction/#conflict-scenarios","title":"Conflict Scenarios","text":""},{"location":"format/table/transaction/#rebasable-conflict-example","title":"Rebasable Conflict Example","text":"<p>The following diagram illustrates a rebasable conflict where two Delete operations modify different rows in the same fragment:</p> <pre><code>gitGraph\n    commit id: \"v1\"\n    commit id: \"v2\"\n    branch writer-a\n    branch writer-b\n    checkout writer-a\n    commit id: \"Delete rows 100-199\" tag: \"read_version=2\"\n    checkout writer-b\n    commit id: \"Delete rows 500-599\" tag: \"read_version=2\"\n    checkout main\n    merge writer-a tag: \"v3\"\n    checkout writer-b\n    commit id: \"Rebase: merge deletion vectors\" type: HIGHLIGHT\n    checkout main\n    merge writer-b tag: \"v4\"</code></pre> <p>In this scenario:</p> <ul> <li>Writer A deletes rows 100-199 and successfully commits version 3</li> <li>Writer B attempts to commit but detects version 3 exists</li> <li>Writer B's transaction is rebasable because it only modified deletion vectors (not data files) and <code>affected_rows</code> do not overlap</li> <li>Writer B rebases by merging Writer A's deletion vector with its own, write it to storage</li> <li>Writer B successfully commits version 4</li> </ul>"},{"location":"format/table/transaction/#retryable-conflict-example","title":"Retryable Conflict Example","text":"<p>The following diagram illustrates a retryable conflict where an Update operation encounters a concurrent Rewrite (compaction) that prevents automatic rebasing:</p> <pre><code>gitGraph\n    commit id: \"v1\"\n    commit id: \"v2\"\n    branch writer-a\n    branch writer-b\n    checkout writer-a\n    commit id: \"Compact fragments 1-5\" tag: \"read_version=2\"\n    checkout writer-b\n    commit id: \"Update rows in fragment 3\" tag: \"read_version=2\"\n    checkout main\n    merge writer-a tag: \"v3: fragments compacted\"\n    checkout writer-b\n    commit id: \"Detect conflict: cannot rebase\" type: REVERSE</code></pre> <p>In this scenario:</p> <ul> <li>Writer A compacts fragments 1-5 into a single fragment and successfully commits version 3</li> <li>Writer B attempts to update rows in fragment 3 but detects version 3 exists</li> <li>Writer B's Update transaction is retryable but not rebasable: fragment 3 no longer exists after compaction</li> <li>The commit layer returns a retryable conflict error</li> <li>The application must re-execute the Update operation against version 3, locating the rows in the new compacted fragment</li> </ul>"},{"location":"format/table/transaction/#incompatible-conflict-example","title":"Incompatible Conflict Example","text":"<p>The following diagram illustrates an incompatible conflict where a Delete operation encounters a concurrent Restore that fundamentally invalidates the operation:</p> <pre><code>gitGraph\n    commit id: \"v1\"\n    commit id: \"v2\"\n    commit id: \"v3\"\n    branch writer-a\n    branch writer-b\n    checkout writer-a\n    commit id: \"Restore to v1\" tag: \"read_version=3\"\n    checkout writer-b\n    commit id: \"Delete rows added in v2-v3\" tag: \"read_version=3\"\n    checkout main\n    merge writer-a tag: \"v4: restored to v1\"\n    checkout writer-b\n    commit id: \"Detect conflict: incompatible\" type: REVERSE</code></pre> <p>In this scenario:</p> <ul> <li>Writer A restores the table to version 1 and successfully commits version 4</li> <li>Writer B attempts to delete rows that were added between versions 2 and 3</li> <li>Writer B's Delete transaction is incompatible: the table has been restored to version 1, and the rows it intended to delete no longer exist</li> <li>The commit fails with a non-retryable error</li> <li>If the caller retries the deletion operation against version 4, it would either delete nothing (if those rows don't exist in v1) or delete different rows (if similar row IDs exist in v1), producing semantically different results than originally intended</li> </ul>"},{"location":"format/table/transaction/#external-manifest-store","title":"External Manifest Store","text":"<p>If the backing object store does not support atomic operations (rename-if-not-exists or put-if-not-exists), an external manifest store can be used to enable concurrent writers.</p> <p>An external manifest store is a key-value store that supports put-if-not-exists operations. The external manifest store supplements but does not replace the manifests in object storage. A reader unaware of the external manifest store can still read the table, but may observe a version up to one commit behind the true latest version.</p>"},{"location":"format/table/transaction/#commit-process-with-external-store","title":"Commit Process with External Store","text":"<p>The commit process follows a four-step protocol:</p> <p></p> <ol> <li> <p>Stage manifest: <code>PUT_OBJECT_STORE {dataset}/_versions/{version}.manifest-{uuid}</code></p> <ul> <li>Write the new manifest to object storage under a unique path determined by a new UUID</li> <li>This staged manifest is not yet visible to readers</li> </ul> </li> <li> <p>Commit to external store: <code>PUT_EXTERNAL_STORE base_uri, version, {dataset}/_versions/{version}.manifest-{uuid}</code></p> <ul> <li>Atomically commit the path of the staged manifest to the external store using put-if-not-exists</li> <li>The commit is effectively complete after this step</li> <li>If this operation fails due to conflict, another writer has committed this version</li> </ul> </li> <li> <p>Finalize in object store: <code>COPY_OBJECT_STORE {dataset}/_versions/{version}.manifest-{uuid} \u2192 {dataset}/_versions/{version}.manifest</code></p> <ul> <li>Copy the staged manifest to the final path</li> <li>This makes the manifest discoverable by readers unaware of the external store</li> </ul> </li> <li> <p>Update external store pointer: <code>PUT_EXTERNAL_STORE base_uri, version, {dataset}/_versions/{version}.manifest</code></p> <ul> <li>Update the external store to point to the finalized manifest path</li> <li>Completes the synchronization between external store and object storage</li> </ul> </li> </ol> <p>Fault Tolerance:</p> <p>If the writer fails after step 2 but before step 4, the external store and object store are temporarily out of sync. Readers detect this condition and attempt to complete the synchronization. If synchronization fails, the reader refuses to load to ensure dataset portability.</p>"},{"location":"format/table/transaction/#reader-process-with-external-store","title":"Reader Process with External Store","text":"<p>The reader follows a validation and synchronization protocol:</p> <p></p> <ol> <li> <p>Query external store: <code>GET_EXTERNAL_STORE base_uri, version</code> \u2192 <code>path</code></p> <ul> <li>Retrieve the manifest path for the requested version</li> <li>If the path does not end with a UUID, return it directly (synchronization complete)</li> <li>If the path ends with a UUID, synchronization is required</li> </ul> </li> <li> <p>Synchronize to object store: <code>COPY_OBJECT_STORE {dataset}/_versions/{version}.manifest-{uuid} \u2192 {dataset}/_versions/{version}.manifest</code></p> <ul> <li>Attempt to finalize the staged manifest</li> <li>This operation is idempotent</li> </ul> </li> <li> <p>Update external store: <code>PUT_EXTERNAL_STORE base_uri, version, {dataset}/_versions/{version}.manifest</code></p> <ul> <li>Update the external store to reflect the finalized path</li> <li>Future readers will see the synchronized state</li> </ul> </li> <li> <p>Return finalized path: Return <code>{dataset}/_versions/{version}.manifest</code></p> <ul> <li>Always return the finalized path</li> <li>If synchronization fails, return an error to prevent reading inconsistent state</li> </ul> </li> </ol> <p>This protocol ensures that datasets using external manifest stores remain portable: copying the dataset directory preserves all data without requiring the external store.</p>"},{"location":"format/table/versioning/","title":"Format Versioning","text":""},{"location":"format/table/versioning/#feature-flags","title":"Feature Flags","text":"<p>As the table format evolves, new feature flags are added to the format. There are two separate fields for checking for feature flags, depending on whether you are trying to read or write the table. Readers should check the <code>reader_feature_flags</code> to see if there are any flag it is not aware of. Writers should check <code>writer_feature_flags</code>. If either sees a flag they don't know, they should return an \"unsupported\" error on any read or write operation.</p>"},{"location":"format/table/versioning/#current-feature-flags","title":"Current Feature Flags","text":"Flag Bit Flag Name Reader Required Writer Required Description 1 <code>FLAG_DELETION_FILES</code> Yes Yes Fragments may contain deletion files, which record the tombstones of soft-deleted rows. 2 <code>FLAG_STABLE_ROW_IDS</code> Yes Yes Row IDs are stable for both moves and updates. Fragments contain an index mapping row IDs to row addresses. 4 <code>FLAG_USE_V2_FORMAT_DEPRECATED</code> No No Files are written with the new v2 format. This flag is deprecated and no longer used. 8 <code>FLAG_TABLE_CONFIG</code> No Yes Table config is present in the manifest. 16 <code>FLAG_BASE_PATHS</code> Yes Yes Dataset uses multiple base paths (for shallow clones or multi-base datasets). <p>Flags with bit values 32 and above are unknown and will cause implementations to reject the dataset with an \"unsupported\" error.</p>"},{"location":"format/table/index/","title":"Indices in Lance","text":"<p>Lance supports three main categories of indices to accelerate data access:</p> <ol> <li>Scalar Indices - Traditional indices for accelerating various database query patterns</li> <li>Vector Indices - Specialized indices for vector search</li> <li>System Indices - Auxiliary indices for accelerating internal system operations</li> </ol>"},{"location":"format/table/index/#index-section-in-manifest","title":"Index Section in Manifest","text":"<p>Lance main protobuf manifest stores the file position of the index section, so that the index section is not loaded when the dataset is opened, and only loaded when needed:</p> <pre><code>optional uint64 index_section = 6;\n</code></pre>"},{"location":"format/table/index/#index-metadata","title":"Index Metadata","text":"<p>Index section stores a list of index metadata:</p> <pre><code>message IndexSection {\n  repeated IndexMetadata indices = 1;\n\n}\n\nmessage IndexMetadata {\n  // Unique ID of an index. It is unique across all the dataset versions.\n  UUID uuid = 1;\n\n  // The columns to build the index. These refer to file.Field.id.\n  repeated int32 fields = 2;\n\n  // Index name. Must be unique within one dataset version.\n  string name = 3;\n\n  // The version of the dataset this index was built from.\n  uint64 dataset_version = 4;\n\n  // A bitmap of the included fragment ids.\n  //\n  // This may by used to determine how much of the dataset is covered by the\n  // index. This information can be retrieved from the dataset by looking at\n  // the dataset at `dataset_version`. However, since the old version may be\n  // deleted while the index is still in use, this information is also stored\n  // in the index.\n  //\n  // The bitmap is stored as a 32-bit Roaring bitmap.\n  bytes fragment_bitmap = 5;\n\n  // Details, specific to the index type, which are needed to load / interpret the index\n  //\n  // Indices should avoid putting large amounts of information in this field, as it will\n  // bloat the manifest.\n  google.protobuf.Any index_details = 6;\n\n  // The minimum lance version that this index is compatible with.\n  optional int32 index_version = 7;\n\n  // Timestamp when the index was created (UTC timestamp in milliseconds since epoch)\n  //\n  // This field is optional for backward compatibility. For existing indices created before\n  // this field was added, this will be None/null.\n  optional uint64 created_at = 8;\n\n  // The base path index of the data file. Used when the file is imported or referred from another dataset.\n  // Lance use it as key of the base_paths field in Manifest to determine the actual base path of the data file.\n  optional uint32 base_id = 9;\n\n}\n</code></pre>"},{"location":"format/table/index/#index-id-name-and-delta-indices","title":"Index ID, Name and Delta Indices","text":"<p>Each index has a unique UUID. Multiple indices of different IDs can share the same name. When this happens, these indices are called Delta Indices because they together form a complete index. Delta indices are typically used when the index is updated incrementally to avoid full rebuild. The Lance SDK provides functions for users to choose when to create delta indices, and when to merge them back into a single index.</p>"},{"location":"format/table/index/#index-coverage-and-fragment-bitmap","title":"Index Coverage and Fragment Bitmap","text":"<p>An index records the fragments it covers using a bitmap of the <code>uint32</code> fragment IDs,  so that during the query planning phase, Lance can generate a split plan to leverage the index for covered fragments, and perform scan for uncovered fragments and merge the results.</p>"},{"location":"format/table/index/#index-remap-and-row-address","title":"Index Remap and Row Address","text":"<p>In general, indices describe how to find a row address based on some value of a column. For example, a B-tree index can be used to find the row address of a specific value in a sorted array.</p> <p>When compaction happens, because the row address has changed and some delete markers are removed, the index needs to be updated accordingly. This update is fast because it's a pure mapping operation to delete some values or change the old row address to the new row address. We call this process Index Remap. For more details, see Fragment Reuse Index</p>"},{"location":"format/table/index/#stable-row-id-for-index","title":"Stable Row ID for Index","text":"<p>Using a stable row ID to replace the row address for an index is a work in progress. The main benefit is that remap is not needed, and an update only needs to invalidate the index if related column data has changed. The tradeoff is that it requires an additional index search to translate a stable row ID to the physical row address. We are still working on evaluating the performance impact of this change before making it more widely used.</p>"},{"location":"format/table/index/#index-storage","title":"Index Storage","text":"<p>The content of each index is stored at <code>_indices/{UUID}</code> directory under the dataset directory. We call this location the index directory. The actual content stored in the index directory depends on the index type.</p>"},{"location":"format/table/index/scalar/bitmap/","title":"Bitmap Index","text":"<p>Bitmap indices use bit arrays to represent the presence or absence of values, providing extremely fast query performance for low-cardinality columns.</p>"},{"location":"format/table/index/scalar/bitmap/#index-details","title":"Index Details","text":"<pre><code>message BitmapIndexDetails {\n\n}\n</code></pre>"},{"location":"format/table/index/scalar/bitmap/#storage-layout","title":"Storage Layout","text":"<p>The bitmap index consists of a single file <code>bitmap_page_lookup.lance</code> that stores the mapping from values to their bitmaps.</p>"},{"location":"format/table/index/scalar/bitmap/#file-schema","title":"File Schema","text":"Column Type Nullable Description <code>keys</code> {DataType} true The unique value from the indexed column <code>bitmaps</code> Binary true Serialized RowIdTreeMap containing row IDs where this value appears"},{"location":"format/table/index/scalar/bitmap/#accelerated-queries","title":"Accelerated Queries","text":"Query Type Description Operation Equals <code>column = value</code> Returns the bitmap for the specific value Range <code>column BETWEEN a AND b</code> Unions all bitmaps for values in the range IsIn <code>column IN (v1, v2, ...)</code> Unions bitmaps for all specified values IsNull <code>column IS NULL</code> Returns the pre-computed null bitmap"},{"location":"format/table/index/scalar/bloom_filter/","title":"Bloom Filter Index","text":"<p>Bloom filters are probabilistic data structures that allow for fast membership testing. They are space-efficient and can test whether an element is a member of a set. It's an inexact filter - they may include false positives but never false negatives.</p>"},{"location":"format/table/index/scalar/bloom_filter/#index-details","title":"Index Details","text":"<pre><code>message BloomFilterIndexDetails {\n\n}\n</code></pre>"},{"location":"format/table/index/scalar/bloom_filter/#storage-layout","title":"Storage Layout","text":"<p>The bloom filter index stores zone-based bloom filters in a single file:</p> <ol> <li><code>bloomfilter.lance</code> - Bloom filter statistics and data for each zone</li> </ol>"},{"location":"format/table/index/scalar/bloom_filter/#bloom-filter-file-schema","title":"Bloom Filter File Schema","text":"Column Type Nullable Description <code>fragment_id</code> UInt64 false Fragment containing this zone <code>zone_start</code> UInt64 false Starting row offset within the fragment <code>zone_length</code> UInt64 false Number of rows in this zone <code>has_null</code> Boolean false Whether this zone contains any null values <code>bloom_filter_data</code> Binary false Serialized SBBF (Split Block Bloom Filter) data"},{"location":"format/table/index/scalar/bloom_filter/#schema-metadata","title":"Schema Metadata","text":"Key Type Description <code>bloomfilter_item</code> String Expected number of items per zone (default: \"8192\") <code>bloomfilter_probability</code> String False positive probability (default: \"0.00057\", ~1 in 1754)"},{"location":"format/table/index/scalar/bloom_filter/#bloom-filter-spec","title":"Bloom Filter Spec","text":"<p>The bloom filter index uses a Split Block Bloom Filter (SBBF) implementation, which is optimized for SIMD operations.</p>"},{"location":"format/table/index/scalar/bloom_filter/#sbbf-structure","title":"SBBF Structure","text":"<p>The SBBF divides the bit array into blocks of 256 bits, where each block consists of 8 contiguous 32-bit words. This structure enables efficient SIMD operations and cache-friendly memory access patterns. The block layout is the following:</p> <ul> <li>Block size: 256 bits (32 bytes)</li> <li>Words per block: 8 \u00d7 32-bit integers</li> <li>Minimum filter size: 32 bytes (1 block)</li> <li>Maximum filter size: 128 MiB</li> </ul>"},{"location":"format/table/index/scalar/bloom_filter/#hashing-mechanism","title":"Hashing Mechanism","text":"<p>The SBBF uses xxHash64 with seed=0 for primary hashing, combined with a salt-based secondary hashing scheme:</p> <ol> <li>Primary hash: xxHash64(value) \u2192 64-bit hash</li> <li>Block selection: Upper 32 bits determine which block to use</li> <li>Bit selection: Lower 32 bits combined with 8 salt values set 8 bits in the block</li> </ol>"},{"location":"format/table/index/scalar/bloom_filter/#salt-values","title":"Salt Values","text":"<pre><code>0x47b6137b\n0x44974d91\n0x8824ad5b\n0xa2b7289d\n0x705495c7\n0x2df1424b\n0x9efc4947\n0x5c6bfb31\n</code></pre> <p>Each salt value generates one bit position within the block, ensuring uniform distribution.</p>"},{"location":"format/table/index/scalar/bloom_filter/#filter-sizing-algorithm","title":"Filter Sizing Algorithm","text":"<p>The SBBF automatically determines optimal filter size based on: - NDV (Number of Distinct Values): Expected unique items - FPP (False Positive Probability): Target error rate</p> <p>The implementation uses binary search to find the minimum log\u2082(bytes) that achieves the desired FPP, using Putze et al.'s cache-efficient bloom filter formula.</p>"},{"location":"format/table/index/scalar/bloom_filter/#fpp-convergence","title":"FPP Convergence","text":"<p>The implementation uses up to 750 iterations of Poisson distribution calculations to ensure accurate FPP estimation, particularly for dense filters where NDV approaches filter capacity.</p>"},{"location":"format/table/index/scalar/bloom_filter/#serialization","title":"Serialization","text":"<p>The SBBF is serialized as a contiguous byte array stored in the <code>bloom_filter_data</code> column:</p> <pre><code>[Block 0][Block 1]...[Block N-1]\n</code></pre> <p>Where each block is 32 bytes:</p> <pre><code>[Word 0][Word 1][Word 2][Word 3][Word 4][Word 5][Word 6][Word 7]\n</code></pre> <p>Each word is a 32-bit little-endian integer (4 bytes), with:</p> <ul> <li>Total size: Must be a multiple of 32 bytes</li> <li>Byte order: Little-endian for all 32-bit words</li> <li>Block alignment: Each block starts at offset <code>i * 32</code></li> <li>Word offset: Word <code>j</code> in block <code>i</code> is at byte offset <code>i * 32 + j * 4</code></li> </ul>"},{"location":"format/table/index/scalar/bloom_filter/#example","title":"Example","text":"<p>For a filter with 2 blocks (64 bytes total): <pre><code>Offset  0-3:   Block 0, Word 0 (32-bit LE)\nOffset  4-7:   Block 0, Word 1 (32-bit LE)\n...\nOffset 28-31:  Block 0, Word 7 (32-bit LE)\nOffset 32-35:  Block 1, Word 0 (32-bit LE)\n...\nOffset 60-63:  Block 1, Word 7 (32-bit LE)\n</code></pre></p>"},{"location":"format/table/index/scalar/bloom_filter/#accelerated-queries","title":"Accelerated Queries","text":"<p>The bloom filter index provides inexact results for the following query types:</p> Query Type Description Operation Result Type Equals <code>column = value</code> Tests if value exists in bloom filter AtMost IsIn <code>column IN (v1, v2, ...)</code> Tests if any value exists in bloom filter AtMost IsNull <code>column IS NULL</code> Returns zones where has_null is true AtMost"},{"location":"format/table/index/scalar/btree/","title":"BTree Index","text":"<p>The BTree index is a two-level structure that provides efficient range queries and sorted access.  It strikes a balance between an expensive memory structure containing all values  and an expensive disk structure that can't be efficiently searched.</p> <p>The upper layers of the BTree are designed to be cached in memory and stored in a  BTree structure (<code>page_lookup.lance</code>), while the leaves are searched using sub-indices  (<code>page_data.lance</code>, currently just a flat file).  This design enables efficient memory usage - for example, with 1 billion values,  the index can store 256K leaves of size 4K each, requiring only a few MiB of memory  (depending on data type) for the BTree metadata while narrowing any search to just 4K values.</p>"},{"location":"format/table/index/scalar/btree/#index-details","title":"Index Details","text":"<pre><code>message BTreeIndexDetails {\n\n}\n</code></pre>"},{"location":"format/table/index/scalar/btree/#storage-layout","title":"Storage Layout","text":"<p>The BTree index consists of two files:</p> <ol> <li><code>page_lookup.lance</code> - The BTree structure mapping value ranges to page numbers</li> <li><code>page_data.lance</code> - The actual sub-indices (flat file) containing sorted values and row IDs</li> </ol>"},{"location":"format/table/index/scalar/btree/#page-lookup-file-schema-btree-structure","title":"Page Lookup File Schema (BTree Structure)","text":"Column Type Nullable Description <code>min</code> {DataType} true Minimum value in the page (forms BTree keys) <code>max</code> {DataType} true Maximum value in the page (for range pruning) <code>null_count</code> UInt32 false Number of null values in the page <code>page_idx</code> UInt32 false Page number pointing to the sub-index in page_data.lance"},{"location":"format/table/index/scalar/btree/#schema-metadata","title":"Schema Metadata","text":"Key Type Description <code>batch_size</code> String Number of rows per page (default: \"4096\")"},{"location":"format/table/index/scalar/btree/#page-data-file-schema-sub-indices","title":"Page Data File Schema (Sub-indices)","text":"Column Type Nullable Description <code>values</code> {DataType} true Sorted values from the indexed column (flat file) <code>ids</code> UInt64 false Row IDs corresponding to each value"},{"location":"format/table/index/scalar/btree/#accelerated-queries","title":"Accelerated Queries","text":"<p>The BTree index provides exact results for the following query types:</p> Query Type Description Operation Equals <code>column = value</code> BTree lookup to find relevant pages, then search within sub-indices Range <code>column BETWEEN a AND b</code> BTree traversal for pages overlapping the range, then search each sub-index IsIn <code>column IN (v1, v2, ...)</code> Multiple BTree lookups, union results from all matching sub-indices IsNull <code>column IS NULL</code> Returns rows from all pages where null_count &gt; 0"},{"location":"format/table/index/scalar/fts/","title":"Full Text Search Index","text":"<p>The full text search (FTS) index (a.k.a. inverted index) provides efficient text search by mapping terms to the documents containing them. It's designed for high-performance text search with support for various scoring algorithms and phrase queries.</p>"},{"location":"format/table/index/scalar/fts/#index-details","title":"Index Details","text":"<pre><code>message InvertedIndexDetails {\n  // Marking this field as optional as old versions of the index store blank details and we\n  // need to make sure we have a proper optional field to detect this.\n  optional string base_tokenizer = 1;\n  string language = 2;\n  bool with_position = 3;\n  optional uint32 max_token_length = 4;\n  bool lower_case = 5;\n  bool stem = 6;\n  bool remove_stop_words = 7;\n  bool ascii_folding = 8;\n  uint32 min_ngram_length = 9;\n  uint32 max_ngram_length = 10;\n  bool prefix_only = 11;\n\n}\n</code></pre>"},{"location":"format/table/index/scalar/fts/#storage-layout","title":"Storage Layout","text":"<p>The FTS index consists of multiple files storing the token dictionary, document information, and posting lists:</p> <ol> <li><code>tokens.lance</code> - Token dictionary mapping tokens to token IDs</li> <li><code>docs.lance</code> - Document metadata including token counts</li> <li><code>invert.lance</code> - Compressed posting lists for each token</li> <li><code>metadata.lance</code> - Index metadata and configuration</li> </ol>"},{"location":"format/table/index/scalar/fts/#token-dictionary-file-schema","title":"Token Dictionary File Schema","text":"Column Type Nullable Description <code>_token</code> Utf8 false The token string <code>_token_id</code> UInt32 false Unique identifier for the token"},{"location":"format/table/index/scalar/fts/#document-file-schema","title":"Document File Schema","text":"Column Type Nullable Description <code>_rowid</code> UInt64 false Document row ID <code>_num_tokens</code> UInt32 false Number of tokens in the document"},{"location":"format/table/index/scalar/fts/#fts-list-file-schema","title":"FTS List File Schema","text":"Column Type Nullable Description <code>_posting</code> List false Compressed posting lists (delta-encoded row IDs and frequencies) <code>_max_score</code> Float32 false Maximum score for the token (for query optimization) <code>_length</code> UInt32 false Number of documents containing the token <code>_compressed_position</code> List&gt; true Optional compressed position lists for phrase queries"},{"location":"format/table/index/scalar/fts/#metadata-file-schema","title":"Metadata File Schema","text":"<p>The metadata file contains JSON-serialized configuration and partition information:</p> Key Type Description <code>partitions</code> Array List of partition IDs for distributed index organization <code>params</code> JSON Object Serialized InvertedIndexParams with tokenizer config"},{"location":"format/table/index/scalar/fts/#invertedindexparams-structure","title":"InvertedIndexParams Structure","text":"Field Type Default Description <code>base_tokenizer</code> String \"simple\" Base tokenizer type (see Tokenizers section) <code>language</code> String \"English\" Language for stemming and stop words <code>with_position</code> Boolean false Store term positions for phrase queries (increases index size) <code>max_token_length</code> UInt32? None Maximum token length (tokens longer than this are removed) <code>lower_case</code> Boolean true Convert tokens to lowercase <code>stem</code> Boolean false Apply language-specific stemming <code>remove_stop_words</code> Boolean false Remove common stop words for the specified language <code>ascii_folding</code> Boolean true Convert accented characters to ASCII equivalents <code>min_gram</code> UInt32 2 Minimum n-gram length (only for ngram tokenizer) <code>max_gram</code> UInt32 15 Maximum n-gram length (only for ngram tokenizer) <code>prefix_only</code> Boolean false Generate only prefix n-grams (only for ngram tokenizer)"},{"location":"format/table/index/scalar/fts/#tokenizers","title":"Tokenizers","text":"<p>The full text search index supports multiple tokenizer types for different text processing needs:</p>"},{"location":"format/table/index/scalar/fts/#base-tokenizers","title":"Base Tokenizers","text":"Tokenizer Description Use Case simple Splits on whitespace and punctuation, removes non-alphanumeric characters General text (default) whitespace Splits only on whitespace characters Preserve punctuation raw No tokenization, treats entire text as single token Exact matching ngram Breaks text into overlapping character sequences Substring/fuzzy search jieba/* Chinese text tokenizer with word segmentation Chinese text lindera/* Japanese text tokenizer with morphological analysis Japanese text"},{"location":"format/table/index/scalar/fts/#jieba-tokenizer-chinese","title":"Jieba Tokenizer (Chinese)","text":"<p>Jieba is a popular Chinese text segmentation library that uses a dictionary-based approach with statistical methods for word segmentation.</p> <ul> <li>Configuration: Uses a <code>config.json</code> file in the model directory</li> <li>Models: Must be downloaded and placed in the Lance home directory under <code>jieba/</code></li> <li>Usage: Specify as <code>jieba/&lt;model_name&gt;</code> or just <code>jieba</code> for the default model</li> <li>Config Structure:   <pre><code>{\n  \"main\": \"path/to/main/dictionary\",\n  \"users\": [\"path/to/user/dict1\", \"path/to/user/dict2\"]\n}\n</code></pre></li> <li>Features:</li> <li>Accurate word segmentation for Simplified and Traditional Chinese</li> <li>Support for custom user dictionaries</li> <li>Multiple segmentation modes (precise, full, search engine)</li> </ul>"},{"location":"format/table/index/scalar/fts/#lindera-tokenizer-japanese","title":"Lindera Tokenizer (Japanese)","text":"<p>Lindera is a morphological analysis tokenizer specifically designed for Japanese text. It provides proper word segmentation for Japanese, which doesn't use spaces between words.</p> <ul> <li>Configuration: Uses a <code>config.yml</code> file in the model directory</li> <li>Models: Must be downloaded and placed in the Lance home directory under <code>lindera/</code></li> <li>Usage: Specify as <code>lindera/&lt;model_name&gt;</code> where <code>&lt;model_name&gt;</code> is the subdirectory containing the model files</li> <li>Features:</li> <li>Morphological analysis with part-of-speech tagging</li> <li>Dictionary-based tokenization</li> <li>Support for custom user dictionaries</li> </ul>"},{"location":"format/table/index/scalar/fts/#token-filters","title":"Token Filters","text":"<p>Token filters are applied in sequence after the base tokenizer:</p> Filter Description Configuration RemoveLong Removes tokens exceeding max_token_length <code>max_token_length</code> LowerCase Converts tokens to lowercase <code>lower_case</code> (default: true) Stemmer Reduces words to their root form <code>stem</code>, <code>language</code> StopWords Removes common words like \"the\", \"is\", \"at\" <code>remove_stop_words</code>, <code>language</code> AsciiFolding Converts accented characters to ASCII <code>ascii_folding</code> (default: true)"},{"location":"format/table/index/scalar/fts/#supported-languages","title":"Supported Languages","text":"<p>For stemming and stop word removal, the following languages are supported: Arabic, Danish, Dutch, English, Finnish, French, German, Greek, Hungarian, Italian, Norwegian, Portuguese, Romanian, Russian, Spanish, Swedish, Tamil, Turkish</p>"},{"location":"format/table/index/scalar/fts/#document-type","title":"Document Type","text":"<p>Lance supports 2 kinds of documents: text and json. Different document types have different tokenization rules, and parse tokens in different format.</p>"},{"location":"format/table/index/scalar/fts/#text-type","title":"Text Type","text":"<p>Text type includes text and list of text. Tokens are generated by base_tokenizer.</p> <p>The example below shows how text document is parsed into tokens.  <pre><code>Tom lives in San Francisco.\n</code></pre></p> <p>The tokens are below. <pre><code>Tom\nlives\nin\nSan\nFrancisco\n</code></pre></p>"},{"location":"format/table/index/scalar/fts/#json-type","title":"Json Type","text":"<p>Json is a nested structure, lance breaks down json document into tokens in triplet format <code>path,type,value</code>. The valid types are: str, number, bool, null.</p> <p>In scenarios where the triplet value is a str, the text value will be further tokenized using the base_tokenizer, resulting in multiple triplet tokens.</p> <p>During querying, the Json Tokenizer uses the triplet format instead of the json format, which simplifies the query syntax.</p> <p>The example below shows how the json document is tokenized. Assume we have the following json document: <pre><code>{\n  \"name\": \"Lance\",\n  \"legal.age\": 30,\n  \"address\": {\n    \"city\": \"San Francisco\",\n    \"zip:us\": 94102\n  }\n}\n</code></pre></p> <p>After parsing, the document will be tokenized into the following tokens: <pre><code>name,str,Lance\nlegal.age,number,30\naddress.city,str,San\naddress.city,str,Francisco\naddress.zip:us,number,94102\n</code></pre></p> <p>Then we do full text search in triplet format. To search for \"San Francisco,\" we can search with one of the triplets below: <pre><code>address.city:San Francisco\naddress.city:San\naddress.city:Francisco\n</code></pre></p>"},{"location":"format/table/index/scalar/fts/#accelerated-queries","title":"Accelerated Queries","text":"<p>Lance SDKs provide dedicated full text search APIs to leverage the FTS index capabilities.  These APIs support complex query types beyond simple token matching,  enabling sophisticated text search operations. Here are the query types enabled by the FTS index:</p> Query Type Description Example Usage Result Type contains_tokens Basic token-based search (UDF) with BM25 scoring and automatic result ranking SQL: <code>contains_tokens(column, 'search terms')</code> AtMost match Match query with configurable AND/OR operators and relevance scoring <code>{\"match\": {\"query\": \"text\", \"operator\": \"and/or\"}}</code> AtMost phrase Exact phrase matching with position information (requires <code>with_position: true</code>) <code>{\"phrase\": {\"query\": \"exact phrase\"}}</code> AtMost boolean Complex boolean queries with must/should/must_not clauses for sophisticated search logic <code>{\"boolean\": {\"must\": [...], \"should\": [...]}}</code> AtMost multi_match Search across multiple fields simultaneously with unified scoring <code>{\"multi_match\": [{\"field1\": \"query\"}, ...]}</code> AtMost boost Boost relevance scores for specific terms or queries by a configurable factor <code>{\"boost\": {\"query\": {...}, \"factor\": 2.0}}</code> AtMost"},{"location":"format/table/index/scalar/label_list/","title":"Label List Index","text":"<p>Label list indices are optimized for columns containing multiple labels or tags per row. They provide efficient set-based queries on multi-value columns using an underlying bitmap index.</p>"},{"location":"format/table/index/scalar/label_list/#index-details","title":"Index Details","text":"<pre><code>message LabelListIndexDetails {\n\n}\n</code></pre>"},{"location":"format/table/index/scalar/label_list/#storage-layout","title":"Storage Layout","text":"<p>The label list index uses a bitmap index internally and stores its data in:</p> <ol> <li><code>bitmap_page_lookup.lance</code> - Bitmap index mapping unique labels to row IDs</li> </ol>"},{"location":"format/table/index/scalar/label_list/#file-schema","title":"File Schema","text":"Column Type Nullable Description <code>keys</code> {DataType} true The unique label value from the indexed column <code>bitmaps</code> Binary true Serialized RowIdTreeMap containing row IDs where this label appears"},{"location":"format/table/index/scalar/label_list/#accelerated-queries","title":"Accelerated Queries","text":"<p>The label list index provides exact results for the following query types:</p> Query Type Description Operation Result Type array_has_all Array contains all specified values Intersects bitmaps for all specified labels Exact array_has_any Array contains any of specified values Unions bitmaps for all specified labels Exact"},{"location":"format/table/index/scalar/ngram/","title":"N-gram Index","text":"<p>N-gram indices break text into overlapping sequences (trigrams) for efficient substring matching. They provide fast text search by indexing all 3-character sequences in the text after applying ASCII folding and lowercasing.</p>"},{"location":"format/table/index/scalar/ngram/#index-details","title":"Index Details","text":"<pre><code>message NGramIndexDetails {\n\n}\n</code></pre>"},{"location":"format/table/index/scalar/ngram/#storage-layout","title":"Storage Layout","text":"<p>The N-gram index stores tokenized text as trigrams with their posting lists:</p> <ol> <li><code>ngram_postings.lance</code> - Trigram tokens and their posting lists</li> </ol>"},{"location":"format/table/index/scalar/ngram/#file-schema","title":"File Schema","text":"Column Type Nullable Description <code>tokens</code> UInt32 true Hashed trigram token <code>posting_list</code> Binary false Compressed bitmap of row IDs containing the token"},{"location":"format/table/index/scalar/ngram/#accelerated-queries","title":"Accelerated Queries","text":"<p>The N-gram index provides inexact results for the following query types:</p> Query Type Description Operation Result Type contains Substring search in text Finds all trigrams in query, intersects posting lists AtMost"},{"location":"format/table/index/scalar/zonemap/","title":"Zone Map Index","text":"<p>Zone maps are a columnar database technique for predicate pushdown and scan pruning. They break data into fixed-size chunks called \"zones\" and maintain summary statistics (min, max, null count) for each zone, enabling efficient filtering by eliminating zones that cannot contain matching values.</p> <p>Zone maps are \"inexact\" filters - they can definitively exclude zones but may include false positives that require rechecking.</p>"},{"location":"format/table/index/scalar/zonemap/#index-details","title":"Index Details","text":"<pre><code>message ZoneMapIndexDetails {\n\n}\n</code></pre>"},{"location":"format/table/index/scalar/zonemap/#storage-layout","title":"Storage Layout","text":"<p>The zone map index stores zone statistics in a single file:</p> <ol> <li><code>zonemap.lance</code> - Zone statistics for query pruning</li> </ol>"},{"location":"format/table/index/scalar/zonemap/#zone-statistics-file-schema","title":"Zone Statistics File Schema","text":"Column Type Nullable Description <code>min</code> {DataType} true Minimum value in the zone <code>max</code> {DataType} true Maximum value in the zone <code>null_count</code> UInt32 false Number of null values in the zone <code>nan_count</code> UInt32 false Number of NaN values (for float types) <code>fragment_id</code> UInt64 false Fragment containing this zone <code>zone_start</code> UInt64 false Starting row offset within the fragment <code>zone_length</code> UInt32 false Number of rows in this zone"},{"location":"format/table/index/scalar/zonemap/#schema-metadata","title":"Schema Metadata","text":"Key Type Description <code>rows_per_zone</code> String Number of rows per zone (default: \"8192\")"},{"location":"format/table/index/scalar/zonemap/#accelerated-queries","title":"Accelerated Queries","text":"<p>The zone map index provides inexact results for the following query types:</p> Query Type Description Operation Result Type Equals <code>column = value</code> Includes zones where min \u2264 value \u2264 max AtMost Range <code>column BETWEEN a AND b</code> Includes zones where ranges overlap AtMost IsIn <code>column IN (v1, v2, ...)</code> Includes zones that could contain any value AtMost IsNull <code>column IS NULL</code> Includes zones where null_count &gt; 0 AtMost"},{"location":"format/table/index/system/frag_reuse/","title":"Fragment Reuse Index","text":"<p>The Fragment Reuse Index is an internal index used to optimize fragment operations  during compaction and dataset updates.</p> <p>When data modifications happen against a Lance table, it could trigger compaction and index optimization at the same time to improve data layout and index coverage. By default, compaction will remap all indices at the same time to prevent read regression. This means both compaction and index optimization could modify the same index and cause one process to fail. Typically, the compaction would fail because it has to modify all indices and takes longer, resulting in table layout degrading over time.</p> <p>Fragment Reuse Index allows a compaction to defer the index remap process. Suppose a compaction removes fragments A and B and produces C. At query runtime, it reuses the old fragments A and B by  updating the row addresses related to A and B in the index to the latest ones in C. Because indices are typically cached in memory after initial load, the in-memory index is up to date after the fragment reuse application process.</p>"},{"location":"format/table/index/system/frag_reuse/#index-details","title":"Index Details","text":"<pre><code>message FragmentReuseIndexDetails {\n\n  oneof content {\n    // if &lt; 200KB, store the content inline, otherwise store the InlineContent bytes in external file\n    InlineContent inline = 1;\n    ExternalFile external = 2;\n  }\n\n  message InlineContent {\n    repeated Version versions = 1;\n  }\n\n  message FragmentDigest {\n    uint64 id = 1;\n\n    uint64 physical_rows = 2;\n\n    uint64 num_deleted_rows = 3;\n  }\n\n  // A summarized version of the RewriteGroup information in a Rewrite transaction\n  message Group {\n    // A roaring treemap of the changed row addresses.\n    // When combined with the old fragment IDs and new fragment IDs,\n    // it can recover the full mapping of old row addresses to either new row addresses or deleted.\n    // this mapping can then be used to remap indexes or satisfy index queries for the new unindexed fragments.\n    bytes changed_row_addrs = 1;\n\n    repeated FragmentDigest old_fragments = 2;\n\n    repeated FragmentDigest new_fragments = 3;\n  }\n\n  message Version {\n    // The dataset_version at the time the index adds this version entry\n    uint64 dataset_version = 1;\n\n    repeated Group groups = 3;\n  }\n\n}\n</code></pre>"},{"location":"format/table/index/system/frag_reuse/#expected-use-pattern","title":"Expected Use Pattern","text":"<p>Fragment Reuse Index should be created if the user defers index remap in compaction. The index accumulates a new reuse version every time a compaction is executed.</p> <p>As long as all the scalar and vector indices are created after the specific reuse version, the indices are all caught up and the specific reuse version can be trimmed.</p> <p>It is expected that the user schedules an additional process to trim the index periodically to keep the list of reuse versions in control.</p>"},{"location":"format/table/index/system/memwal/","title":"MemWAL Index","text":"<p>The MemTable and Write-Ahead Log (MemWAL) Index is used for fast upserts into the Lance table.</p> <p>The index is used as the centralized synchronization system for a log-structured merge tree (LSM-tree), leaving the actual implementation of the MemTable and WAL up to the specific implementer of the spec.</p> <p>Each region represents a single writer that writes to both a MemTable and a WAL, and a region can have increasing generations of MemWALs. Every time data is written into a WAL, the index is updated with the latest watermark. If a specific writer of a region dies, a new writer is able to read the information in the specific region and replay the WAL.</p>"},{"location":"format/table/index/system/memwal/#index-details","title":"Index Details","text":"<pre><code>message MemWalIndexDetails {\n\n  repeated MemWal mem_wal_list = 1;\n\n  message MemWalId {\n    // The name of the region that this specific MemWAL is responsible for.\n    string region = 1;\n\n    // The generation of the MemWAL.\n    // Every time a new MemWAL is created and an old one is sealed,\n    // the generation number of the next MemWAL is incremented.\n    // At any given point of time for all MemWALs of the same name,\n    // there must be only 1 generation that is not sealed.\n    uint64 generation = 2;\n  }\n\n  // A combination of MemTable and WAL for fast upsert.\n  message MemWal {\n\n    enum State {\n      // MemWAL is open and accepting new entries\n      OPEN = 0;\n      // When a MemTable is considered full, the writer should update this MemWAL as sealed\n      // and create a new MemWAL to write to atomically.\n      SEALED = 1;\n      // When a MemTable is sealed, it can be flushed asynchronously to disk.\n      // This state indicates the data has been persisted to disk but not yet merged\n      // into the source table.\n      FLUSHED = 2;\n      // When the flushed data has been merged into the source table.\n      // After a MemWAL is merged, the cleanup process can delete the WAL.\n      MERGED = 3;\n    }\n\n    MemWalId id = 1;\n\n    // The MemTable location, which is likely an in-memory address starting with memory://.\n    // The actual details of how the MemTable is stored is outside the concern of Lance.\n    string mem_table_location = 2;\n\n    // the root location of the WAL.\n    // THe WAL storage durability determines the data durability.\n    // This location is immutable once set at MemWAL creation time.\n    string wal_location = 3;\n\n    // All entries in the WAL, serialized as U64Segment.\n    // Each entry in the WAL has a uint64 sequence ID starting from 0.\n    // The actual details of how the WAL entry is stored is outside the concern of Lance.\n    // In most cases this U64Segment should be a simple range.\n    // Every time the writer starts writing, it must always try to atomically write to the last entry ID + 1.\n    // If fails due to concurrent writer, it then tries to write to the +2, +3, +4, etc. entry ID until succeed.\n    // but if there are 2 writers accidentally writing to the same WAL concurrently,\n    // although one writer will fail to update this index at commit time,\n    // the WAL entry is already written,\n    // causing some holes within the U64Segment range.\n    bytes wal_entries = 4;\n\n    // The current state of the MemWAL, indicating its lifecycle phase.\n    // States progress: OPEN -&gt; SEALED -&gt; FLUSHED\n    // OPEN: MemWAL is accepting new WAL entries\n    // SEALED: MemWAL has been sealed and no longer accepts new WAL entries\n    // FLUSHED: MemWAL has been flushed to the source Lance table and can be cleaned up\n    State state = 5;\n\n    // The owner identifier for this MemWAL, used for compare-and-swap operations.\n    // When a writer wants to perform any operation on this MemWAL, it must provide\n    // the expected owner_id. This serves as an optimistic lock to prevent concurrent\n    // writers from interfering with each other. When a new writer starts replay,\n    // it must first atomically update this owner_id to claim ownership.\n    // All subsequent operations will fail if the owner_id has changed.\n    string owner_id = 6;\n\n    // The dataset version that last updated this MemWAL.\n    // This is set to the new dataset version whenever the MemWAL is created or modified.\n    uint64 last_updated_dataset_version = 7;\n  }\n\n}\n</code></pre>"},{"location":"format/table/index/system/memwal/#expected-use-pattern","title":"Expected Use Pattern","text":"<p>It is expected that:</p> <ol> <li>there is exactly one writer for each region, guaranteed by optimistic update of the owner_id</li> <li>each writer updates the MemWAL index after a successful write to WAL and MemTable</li> <li>a new writer always finds unsealed MemWALs and performs replay before accepting new writes</li> <li>background processes are responsible for merging flushed MemWALs to the main Lance table, and making index up to date.</li> <li>a MemWAL-aware reader is able to merge results of MemTables in the MemWALs with results in the base Lance table. </li> </ol>"},{"location":"format/table/index/vector/","title":"Vector Indices","text":"<p>Lance provides a powerful and extensible secondary index system for efficient vector similarity search.  All vector indices are stored as regular Lance files, making them portable and easy to manage. It is designed for efficient similarity search across large-scale vector datasets.</p>"},{"location":"format/table/index/vector/#concepts","title":"Concepts","text":"<p>Lance splits each vector index into 3 parts - clustering, sub-index and quantization.</p>"},{"location":"format/table/index/vector/#clustering","title":"Clustering","text":"<p>Clustering divides all the vectors into different disjoint clusters (a.k.a. partitions). Lance currently supports using Inverted File (IVF) as the primary clustering mechanism. IVF partitions the vectors into clusters using the k-means clustering algorithm.  Each cluster contains vectors that are similar to the cluster centroid. During search, only the most relevant clusters are examined, dramatically reducing search time. IVF can be combined with any sub-index type and quantization method.</p>"},{"location":"format/table/index/vector/#sub-index","title":"Sub-Index","text":"<p>The sub-index determines how vectors are organized for search. Lance currently supports:</p> <ul> <li>FLAT: Exact search with no approximation - scans all vectors</li> <li>HNSW: Hierarchical Navigable Small World graphs for fast approximate search</li> </ul>"},{"location":"format/table/index/vector/#quantization","title":"Quantization","text":"<p>The quantization method determines how vectors are stored and compressed. Lance currently supports:</p> <ul> <li>Product Quantization (PQ): Compresses vectors by splitting them into smaller sub-vectors and quantizing each independently</li> <li>Scalar Quantization (SQ): Applies scalar quantization to each dimension of the vector independently</li> <li>RabitQ (RQ): Uses random rotation and binary quantization for extreme compression</li> <li>FLAT: No quantization, keeps original vectors for exact search</li> </ul>"},{"location":"format/table/index/vector/#common-combinations","title":"Common Combinations","text":"<p>When we refer to an index type, it is typically <code>{clustering}_{sub_index}_{quantization}</code>. If sub-index is just <code>FLAT</code>, we usually omit it and just refer to it by <code>{clustering}_{quantization}</code>. Here are the commonly used combinations:</p> Index Type Name Description IVF_PQ Inverted File with Product Quantization Combines IVF clustering with PQ compression for efficient storage and search IVF_HNSW_SQ Inverted File with HNSW and Scalar Quantization Uses IVF for coarse clustering and HNSW for fine-grained search with scalar quantization IVF_SQ Inverted File with Scalar Quantization Combines IVF clustering with scalar quantization for balanced compression IVF_RQ Inverted File with RabitQ Combines IVF clustering with RabitQ for extreme compression using binary quantization IVF_FLAT Inverted File without quantization Uses IVF clustering with exact vector storage for precise search within clusters"},{"location":"format/table/index/vector/#versioning","title":"Versioning","text":"<p>The Lance vector index format has gone through 3 versions so far. This document currently only records version 3 which is the latest version. The specific version of the vector index is recorded in the <code>index_version</code> field of the generic index metadata.</p>"},{"location":"format/table/index/vector/#storage-layout-v3","title":"Storage Layout (V3)","text":"<p>Each vector index is stored as 2 regular Lance files - index file and auxiliary file.</p>"},{"location":"format/table/index/vector/#index-file","title":"Index File","text":"<p>The index structure file containing the search graph/structure with index-specific schema. It is stored as a Lance file with name <code>index.idx</code> within the index directory.</p>"},{"location":"format/table/index/vector/#arrow-schema","title":"Arrow Schema","text":"<p>The index file stores the search structure with graph or flat organization. The Arrow schema of the Lance file varies depending on the sub-index type used.</p> <p>Note</p> <p>All partitions are stored in the same file, and partitions must be written in order.</p>"},{"location":"format/table/index/vector/#flat","title":"FLAT","text":"<p>FLAT indices perform exact search with no approximation. This is essentially an empty file with a minimal schema:</p> Column Type Nullable Description <code>__flat_marker</code> uint64 false Marker field for FLAT index (no actual data)"},{"location":"format/table/index/vector/#hnsw","title":"HNSW","text":"<p>HNSW (Hierarchical Navigable Small World) indices provide fast approximate search through a multi-level graph structure. This stores the HNSW graph with the following schema:</p> Column Type Nullable Description <code>__vector_id</code> uint64 false Vector identifier <code>__neighbors</code> list false Neighbor node IDs <code>_distance</code> list false Distances to neighbors <p>Note</p> <p>HNSW consists of multiple levels, and all levels must be written in order starting from level 0.</p>"},{"location":"format/table/index/vector/#arrow-schema-metadata","title":"Arrow Schema Metadata","text":"<p>The index file contains metadata in its Arrow schema metadata to describe the index configuration and structure. Here are the metadata keys and their corresponding values:</p>"},{"location":"format/table/index/vector/#lanceindex","title":"\"lance:index\"","text":"<p>Contains basic index configuration information in JSON:</p> JSON Key Type Expected Values <code>type</code> String Index type (e.g., \"IVF_PQ\", \"IVF_RQ\", \"IVF_HNSW\", \"FLAT\") <code>distance_type</code> String Distance metric (e.g., \"l2\", \"cosine\", \"dot\")"},{"location":"format/table/index/vector/#lanceivf","title":"\"lance:ivf\"","text":"<p>References the IVF metadata stored in the Lance file global buffer. This value records the global buffer index, currently this is always \"1\".</p> <p>Note</p> <p>Global buffer indices in Lance files are 1-based,  so you need to subtract 1 when accessing them through code.</p>"},{"location":"format/table/index/vector/#lanceflat","title":"\"lance:flat\"","text":"<p>Contains partition-specific metadata for the <code>FLAT</code> sub-index structure. This is an empty string since FLAT indices don't require additional metadata at this moment.</p>"},{"location":"format/table/index/vector/#lancehnsw","title":"\"lance:hnsw\"","text":"<p>Contains the HNSW-specific JSON metadata for each partition, including graph structure information:</p> JSON Key Type Expected Values <code>entry_point</code> u32 Starting node for graph traversal <code>params</code> Object HNSW construction parameters (see below) <code>level_offsets</code> Array Offset for each level in the graph <p>The <code>params</code> object contains the following HNSW construction parameters:</p> JSON Key Type Description Default <code>max_level</code> u16 Maximum level of the HNSW graph 7 <code>m</code> usize Number of connections to establish while inserting new element 20 <code>ef_construction</code> usize Size of the dynamic list for candidates 150 <code>prefetch_distance</code> Option Number of vectors ahead to prefetch while building Some(2)"},{"location":"format/table/index/vector/#lance-file-global-buffer","title":"Lance File Global Buffer","text":""},{"location":"format/table/index/vector/#ivf-metadata","title":"IVF Metadata","text":"<p>For efficiency, Lance serializes IVF metadata to protobuf format and stores it in the Lance file global buffer:</p> <pre><code>message IVF {\n  // Centroids of partitions. `dimension * num_partitions` of float32s.\n  //\n  // Deprecated, use centroids_tensor instead.\n  repeated float centroids = 1;  // [deprecated = true];\n\n  // File offset of each partition.\n  repeated uint64 offsets = 2;\n\n  // Number of records in the partition.\n  repeated uint32 lengths = 3;\n\n  // Tensor of centroids. `num_partitions * dimension` of float32s.\n  Tensor centroids_tensor = 4;\n\n  // KMeans loss.\n  optional double loss = 5;\n\n}\n</code></pre>"},{"location":"format/table/index/vector/#auxiliary-file","title":"Auxiliary File","text":"<p>The auxiliary file is a vector storage for quantized vectors. It is stored as a Lance file named <code>auxiliary.idx</code> within the index directory.</p>"},{"location":"format/table/index/vector/#arrow-schema_1","title":"Arrow Schema","text":"<p>Since the auxiliary file stores the actual (quantized) vectors, the Arrow schema of the Lance file varies depending on the quantization method used.</p> <p>Note</p> <p>All partitions are stored in the same file, and partitions must be written in order.</p>"},{"location":"format/table/index/vector/#flat_1","title":"FLAT","text":"<p>No quantization applied - stores original vectors in their full precision:</p> Column Type Nullable Description <code>_rowid</code> uint64 false Row identifier <code>flat</code> list[dimension] false Original vector values (list_size = vector dimension)"},{"location":"format/table/index/vector/#pq","title":"PQ","text":"<p>Compresses vectors using product quantization for significant memory savings:</p> Column Type Nullable Description <code>_rowid</code> uint64 false Row identifier <code>__pq_code</code> list[m] false PQ codes (list_size = number of subvectors)"},{"location":"format/table/index/vector/#sq","title":"SQ","text":"<p>Compresses vectors using scalar quantization for moderate memory savings:</p> Column Type Nullable Description <code>_rowid</code> uint64 false Row identifier <code>__sq_code</code> list[dimension] false SQ codes (list_size = vector dimension)"},{"location":"format/table/index/vector/#rq","title":"RQ","text":"<p>Compresses vectors using RabitQ with random rotation and binary quantization for extreme compression:</p> Column Type Nullable Description <code>_rowid</code> uint64 false Row identifier <code>_rabit_codes</code> list[dimension / 8] false Binary quantized codes (1 bit per dimension, packed into bytes) <code>__add_factors</code> float32 false Additive correction factors for distance computation <code>__scale_factors</code> float32 false Scale correction factors for distance computation"},{"location":"format/table/index/vector/#arrow-schema-metadata_1","title":"Arrow Schema Metadata","text":"<p>The auxiliary file also contains metadata in its Arrow schema metadata for vector storage configuration. Here are the metadata keys and their corresponding values:</p>"},{"location":"format/table/index/vector/#distance_type","title":"\"distance_type\"","text":"<p>The distance metric used to compute similarity between vectors (e.g., \"l2\", \"cosine\", \"dot\").</p>"},{"location":"format/table/index/vector/#lanceivf_1","title":"\"lance:ivf\"","text":"<p>Similar to the index file's \"lance:ivf\" but focused on vector storage layout.  This doesn't contain the partitions' centroids. It's only used for tracking each partition's offset and length in the auxiliary file.</p>"},{"location":"format/table/index/vector/#lancerabit","title":"\"lance:rabit\"","text":"<p>Contains RabitQ-specific metadata in JSON format (only present for RQ quantization). This includes the rotation matrix position, number of bits, and packing information. See the RQ metadata specification in the \"storage_metadata\" section below.</p>"},{"location":"format/table/index/vector/#storage_metadata","title":"\"storage_metadata\"","text":"<p>Contains quantizer-specific metadata as a list of JSON strings. Currently, the list always contains exactly 1 element with the quantizer metadata.</p> <p>For Product Quantization (PQ):</p> JSON Key Type Description <code>codebook_position</code> usize Position of the codebook in the global buffer <code>nbits</code> u32 Number of bits per subvector code (e.g., 8 bits = 256 codewords) <code>num_sub_vectors</code> usize Number of subvectors (m) <code>dimension</code> usize Original vector dimension <code>transposed</code> bool Whether the codebook is stored in transposed layout <p>For Scalar Quantization (SQ):</p> JSON Key Type Description <code>dim</code> usize Vector dimension <code>num_bits</code> u16 Number of bits for quantization <code>bounds</code> Range Min/max bounds for scalar quantization <p>For RabitQ (RQ):</p> JSON Key Type Description <code>rotate_mat_position</code> u32 Position of the rotation matrix in the global buffer <code>num_bits</code> u8 Number of bits per dimension (currently always 1) <code>packed</code> bool Whether codes are packed for optimized computation"},{"location":"format/table/index/vector/#lance-file-global-buffer_1","title":"Lance File Global Buffer","text":""},{"location":"format/table/index/vector/#quantization-codebook","title":"Quantization Codebook","text":"<p>For product quantization, the codebook is stored in <code>Tensor</code> format  in the auxiliary file's global buffer for efficient access:</p> <pre><code>message Tensor {\n  enum DataType {\n    BFLOAT16 = 0;\n    FLOAT16 = 1;\n    FLOAT32 = 2;\n    FLOAT64 = 3;\n    UINT8 = 4;\n    UINT16 = 5;\n    UINT32 = 6;\n    UINT64 = 7;\n  }\n\n  DataType data_type = 1;\n\n  // Data shape, [dim1, dim2, ...]\n  repeated uint32 shape = 2;\n\n  // Data buffer\n  bytes data = 3;\n\n}\n</code></pre>"},{"location":"format/table/index/vector/#rotation-matrix","title":"Rotation Matrix","text":"<p>For RabitQ, the rotation matrix is stored in <code>Tensor</code> format in the auxiliary file's global buffer. The rotation matrix is an orthogonal matrix used  to rotate vectors before binary quantization:</p> <pre><code>message Tensor {\n  enum DataType {\n    BFLOAT16 = 0;\n    FLOAT16 = 1;\n    FLOAT32 = 2;\n    FLOAT64 = 3;\n    UINT8 = 4;\n    UINT16 = 5;\n    UINT32 = 6;\n    UINT64 = 7;\n  }\n\n  DataType data_type = 1;\n\n  // Data shape, [dim1, dim2, ...]\n  repeated uint32 shape = 2;\n\n  // Data buffer\n  bytes data = 3;\n\n}\n</code></pre> <p>The rotation matrix has shape <code>[code_dim, code_dim]</code> where <code>code_dim = dimension * num_bits</code>.</p>"},{"location":"format/table/index/vector/#appendices","title":"Appendices","text":""},{"location":"format/table/index/vector/#appendix-1-example-ivf_pq-format","title":"Appendix 1: Example IVF_PQ Format","text":"<p>This example shows how an <code>IVF_PQ</code> index is physically laid out. Assume vectors have dimension 128, PQ uses 16 num_sub_vectors (m=16) with 8 num_bits per subvector, and distance type is \"l2\".</p>"},{"location":"format/table/index/vector/#index-file_1","title":"Index File","text":"<ul> <li> <p>Arrow Schema Metadata:</p> <ul> <li><code>\"lance:index\"</code> \u2192 <code>{ \"type\": \"IVF_PQ\", \"distance_type\": \"l2\" }</code></li> <li><code>\"lance:ivf\"</code> \u2192 \"1\" (references IVF metadata in the global buffer)</li> <li><code>\"lance:flat\"</code> \u2192 <code>[\"\", \"\", ...]</code> (one empty string per partition; IVF_PQ uses a FLAT sub-index inside each partition)</li> </ul> </li> <li> <p>Lance File Global buffer (Protobuf):</p> <ul> <li><code>Ivf</code> message containing:<ul> <li><code>centroids_tensor</code>: shape <code>[num_partitions, 128]</code> (float32)</li> <li><code>offsets</code>: start offset (row) of each partition in <code>auxiliary.idx</code></li> <li><code>lengths</code>: number of vectors in each partition</li> <li><code>loss</code>: k-means loss (optional)</li> </ul> </li> </ul> </li> </ul>"},{"location":"format/table/index/vector/#auxiliary-file_1","title":"Auxiliary File","text":"<ul> <li>Arrow Schema Metadata:<ul> <li><code>\"distance_type\"</code> \u2192 <code>\"l2\"</code></li> <li><code>\"lance:ivf\"</code> \u2192 tracks per-partition <code>offsets</code> and <code>lengths</code> (no centroids here)</li> <li><code>\"storage_metadata\"</code> \u2192 <code>[ \"{\"pq\":{\"num_sub_vectors\":16,\"nbits\":8,\"dimension\":128,\"transposed\":true}}\" ]</code></li> </ul> </li> <li>Lance File Global buffer:<ul> <li><code>Tensor</code> codebook with shape <code>[256, num_sub_vectors, dim/num_sub_vectors]</code> = <code>[256, 16, 8]</code> (float32)</li> </ul> </li> <li>Rows with Arrow schema: </li> </ul> <pre><code>pa.schema([\n    pa.field(\"_rowid\", pa.uint64()),\n    pa.field(\"__pq_code\", pa.list(pa.uint8(), list_size=16)), # m subvector codes\n])\n</code></pre>"},{"location":"format/table/index/vector/#appendix-2-example-ivf_rq-format","title":"Appendix 2: Example IVF_RQ Format","text":"<p>This example shows how an <code>IVF_RQ</code> index is physically laid out. Assume vectors have dimension 128, RQ uses 1 bit per dimension (num_bits=1), and distance type is \"l2\".</p>"},{"location":"format/table/index/vector/#index-file_2","title":"Index File","text":"<ul> <li> <p>Arrow Schema Metadata:</p> <ul> <li><code>\"lance:index\"</code> \u2192 <code>{ \"type\": \"IVF_RQ\", \"distance_type\": \"l2\" }</code></li> <li><code>\"lance:ivf\"</code> \u2192 \"1\" (references IVF metadata in the global buffer)</li> <li><code>\"lance:flat\"</code> \u2192 <code>[\"\", \"\", ...]</code> (one empty string per partition; IVF_RQ uses a FLAT sub-index inside each partition)</li> </ul> </li> <li> <p>Lance File Global buffer (Protobuf):</p> <ul> <li><code>Ivf</code> message containing:<ul> <li><code>centroids_tensor</code>: shape <code>[num_partitions, 128]</code> (float32)</li> <li><code>offsets</code>: start offset (row) of each partition in <code>auxiliary.idx</code></li> <li><code>lengths</code>: number of vectors in each partition</li> <li><code>loss</code>: k-means loss (optional)</li> </ul> </li> </ul> </li> </ul>"},{"location":"format/table/index/vector/#auxiliary-file_2","title":"Auxiliary File","text":"<ul> <li>Arrow Schema Metadata:<ul> <li><code>\"distance_type\"</code> \u2192 <code>\"l2\"</code></li> <li><code>\"lance:ivf\"</code> \u2192 tracks per-partition <code>offsets</code> and <code>lengths</code> (no centroids here)</li> <li><code>\"lance:rabit\"</code> \u2192 <code>\"{\"rotate_mat_position\":1,\"num_bits\":1,\"packed\":true}\"</code></li> </ul> </li> <li>Lance File Global buffer:<ul> <li><code>Tensor</code> rotation matrix with shape <code>[code_dim, code_dim]</code> = <code>[128, 128]</code> (float32)</li> </ul> </li> <li>Rows with Arrow schema: </li> </ul> <pre><code>pa.schema([\n    pa.field(\"_rowid\", pa.uint64()),\n    pa.field(\"_rabit_codes\", pa.list(pa.uint8(), list_size=16)), # dimension/8 = 128/8 = 16 bytes\n    pa.field(\"__add_factors\", pa.float32()),\n    pa.field(\"__scale_factors\", pa.float32()),\n])\n</code></pre>"},{"location":"format/table/index/vector/#appendix-3-accessing-index-file-with-python","title":"Appendix 3: Accessing Index File with Python","text":"<p>The following example demonstrates how to read and parse different components in the Lance index files using Python:</p> <pre><code>import pyarrow as pa\nimport lance\n\n# Open the index file\nindex_reader = lance.LanceFileReader.read_file(\"path/to/index.idx\")\n\n# Access schema metadata\nschema_metadata = index_reader.metadata().schema.metadata\n\n# Get the IVF metadata reference from schema\nivf_ref = schema_metadata.get(b\"lance:ivf\")  # Returns b\"1\" for global buffer index\n\n# Read the global buffer containing IVF metadata\nif ivf_ref:\n    buffer_index = int(ivf_ref) - 1  # Global buffer indices are 1-based\n    ivf_buffer = index_reader.global_buffer(buffer_index)\n\n    # Parse the protobuf message (requires lance protobuf definitions)\n    # ivf_metadata = parse_ivf_protobuf(ivf_buffer)\n\n# For auxiliary file with PQ codebook\naux_reader = lance.LanceFileReader.read_file(\"path/to/auxiliary.idx\")\n\n# Get storage metadata\nstorage_metadata = aux_reader.metadata().schema.metadata.get(b\"storage_metadata\")\nif storage_metadata:\n    import json\n    pq_metadata = json.loads(storage_metadata.decode())[0]  # First element of the list\n    pq_params = json.loads(pq_metadata)\n\n    # Access the codebook from global buffer\n    codebook_position = pq_params.get(\"codebook_position\", 1)\n    if codebook_position &gt; 0:\n        codebook_buffer = aux_reader.global_buffer(codebook_position - 1)\n        # Parse the tensor protobuf\n        # codebook_tensor = parse_tensor_protobuf(codebook_buffer)\n</code></pre>"},{"location":"guide/arrays/","title":"Extension Arrays","text":"<p>Lance provides extensions for Arrow arrays and Pandas Series to represent data types for machine learning applications.</p>"},{"location":"guide/arrays/#bfloat16","title":"BFloat16","text":"<p>BFloat16 is a 16-bit floating point number that is designed for machine learning use cases. Intuitively, it only has 2-3 digits of precision, but it has the same range as a 32-bit float: ~1e-38 to ~1e38. By comparison, a 16-bit float has a range of ~5.96e-8 to 65504.</p> <p>Lance provides an Arrow extension array (<code>lance.arrow.BFloat16Array</code>) and a Pandas extension array (<code>lance._arrow.PandasBFloat16Type</code>) for BFloat16. These are compatible with the ml_dtypes bfloat16 NumPy extension array.</p> <p>If you are using Pandas, you can use the <code>lance.bfloat16</code> dtype string to create the array:</p> <pre><code>import lance.arrow\n\npd.Series([1.1, 2.1, 3.4], dtype=\"lance.bfloat16\")\n# 0    1.1015625\n# 1      2.09375\n# 2      3.40625\n# dtype: lance.bfloat16\n</code></pre> <p>To create an Arrow array, use the <code>lance.arrow.bfloat16_array</code> function:</p> <pre><code>from lance.arrow import bfloat16_array\n\nbfloat16_array([1.1, 2.1, 3.4])\n# &lt;lance.arrow.BFloat16Array object at 0x000000016feb94e0&gt;\n# [\n#   1.1015625,\n#   2.09375,\n#   3.40625\n# ]\n</code></pre> <p>Finally, if you have a pre-existing NumPy array, you can convert it into either:</p> <pre><code>import numpy as np\nfrom ml_dtypes import bfloat16\nfrom lance.arrow import PandasBFloat16Array, BFloat16Array\n\nnp_array = np.array([1.1, 2.1, 3.4], dtype=bfloat16)\nPandasBFloat16Array.from_numpy(np_array)\n# &lt;PandasBFloat16Array&gt;\n# [1.1015625, 2.09375, 3.40625]\n# Length: 3, dtype: lance.bfloat16\nBFloat16Array.from_numpy(np_array)\n# &lt;lance.arrow.BFloat16Array object at 0x...&gt;\n# [\n#   1.1015625,\n#   2.09375,\n#   3.40625\n# ]\n</code></pre> <p>When reading, these can be converted back to to the NumPy bfloat16 dtype using each array class's <code>to_numpy</code> method.</p>"},{"location":"guide/arrays/#imageuri","title":"ImageURI","text":"<p><code>lance.arrow.ImageURIArray</code> is an array that stores the URI location of images in some other storage system. For example, <code>file:///path/to/image.png</code> for a local filesystem or <code>s3://bucket/path/image.jpeg</code> for an image on AWS S3. Use this array type when you want to lazily load images from an existing storage medium.</p> <p>It can be created by calling <code>lance.arrow.ImageURIArray.from_uris</code> with a list of URIs represented by either <code>pyarrow.StringArray</code> or an iterable that yields strings. Note that the URIs are not strongly validated and images are not read into memory automatically.</p> <pre><code>from lance.arrow import ImageURIArray\n\nImageURIArray.from_uris([\n   \"/tmp/image1.jpg\",\n   \"file:///tmp/image2.jpg\",\n   \"s3://example/image3.jpg\"\n])\n# &lt;lance.arrow.ImageURIArray object at 0x...&gt;\n# ['/tmp/image1.jpg', 'file:///tmp/image2.jpg', 's3://example/image3.jpg']\n</code></pre> <p><code>lance.arrow.ImageURIArray.read_uris</code> will read images into memory and return them as a new <code>lance.arrow.EncodedImageArray</code> object.</p> <pre><code>from lance.arrow import ImageURIArray\n\nrelative_path = \"images/1.png\"\nuris = [os.path.join(os.path.dirname(__file__), relative_path)]\nImageURIArray.from_uris(uris).read_uris()\n# &lt;lance.arrow.EncodedImageArray object at 0x...&gt;\n# [b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00...']\n</code></pre>"},{"location":"guide/arrays/#encodedimage","title":"EncodedImage","text":"<p><code>lance.arrow.EncodedImageArray</code> is an array that stores jpeg and png images in their encoded and compressed representation as they would appear written on disk. Use this array when you want to manipulate images in their compressed format such as when you're reading them from disk or embedding them into HTML.</p> <p>It can be created by calling <code>lance.arrow.ImageURIArray.read_uris</code> on an existing <code>lance.arrow.ImageURIArray</code>. This will read the referenced images into memory. It can also be created by calling <code>lance.arrow.ImageArray.from_array</code> and passing it an array of encoded images already read into <code>pyarrow.BinaryArray</code> or by calling <code>lance.arrow.ImageTensorArray.to_encoded</code>.</p> <p>A <code>lance.arrow.EncodedImageArray.to_tensor</code> method is provided to decode encoded images and return them as <code>lance.arrow.FixedShapeImageTensorArray</code>, from which they can be converted to numpy arrays or TensorFlow tensors. For decoding images, it will first attempt to use a decoder provided via the optional function parameter. If decoder is not provided it will attempt to use Pillow and tensorflow in that order. If neither library or custom decoder is available an exception will be raised.</p> <pre><code>from lance.arrow import ImageURIArray\n\nuris = [os.path.join(os.path.dirname(__file__), \"images/1.png\")]\nencoded_images = ImageURIArray.from_uris(uris).read_uris()\nprint(encoded_images.to_tensor())\n\ndef tensorflow_decoder(images):\n    import tensorflow as tf\n    import numpy as np\n\n    return np.stack(tf.io.decode_png(img.as_py(), channels=3) for img in images.storage)\n\nprint(encoded_images.to_tensor(tensorflow_decoder))\n# &lt;lance.arrow.FixedShapeImageTensorArray object at 0x...&gt;\n# [[42, 42, 42, 255]]\n# &lt;lance.arrow.FixedShapeImageTensorArray object at 0x...&gt;\n# [[42, 42, 42, 255]]\n</code></pre>"},{"location":"guide/arrays/#fixedshapeimagetensor","title":"FixedShapeImageTensor","text":"<p><code>lance.arrow.FixedShapeImageTensorArray</code> is an array that stores images as tensors where each individual pixel is represented as a numeric value. Typically images are stored as 3 dimensional tensors shaped (height, width, channels). In color images each pixel is represented by three values (channels) as per RGB color model. Images from this array can be read out as numpy arrays individually or stacked together into a single 4 dimensional numpy array shaped (batch_size, height, width, channels).</p> <p>It can be created by calling <code>lance.arrow.EncodedImageArray.to_tensor</code> on a previously existing <code>lance.arrow.EncodedImageArray</code>. This will decode encoded images and return them as a <code>lance.arrow.FixedShapeImageTensorArray</code>. It can also be created by calling <code>lance.arrow.ImageArray.from_array</code> and passing in a <code>pyarrow.FixedShapeTensorArray</code>.</p> <p>It can be encoded into to <code>lance.arrow.EncodedImageArray</code> by calling <code>lance.arrow.FixedShapeImageTensorArray.to_encoded</code> and passing custom encoder If encoder is not provided it will attempt to use tensorflow and Pillow in that order. Default encoders will encode to PNG. If neither library is available it will raise an exception.</p> <pre><code>from lance.arrow import ImageURIArray\n\nuris = [image_uri]\ntensor_images = ImageURIArray.from_uris(uris).read_uris().to_tensor()\ntensor_images.to_encoded()\n# &lt;lance.arrow.EncodedImageArray object at 0x...&gt;\n# [...\n# b'\\x89PNG\\r\\n\\x1a...'\n</code></pre>"},{"location":"guide/blob/","title":"Blob As Files","text":"<p>Unlike other data formats, large multimodal data is a first-class citizen in the Lance columnar format. Lance provides a high-level API to store and retrieve large binary objects (blobs) in Lance datasets.</p> <p></p> <p>Lance serves large binary data using <code>lance.BlobFile</code>, which is a file-like object that lazily reads large binary objects.</p> <p>To create a Lance dataset with large blob data, you can mark a large binary column as a blob column by adding the metadata <code>lance-encoding:blob</code> to <code>true</code>.</p> <pre><code>import pyarrow as pa\n\nschema = pa.schema(\n    [\n        pa.field(\"id\", pa.int64()),\n        pa.field(\"video\",\n            pa.large_binary(),\n            metadata={\"lance-encoding:blob\": \"true\"}\n        ),\n    ]\n)\n</code></pre> <p>To write blob data to a Lance dataset, create a PyArrow table with the blob schema and use <code>lance.write_dataset</code>:</p> <pre><code>import lance\n\n# First, download a sample video file for testing\n# wget https://www.learningcontainer.com/wp-content/uploads/2020/05/sample-mp4-file.mp4\nimport urllib.request\nurllib.request.urlretrieve(\n    \"https://www.learningcontainer.com/wp-content/uploads/2020/05/sample-mp4-file.mp4\",\n    \"sample_video.mp4\"\n)\n\n# Then read the video file content\nwith open(\"sample_video.mp4\", 'rb') as f:\n    video_data = f.read()\n\n# Create table with blob data\ntable = pa.table({\n    \"id\": [1],\n    \"video\": [video_data],\n}, schema=schema)\n\n# Write to Lance dataset\nds = lance.write_dataset(\n    table,\n    \"./youtube.lance\",\n    schema=schema\n)\n</code></pre> <p>To fetch blobs from a Lance dataset, you can use <code>lance.dataset.LanceDataset.take_blobs</code>.</p> <p>For example, it's easy to use <code>BlobFile</code> to extract frames from a video file without loading the entire video into memory.</p> <pre><code>import av # pip install av\nimport lance\n\nds = lance.dataset(\"./youtube.lance\")\nstart_time, end_time = 500, 1000\n# Get blob data from the first row (id=0)\nblobs = ds.take_blobs(\"video\", ids=[0])\nwith av.open(blobs[0]) as container:\n    stream = container.streams.video[0]\n    stream.codec_context.skip_frame = \"NONKEY\"\n\n    start_time = start_time / stream.time_base\n    start_time = start_time.as_integer_ratio()[0]\n    end_time = end_time / stream.time_base\n    container.seek(start_time, stream=stream)\n\n    for frame in container.decode(stream):\n        if frame.time &gt; end_time:\n            break\n        display(frame.to_image())\n        clear_output(wait=True) \n</code></pre>"},{"location":"guide/data_evolution/","title":"Data Evolution","text":"<p>Lance supports traditional schema evolution: adding, removing, and altering columns in a dataset. Most of these operations can be performed without rewriting the data files in the dataset, making them very efficient operations. In addition, Lance supports data evolution, which allows you to also backfill existing rows with the new column data without rewriting the data files in the dataset, making it highly suitable for use cases like ML feature engineering.</p> <p>In general, schema changes will conflict with most other concurrent write operations. For example, if you change the schema of the dataset while someone else is appending data to it, either your schema change or the append will fail, depending on the order of the operations. Thus, it's recommended to perform schema changes when no other writes are happening.</p>"},{"location":"guide/data_evolution/#adding-new-columns","title":"Adding new columns","text":""},{"location":"guide/data_evolution/#schema-only","title":"Schema only","text":"<p>A common use case we've seen in production is to add a new column to a dataset without populating it. This is useful to later run a large distributed job to populate the column lazily. To do this, you can use the <code>lance.LanceDataset.add_columns</code> method to add columns with <code>pyarrow.Field</code> or <code>pyarrow.Schema</code>.</p> <pre><code>table = pa.table({\"id\": pa.array([1, 2, 3])})\ndataset = lance.write_dataset(table, \"null_columns\")\n\n# With pyarrow Field\ndataset.add_columns(pa.field(\"embedding\", pa.list_(pa.float32(), 128)))\nassert dataset.schema == pa.schema([\n    (\"id\", pa.int64()),\n    (\"embedding\", pa.list_(pa.float32(), 128)),\n])\n\n# With pyarrow Schema\ndataset.add_columns(pa.schema([\n    (\"label\", pa.string()),\n    (\"score\", pa.float32()),\n]))\nassert dataset.schema == pa.schema([\n    (\"id\", pa.int64()),\n    (\"embedding\", pa.list_(pa.float32(), 128)),\n    (\"label\", pa.string()),\n    (\"score\", pa.float32()),\n])\n</code></pre> <p>This operation is very fast, as it only updates the metadata of the dataset.</p>"},{"location":"guide/data_evolution/#with-data-backfill","title":"With data backfill","text":"<p>New columns can be added and populated within a single operation using the <code>lance.LanceDataset.add_columns</code> method. There are two ways to specify how to populate the new columns: first, by providing a SQL expression for each new column, or second, by providing a function to generate the new column data.</p> <p>SQL expressions can either be independent expressions or reference existing columns. SQL literal values can be used to set a single value for all existing rows.</p> <pre><code>table = pa.table({\"name\": pa.array([\"Alice\", \"Bob\", \"Carla\"])})\ndataset = lance.write_dataset(table, \"names\")\ndataset.add_columns({\n    \"hash\": \"sha256(name)\",\n    \"status\": \"'active'\",\n})\nprint(dataset.to_table().to_pandas())\n#     name                                               hash  status\n# 0  Alice  b';\\xc5\\x10b\\x97&lt;E\\x8dZo-\\x8dd\\xa0#$cT\\xad~\\x0...  active\n# 1    Bob  b'\\xcd\\x9f\\xb1\\xe1H\\xcc\\xd8D.Z\\xa7I\\x04\\xccs\\x...  active\n# 2  Carla  b'\\xad\\x8d\\x83\\xff\\xd8+Z\\x8e\\xd4)\\xe8Y+\\\\\\xb3\\...  active\n</code></pre> <p>You can also provide a Python function to generate the new column data. This can be used, for example, to compute a new embedding column. This function should take a PyArrow RecordBatch and return either a PyArrow RecordBatch or a Pandas DataFrame. The function will be called once for each batch in the dataset.</p> <p>If the function is expensive to compute and can fail, it is recommended to set a checkpoint file in the UDF. This checkpoint file saves the state of the UDF after each invocation, so that if the UDF fails, it can be restarted from the last checkpoint. Note that this file can get quite large, since it needs to store unsaved results for up to an entire data file.</p> <pre><code>import lance\nimport pyarrow as pa\nimport numpy as np\n\ntable = pa.table({\"id\": pa.array([1, 2, 3])})\ndataset = lance.write_dataset(table, \"ids\")\n\n@lance.batch_udf(checkpoint_file=\"embedding_checkpoint.sqlite\")\ndef add_random_vector(batch):\n    embeddings = np.random.rand(batch.num_rows, 128).astype(\"float32\")\n    return pa.RecordBatch.from_arrays(\n        [pa.FixedSizeListArray.from_arrays(embeddings.flatten(), 128)],\n        names=[\"embedding\"]\n    )\ndataset.add_columns(add_random_vector)\n</code></pre>"},{"location":"guide/data_evolution/#using-merge","title":"Using merge","text":"<p>If you have pre-computed one or more new columns, you can add them to an existing dataset using the <code>lance.LanceDataset.merge</code> method. This allows filling in additional columns without having to rewrite the whole dataset.</p> <p>To use the <code>merge</code> method, provide a new dataset that includes the columns you want to add, and a column name to use for joining the new data to the existing dataset.</p> <p>For example, imagine we have a dataset of embeddings and ids:</p> <pre><code>table = pa.table({\n   \"id\": pa.array([1, 2, 3]),\n   \"embedding\": pa.array([np.array([1, 2, 3]), np.array([4, 5, 6]),\n                          np.array([7, 8, 9])])\n})\ndataset = lance.write_dataset(table, \"embeddings\", mode=\"overwrite\")\n</code></pre> <p>Now if we want to add a column of labels we have generated, we can do so by merging a new table:</p> <pre><code>new_data = pa.table({\n   \"id\": pa.array([1, 2, 3]),\n   \"label\": pa.array([\"horse\", \"rabbit\", \"cat\"])\n})\ndataset.merge(new_data, \"id\")\nprint(dataset.to_table().to_pandas())\n#    id  embedding   label\n# 0   1  [1, 2, 3]   horse\n# 1   2  [4, 5, 6]  rabbit\n# 2   3  [7, 8, 9]     cat\n</code></pre>"},{"location":"guide/data_evolution/#dropping-columns","title":"Dropping columns","text":"<p>Finally, you can drop columns from a dataset using the <code>lance.LanceDataset.drop_columns</code> method. This is a metadata-only operation and does not delete the data on disk. This makes it very quick.</p> <pre><code>table = pa.table({\"id\": pa.array([1, 2, 3]),\n                 \"name\": pa.array([\"Alice\", \"Bob\", \"Carla\"])})\ndataset = lance.write_dataset(table, \"names\", mode=\"overwrite\")\ndataset.drop_columns([\"name\"])\nprint(dataset.schema)\n# id: int64\n</code></pre> <p>To actually remove the data from disk, the files must be rewritten to remove the columns and then the old files must be deleted. This can be done using <code>lance.dataset.DatasetOptimizer.compact_files()</code> followed by <code>lance.LanceDataset.cleanup_old_versions()</code>.</p>"},{"location":"guide/data_evolution/#renaming-columns","title":"Renaming columns","text":"<p>Columns can be renamed using the <code>lance.LanceDataset.alter_columns</code> method.</p> <pre><code>table = pa.table({\"id\": pa.array([1, 2, 3])})\ndataset = lance.write_dataset(table, \"ids\")\ndataset.alter_columns({\"path\": \"id\", \"name\": \"new_id\"})\nprint(dataset.to_table().to_pandas())\n#    new_id\n# 0       1\n# 1       2\n# 2       3\n</code></pre> <p>This works for nested columns as well. To address a nested column, use a dot (<code>.</code>) to separate the levels of nesting. For example:</p> <pre><code>data = [\n  {\"meta\": {\"id\": 1, \"name\": \"Alice\"}},\n  {\"meta\": {\"id\": 2, \"name\": \"Bob\"}},\n]\nschema = pa.schema([\n    (\"meta\", pa.struct([\n        (\"id\", pa.int32()),\n        (\"name\", pa.string()),\n    ]))\n])\ndataset = lance.write_dataset(data, \"nested_rename\")\ndataset.alter_columns({\"path\": \"meta.id\", \"name\": \"new_id\"})\nprint(dataset.to_table().to_pandas())\n#                                  meta\n# 0  {'new_id': 1, 'name': 'Alice'}\n# 1    {'new_id': 2, 'name': 'Bob'}\n</code></pre>"},{"location":"guide/data_evolution/#casting-column-data-types","title":"Casting column data types","text":"<p>In addition to changing column names, you can also change the data type of a column using the <code>lance.LanceDataset.alter_columns</code> method. This requires rewriting that column to new data files, but does not require rewriting the other columns.</p> <p>Note</p> <p>If the column has an index, the index will be dropped if the column type is changed.</p> <p>This method can be used to change the vector type of a column. For example, we can change a float32 embedding column into a float16 column to save disk space at the cost of lower precision:</p> <pre><code>table = pa.table({\n   \"id\": pa.array([1, 2, 3]),\n   \"embedding\": pa.FixedShapeTensorArray.from_numpy_ndarray(\n       np.random.rand(3, 128).astype(\"float32\"))\n})\ndataset = lance.write_dataset(table, \"embeddings\")\ndataset.alter_columns({\"path\": \"embedding\",\n                       \"data_type\": pa.list_(pa.float16(), 128)})\nprint(dataset.schema)\n# id: int64\n# embedding: fixed_size_list&lt;item: halffloat&gt;[128]\n#   child 0, item: halffloat\n</code></pre>"},{"location":"guide/distributed_write/","title":"Distributed Write","text":"<p>Warning</p> <p>Lance provides out-of-the-box Ray and Spark integrations.</p> <p>This page is intended for users who wish to perform distributed operations in a custom manner, i.e. using <code>slurm</code> or <code>Kubernetes</code> without the Lance integration.</p>"},{"location":"guide/distributed_write/#overview","title":"Overview","text":"<p>The Lance format is designed to support parallel writing across multiple distributed workers. A distributed write operation can be performed by two phases:</p> <ol> <li>Parallel Writes: Generate new <code>lance.LanceFragment</code> in parallel across multiple workers.</li> <li>Commit: Collect all the <code>lance.FragmentMetadata</code> and commit into a single dataset in a single <code>lance.LanceOperation</code>.</li> </ol> <p></p>"},{"location":"guide/distributed_write/#write-new-data","title":"Write new data","text":"<p>Writing or appending new data is straightforward with <code>lance.fragment.write_fragments</code>.</p> <pre><code>import json\nfrom lance.fragment import write_fragments\n\n# Run on each worker\ndata_uri = \"./dist_write\"\nschema = pa.schema([\n    (\"a\", pa.int32()),\n    (\"b\", pa.string()),\n])\n\n# Run on worker 1\ndata1 = {\n    \"a\": [1, 2, 3],\n    \"b\": [\"x\", \"y\", \"z\"],\n}\nfragments_1 = write_fragments(data1, data_uri, schema=schema)\nprint(\"Worker 1: \", fragments_1)\n\n# Run on worker 2\ndata2 = {\n    \"a\": [4, 5, 6],\n    \"b\": [\"u\", \"v\", \"w\"],\n}\nfragments_2 = write_fragments(data2, data_uri, schema=schema)\nprint(\"Worker 2: \", fragments_2)\n</code></pre> <p>Output: <pre><code>Worker 1:  [FragmentMetadata(id=0, files=...)]\nWorker 2:  [FragmentMetadata(id=0, files=...)]\n</code></pre></p> <p>Now, use <code>lance.fragment.FragmentMetadata.to_json</code> to serialize the fragment metadata, and collect all serialized metadata on a single worker to execute the final commit operation.</p> <pre><code>import json\nfrom lance import FragmentMetadata, LanceOperation\n\n# Serialize Fragments into JSON data\nfragments_json1 = [json.dumps(fragment.to_json()) for fragment in fragments_1]\nfragments_json2 = [json.dumps(fragment.to_json()) for fragment in fragments_2]\n\n# On one worker, collect all fragments\nall_fragments = [FragmentMetadata.from_json(f) for f in \\\n    fragments_json1 + fragments_json2]\n\n# Commit the fragments into a single dataset\n# Use LanceOperation.Overwrite to overwrite the dataset or create new dataset.\nop = lance.LanceOperation.Overwrite(schema, all_fragments)\nread_version = 0 # Because it is empty at the time.\nlance.LanceDataset.commit(\n    data_uri,\n    op,\n    read_version=read_version,\n)\n\n# We can read the dataset using the Lance API:\ndataset = lance.dataset(data_uri)\nassert len(dataset.get_fragments()) == 2\nassert dataset.version == 1\nprint(dataset.to_table().to_pandas())\n</code></pre> <p>Output: <pre><code>     a  b\n0  1  x\n1  2  y\n2  3  z\n3  4  u\n4  5  v\n5  6  w\n</code></pre></p>"},{"location":"guide/distributed_write/#append-data","title":"Append data","text":"<p>Appending additional data follows a similar process. Use <code>lance.LanceOperation.Append</code> to commit the new fragments, ensuring that the <code>read_version</code> is set to the current dataset's version.</p> <pre><code>import lance\n\nds = lance.dataset(data_uri)\nread_version = ds.version # record the read version\n\nop = lance.LanceOperation.Append(schema, all_fragments)\nlance.LanceDataset.commit(\n    data_uri,\n    op,\n    read_version=read_version,\n)\n</code></pre>"},{"location":"guide/distributed_write/#add-new-columns","title":"Add New Columns","text":"<p>Lance Format excels at operations such as adding columns. Thanks to its two-dimensional layout (see this blog post), adding new columns is highly efficient since it avoids copying the existing data files. Instead, the process simply creates new data files and links them to the existing dataset using metadata-only operations.</p> <pre><code>import lance\nfrom pyarrow import RecordBatch\nimport pyarrow.compute as pc\n\ndataset = lance.dataset(\"./add_columns_example\")\nassert len(dataset.get_fragments()) == 2\nassert dataset.to_table().combine_chunks() == pa.Table.from_pydict({\n    \"name\": [\"alice\", \"bob\", \"charlie\", \"craig\", \"dave\", \"eve\"],\n    \"age\": [25, 33, 44, 55, 66, 77],\n}, schema=schema)\n\n\ndef name_len(names: RecordBatch) -&gt; RecordBatch:\n    return RecordBatch.from_arrays(\n        [pc.utf8_length(names[\"name\"])],\n        [\"name_len\"],\n    )\n\n# On Worker 1\nfrag1 = dataset.get_fragments()[0]\nnew_fragment1, new_schema = frag1.merge_columns(name_len, [\"name\"])\n\n# On Worker 2\nfrag2 = dataset.get_fragments()[1]\nnew_fragment2, _ = frag2.merge_columns(name_len, [\"name\"])\n\n# On Worker 3 - Commit\nall_fragments = [new_fragment1, new_fragment2]\nop = lance.LanceOperation.Merge(all_fragments, schema=new_schema)\nlance.LanceDataset.commit(\n    \"./add_columns_example\",\n    op,\n    read_version=dataset.version,\n)\n\n# Verify dataset\ndataset = lance.dataset(\"./add_columns_example\")\nprint(dataset.to_table().to_pandas())\n</code></pre> <p>Output: <pre><code>      name  age  name_len\n0    alice   25         5\n1      bob   33         3\n2  charlie   44         7\n3    craig   55         5\n4     dave   66         4\n5      eve   77         3\n</code></pre></p>"},{"location":"guide/distributed_write/#update-columns","title":"Update Columns","text":"<p>Currently, Lance supports the fragment level update columns ability to update existing columns in a distributed manner.</p> <p>This operation performs a left-outer-hash-join with the right table (new data) on the column specified by <code>left_on</code> and <code>right_on</code>. For every row in the current fragment, the updated column value is: 1. If no matched row on the right side, the column value of the left side row. 2. If there is exactly one corresponding row on the right side, the column value    of the matching row. 3. If there are multiple corresponding rows, the column value of a random row.</p> <pre><code>import lance\nimport pyarrow as pa\n\n# Create initial dataset with two fragments\n# First fragment\ndata1 = pa.table(\n    {\n        \"id\": [1, 2, 3, 4],\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\"],\n        \"score\": [85, 90, 75, 80],\n    }\n)\ndataset_uri = \"./my_dataset.lance\"\ndataset = lance.write_dataset(data1, dataset_uri)\n\n# Second fragment\ndata2 = pa.table(\n    {\n        \"id\": [5, 6, 7, 8],\n        \"name\": [\"Eve\", \"Frank\", \"Grace\", \"Henry\"],\n        \"score\": [88, 92, 78, 82],\n    }\n)\ndataset = lance.write_dataset(data2, dataset_uri, mode=\"append\")\n\n# Prepare update data for fragment 0 using 'id' as join key\nupdate_data1 = pa.table(\n    {\n        \"id\": [1, 3],\n        \"name\": [\"Alan\", \"Chase\"],\n        \"score\": [95, 85],\n    }\n)\n\n# Prepare update data for fragment 1\nupdate_data2 = pa.table(\n    {\n        \"id\": [5, 7],\n        \"name\": [\"Eva\", \"Gracie\"],\n        \"score\": [98, 88],\n    }\n)\n\n# Update fragment 0\nfragment0 = dataset.get_fragment(0)\nupdated_fragment0, fields_modified0 = fragment0.update_columns(\n    update_data1, left_on=\"id\", right_on=\"id\"\n)\n\n# Update fragment 1\nfragment1 = dataset.get_fragment(1)\nupdated_fragment1, fields_modified1 = fragment1.update_columns(\n    update_data2, left_on=\"id\", right_on=\"id\"\n)\n\nunion_fields_modified = list(set(fields_modified0 + fields_modified1))\n# Commit the changes for both fragments\nop = lance.LanceOperation.Update(\n    updated_fragments=[updated_fragment0, updated_fragment1],\n    fields_modified=union_fields_modified,\n)\nupdated_dataset = lance.LanceDataset.commit(\n    str(dataset_uri), op, read_version=dataset.version\n)\n\n# Verify the update\ndataset = lance.dataset(dataset_uri)\nprint(dataset.to_table().to_pandas())\n</code></pre> <p>Output: <pre><code>   id    name  score\n0   1    Alan     95\n1   2     Bob     90\n2   3   Chase     85\n3   4   David     80\n4   5     Eva     98\n5   6   Frank     92\n6   7  Gracie     88\n7   8   Henry     82\n</code></pre></p>"},{"location":"guide/json/","title":"JSON Support","text":"<p>Lance provides comprehensive support for storing and querying JSON data, enabling you to work with semi-structured data efficiently. This guide covers how to store JSON data in Lance datasets and use JSON functions to query and filter your data.</p>"},{"location":"guide/json/#getting-started","title":"Getting Started","text":"<pre><code>import lance\nimport pyarrow as pa\nimport json\n\n# Create a table with JSON data\njson_data = {\"name\": \"Alice\", \"age\": 30, \"city\": \"New York\"}\njson_arr = pa.array([json.dumps(json_data)], type=pa.json_())\ntable = pa.table({\"id\": [1], \"data\": json_arr})\n\n# Write the dataset\nlance.write_dataset(table, \"dataset.lance\")\n</code></pre>"},{"location":"guide/json/#storage-format","title":"Storage Format","text":"<p>Lance stores JSON data internally as JSONB (binary JSON) using the <code>lance.json</code> extension type. This provides:</p> <ul> <li>Efficient storage through binary encoding</li> <li>Fast query performance for nested field access</li> <li>Compatibility with Apache Arrow's JSON type</li> </ul> <p>When you read JSON data back from Lance, it's automatically converted to Arrow's JSON type for seamless integration with your data processing pipelines.</p>"},{"location":"guide/json/#json-functions","title":"JSON Functions","text":"<p>Lance provides a comprehensive set of JSON functions for querying and filtering JSON data. These functions can be used in filter expressions with methods like <code>to_table()</code>, <code>scanner()</code>, and SQL queries through DataFusion integration.</p>"},{"location":"guide/json/#data-access-functions","title":"Data Access Functions","text":""},{"location":"guide/json/#json_extract","title":"json_extract","text":"<p>Extracts a value from JSON using JSONPath syntax.</p> <p>Syntax: <code>json_extract(json_column, json_path)</code></p> <p>Returns: JSON-formatted string representation of the extracted value</p> <p>Example: <pre><code># Sample data: {\"user\": {\"name\": \"Alice\", \"age\": 30}}\nresult = dataset.to_table(\n    filter=\"json_extract(data, '$.user.name') = '\\\"Alice\\\"'\"\n)\n# Returns: \"\\\"Alice\\\"\" for strings, \"30\" for numbers, \"true\" for booleans\n</code></pre></p> <p>Note</p> <p><code>json_extract</code> returns values in JSON format. String values include quotes (e.g., <code>\"Alice\"</code>),  numbers are returned as-is (e.g., <code>30</code>), and booleans as <code>true</code>/<code>false</code>.</p>"},{"location":"guide/json/#json_get","title":"json_get","text":"<p>Retrieves a field or array element from JSON, returning it as JSONB for further processing.</p> <p>Syntax: <code>json_get(json_column, key_or_index)</code></p> <p>Parameters: - <code>key_or_index</code>: Field name (string) or array index (numeric string like \"0\", \"1\")</p> <p>Returns: JSONB binary value (can be used for nested access)</p> <p>Example: <pre><code># Access nested JSON by chaining json_get calls\n# Sample data: {\"user\": {\"profile\": {\"name\": \"Alice\"}}}\nresult = dataset.to_table(\n    filter=\"json_get_string(json_get(json_get(data, 'user'), 'profile'), 'name') = 'Alice'\"\n)\n\n# Access array elements by index\n# Sample data: [\"first\", \"second\", \"third\"]\nresult = dataset.to_table(\n    filter=\"json_get_string(data, '0') = 'first'\"  # Gets first array element\n)\n</code></pre></p>"},{"location":"guide/json/#type-safe-value-extraction","title":"Type-Safe Value Extraction","text":"<p>These functions extract values with strict type conversion. The conversion uses JSONB's built-in strict mode, which requires values to be of compatible types:</p>"},{"location":"guide/json/#json_get_string","title":"json_get_string","text":"<p>Extracts a string value from JSON.</p> <p>Syntax: <code>json_get_string(json_column, key_or_index)</code></p> <p>Parameters: - <code>key_or_index</code>: Field name or array index (as string)</p> <p>Returns: String value (without JSON quotes), null if conversion fails</p> <p>Type Conversion: Uses strict conversion - numbers and booleans are converted to their string representation</p> <p>Example: <pre><code>result = dataset.to_table(\n    filter=\"json_get_string(data, 'name') = 'Alice'\"\n)\n\n# Array access example\n# Sample data: [\"first\", \"second\"]\nresult = dataset.to_table(\n    filter=\"json_get_string(data, '1') = 'second'\"  # Gets second array element\n)\n</code></pre></p>"},{"location":"guide/json/#json_get_int","title":"json_get_int","text":"<p>Extracts an integer value with strict type conversion.</p> <p>Syntax: <code>json_get_int(json_column, key_or_index)</code></p> <p>Returns: 64-bit integer, null if conversion fails</p> <p>Type Conversion: Uses JSONB's strict <code>to_i64()</code> conversion: - Numbers are truncated to integers - Strings must be parseable as numbers - Booleans: true \u2192 1, false \u2192 0</p> <p>Example: <pre><code># {\"age\": 30} works, {\"age\": \"30\"} may work if JSONB allows string parsing\nresult = dataset.to_table(\n    filter=\"json_get_int(data, 'age') &gt; 25\"\n)\n</code></pre></p>"},{"location":"guide/json/#json_get_float","title":"json_get_float","text":"<p>Extracts a floating-point value with strict type conversion.</p> <p>Syntax: <code>json_get_float(json_column, key_or_index)</code></p> <p>Returns: 64-bit float, null if conversion fails</p> <p>Type Conversion: Uses JSONB's strict <code>to_f64()</code> conversion: - Integers are converted to floats - Strings must be parseable as numbers - Booleans: true \u2192 1.0, false \u2192 0.0</p> <p>Example: <pre><code>result = dataset.to_table(\n    filter=\"json_get_float(data, 'score') &gt;= 90.5\"\n)\n</code></pre></p>"},{"location":"guide/json/#json_get_bool","title":"json_get_bool","text":"<p>Extracts a boolean value with strict type conversion.</p> <p>Syntax: <code>json_get_bool(json_column, key_or_index)</code></p> <p>Returns: Boolean, null if conversion fails</p> <p>Type Conversion: Uses JSONB's strict <code>to_bool()</code> conversion: - Numbers: 0 \u2192 false, non-zero \u2192 true - Strings: \"true\" \u2192 true, \"false\" \u2192 false (exact match required) - Other values may fail conversion</p> <p>Example: <pre><code>result = dataset.to_table(\n    filter=\"json_get_bool(data, 'active') = true\"\n)\n</code></pre></p>"},{"location":"guide/json/#existence-and-array-functions","title":"Existence and Array Functions","text":""},{"location":"guide/json/#json_exists","title":"json_exists","text":"<p>Checks if a JSONPath exists in the JSON data.</p> <p>Syntax: <code>json_exists(json_column, json_path)</code></p> <p>Returns: Boolean</p> <p>Example: <pre><code># Find records that have an age field\nresult = dataset.to_table(\n    filter=\"json_exists(data, '$.user.age')\"\n)\n</code></pre></p>"},{"location":"guide/json/#json_array_contains","title":"json_array_contains","text":"<p>Checks if a JSON array contains a specific value.</p> <p>Syntax: <code>json_array_contains(json_column, json_path, value)</code></p> <p>Returns: Boolean</p> <p>Comparison Logic:  - Compares array elements as JSON strings - For string matching, tries both with and without quotes - Example: searching for 'python' matches both <code>\"python\"</code> and <code>python</code> in the array</p> <p>Example: <pre><code># Sample data: {\"tags\": [\"python\", \"ml\", \"data\"]}\nresult = dataset.to_table(\n    filter=\"json_array_contains(data, '$.tags', 'python')\"\n)\n</code></pre></p>"},{"location":"guide/json/#json_array_length","title":"json_array_length","text":"<p>Returns the length of a JSON array.</p> <p>Syntax: <code>json_array_length(json_column, json_path)</code></p> <p>Returns:  - Integer: length of the array - null: if path doesn't exist - Error: if path points to a non-array value</p> <p>Example: <pre><code># Find records with more than 3 tags\nresult = dataset.to_table(\n    filter=\"json_array_length(data, '$.tags') &gt; 3\"\n)\n\n# Empty arrays return 0\nresult = dataset.to_table(\n    filter=\"json_array_length(data, '$.empty_array') = 0\"\n)\n</code></pre></p>"},{"location":"guide/json/#usage-examples","title":"Usage Examples","text":""},{"location":"guide/json/#working-with-nested-json","title":"Working with Nested JSON","text":"<pre><code>import lance\nimport pyarrow as pa\nimport json\n\n# Create nested JSON data\ndata = [\n    {\n        \"id\": 1,\n        \"user\": {\n            \"profile\": {\n                \"name\": \"Alice\",\n                \"settings\": {\n                    \"theme\": \"dark\",\n                    \"notifications\": True\n                }\n            },\n            \"scores\": [95, 87, 92]\n        }\n    },\n    {\n        \"id\": 2,\n        \"user\": {\n            \"profile\": {\n                \"name\": \"Bob\",\n                \"settings\": {\n                    \"theme\": \"light\",\n                    \"notifications\": False\n                }\n            },\n            \"scores\": [88, 91, 85]\n        }\n    }\n]\n\n# Convert to Lance dataset\njson_strings = [json.dumps(d) for d in data]\ntable = pa.table({\n    \"data\": pa.array(json_strings, type=pa.json_())\n})\n\nlance.write_dataset(table, \"nested.lance\")\ndataset = lance.dataset(\"nested.lance\")\n\n# Query nested fields using JSONPath\ndark_theme_users = dataset.to_table(\n    filter=\"json_extract(data, '$.user.profile.settings.theme') = '\\\"dark\\\"'\"\n)\n\n# Or using chained json_get\nhigh_scorers = dataset.to_table(\n    filter=\"json_array_length(data, '$.user.scores') &gt;= 3\"\n)\n</code></pre>"},{"location":"guide/json/#combining-json-with-other-data-types","title":"Combining JSON with Other Data Types","text":"<pre><code># Create mixed-type table with JSON metadata\nproducts = pa.table({\n    \"id\": [1, 2, 3],\n    \"name\": [\"Laptop\", \"Phone\", \"Tablet\"],\n    \"price\": [999.99, 599.99, 399.99],\n    \"specs\": pa.array([\n        json.dumps({\"cpu\": \"i7\", \"ram\": 16, \"storage\": 512}),\n        json.dumps({\"screen\": 6.1, \"battery\": 4000, \"5g\": True}),\n        json.dumps({\"screen\": 10.5, \"battery\": 7000, \"stylus\": True})\n    ], type=pa.json_())\n})\n\nlance.write_dataset(products, \"products.lance\")\ndataset = lance.dataset(\"products.lance\")\n\n# Find products with specific specs\nresult = dataset.to_table(\n    filter=\"price &lt; 600 AND json_get_bool(specs, '5g') = true\"\n)\n</code></pre>"},{"location":"guide/json/#handling-arrays-in-json","title":"Handling Arrays in JSON","text":"<pre><code># Create data with JSON arrays\nrecords = pa.table({\n    \"id\": [1, 2, 3],\n    \"data\": pa.array([\n        json.dumps({\"name\": \"Project A\", \"tags\": [\"python\", \"ml\", \"production\"]}),\n        json.dumps({\"name\": \"Project B\", \"tags\": [\"rust\", \"systems\"]}),\n        json.dumps({\"name\": \"Project C\", \"tags\": [\"python\", \"web\", \"api\", \"production\"]})\n    ], type=pa.json_())\n})\n\nlance.write_dataset(records, \"projects.lance\")\ndataset = lance.dataset(\"projects.lance\")\n\n# Find projects with Python\npython_projects = dataset.to_table(\n    filter=\"json_array_contains(data, '$.tags', 'python')\"\n)\n\n# Find projects with more than 3 tags\ncomplex_projects = dataset.to_table(\n    filter=\"json_array_length(data, '$.tags') &gt; 3\"\n)\n</code></pre>"},{"location":"guide/json/#performance-considerations","title":"Performance Considerations","text":"<ol> <li>Choose the right function: Use <code>json_get_*</code> functions for direct field access and type conversion; use <code>json_extract</code> for complex JSONPath queries.</li> <li>Index frequently queried paths: Consider creating computed columns for frequently accessed JSON paths to improve query performance.</li> <li>Minimize deep nesting: While Lance supports arbitrary nesting, flatter structures generally perform better.</li> <li>Understand type conversion: The <code>json_get_*</code> functions use strict type conversion, which may fail if types don't match. Plan your schema accordingly.</li> <li>Array access: When working with JSON arrays, you can access elements by index using numeric strings (e.g., \"0\", \"1\") with <code>json_get</code> functions.</li> </ol>"},{"location":"guide/json/#integration-with-datafusion","title":"Integration with DataFusion","text":"<p>All JSON functions are available when using Lance with Apache DataFusion for SQL queries. See the DataFusion Integration guide for more details on using JSON functions in SQL contexts.</p>"},{"location":"guide/json/#limitations","title":"Limitations","text":"<ul> <li>JSONPath support follows standard JSONPath syntax but may not support all advanced features</li> <li>Large JSON documents may impact query performance</li> <li>JSON functions are currently only available for filtering, not for projection in query results</li> </ul>"},{"location":"guide/migration/","title":"Migration Guides","text":"<p>Lance aims to avoid breaking changes when possible.  Currently, we are refining the Rust public API so that we can move it out of experimental status and make stronger commitments to backwards compatibility.  The python API is considered stable and breaking changes should generally be communicated (via warnings) for 1-2 months prior to being finalized to give users a chance to migrate.  This page documents the breaking changes between releases and gives advice on how to migrate.</p>"},{"location":"guide/migration/#039","title":"0.39","text":"<ul> <li> <p>The <code>lance</code> crate no longer re-exports utilities from <code>lance-arrow</code> such as <code>RecordBatchExt</code> or <code>SchemaExt</code>.  In the short term, if you are relying on these utilities,  you can add a dependency on the <code>lance-arrow</code> crate.  However, we do not expect <code>lance-arrow</code> to ever be stable, and you may want to consider forking these utilities.</p> </li> <li> <p>Previously, we exported <code>Error</code> and <code>Result</code> as both <code>lance::Error</code> and <code>lance::error::Error</code>.  We have now reduced this to just <code>lance::Error</code>.  We have also removed some internal error utilities (such as <code>OptionExt</code>) from the public API and do not plan on reintroducing these.</p> </li> <li> <p>The Python and Rust <code>dataset::diff_meta</code> API has been removed in favor of <code>dataset::delta</code>,  which returns a <code>DatasetDelta</code> that offers both metadata diff through <code>list_transactions</code> and data diff  through <code>get_inserted_rows</code> and <code>get_updated_rows</code>.</p> </li> <li> <p>Some other minor utilities which had previously been public are now private.  It is unlikely anyone was utilizing' these.  Please open an issue if you were relying on any of these.</p> </li> <li> <p>The <code>lance-namespace</code> Rust crate now splits into <code>lance-namespace</code> that contains the main <code>LanceNamespace</code> trait  and data models, and <code>lance-namespace-impls</code> that has different implementations of the namespace.  The <code>DirectoryNamespace</code> and <code>RestNamespace</code> interfaces have been refactored to be more user friendly. The <code>DirectoryNamespace</code> also now uses Lance ObjectStore for IO instead of directly depending on Apache OpenDAL.</p> </li> </ul>"},{"location":"guide/object_store/","title":"Object Store Configuration","text":"<p>Lance supports object stores such as AWS S3 (and compatible stores), Azure Blob Store, and Google Cloud Storage. Which object store to use is determined by the URI scheme of the dataset path. For example, <code>s3://bucket/path</code> will use S3, <code>az://bucket/path</code> will use Azure, and <code>gs://bucket/path</code> will use GCS.</p> <p>These object stores take additional configuration objects. There are two ways to specify these configurations: by setting environment variables or by passing them to the <code>storage_options</code> parameter of <code>lance.dataset</code> and <code>lance.write_dataset</code>. So for example, to globally set a higher timeout, you would run in your shell:</p> <pre><code>export TIMEOUT=60s\n</code></pre> <p>If you only want to set the timeout for a single dataset, you can pass it as a storage option:</p> <pre><code>import lance\nds = lance.dataset(\"s3://path\", storage_options={\"timeout\": \"60s\"})\n</code></pre>"},{"location":"guide/object_store/#general-configuration","title":"General Configuration","text":"<p>These options apply to all object stores.</p> Key Description <code>allow_http</code> Allow non-TLS, i.e. non-HTTPS connections. Default, <code>False</code>. <code>download_retry_count</code> Number of times to retry a download. Default, <code>3</code>. This limit is applied when the HTTP request succeeds but the response is not fully downloaded, typically due to a violation of <code>request_timeout</code>. <code>allow_invalid_certificates</code> Skip certificate validation on https connections. Default, <code>False</code>. Warning: This is insecure and should only be used for testing. <code>connect_timeout</code> Timeout for only the connect phase of a Client. Default, <code>5s</code>. <code>request_timeout</code> Timeout for the entire request, from connection until the response body has finished. Default, <code>30s</code>. <code>user_agent</code> User agent string to use in requests. <code>proxy_url</code> URL of a proxy server to use for requests. Default, <code>None</code>. <code>proxy_ca_certificate</code> PEM-formatted CA certificate for proxy connections <code>proxy_excludes</code> List of hosts that bypass proxy. This is a comma separated list of domains and IP masks. Any subdomain of the provided domain will be bypassed. For example, <code>example.com, 192.168.1.0/24</code> would bypass <code>https://api.example.com</code>, <code>https://www.example.com</code>, and any IP in the range <code>192.168.1.0/24</code>. <code>client_max_retries</code> Number of times for a s3 client to retry the request. Default, <code>10</code>. <code>client_retry_timeout</code> Timeout for a s3 client to retry the request in seconds. Default, <code>180</code>."},{"location":"guide/object_store/#s3-configuration","title":"S3 Configuration","text":"<p>S3 (and S3-compatible stores) have additional configuration options that configure authorization and S3-specific features (such as server-side encryption).</p> <p>AWS credentials can be set in the environment variables <code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code>, and <code>AWS_SESSION_TOKEN</code>. Alternatively, they can be passed as parameters to the <code>storage_options</code> parameter:</p> <pre><code>import lance\nds = lance.dataset(\n    \"s3://bucket/path\",\n    storage_options={\n        \"access_key_id\": \"my-access-key\",\n        \"secret_access_key\": \"my-secret-key\",\n        \"session_token\": \"my-session-token\",\n    }\n)\n</code></pre> <p>If you are using AWS SSO, you can specify the <code>AWS_PROFILE</code> environment variable. It cannot be specified in the <code>storage_options</code> parameter.</p> <p>The following keys can be used as both environment variables or keys in the <code>storage_options</code> parameter:</p> Key Description <code>aws_region</code> / <code>region</code> The AWS region the bucket is in. This can be automatically detected when using AWS S3, but must be specified for S3-compatible stores. <code>aws_access_key_id</code> / <code>access_key_id</code> The AWS access key ID to use. <code>aws_secret_access_key</code> / <code>secret_access_key</code> The AWS secret access key to use. <code>aws_session_token</code> / <code>session_token</code> The AWS session token to use. <code>aws_endpoint</code> / <code>endpoint</code> The endpoint to use for S3-compatible stores. <code>aws_virtual_hosted_style_request</code> / <code>virtual_hosted_style_request</code> Whether to use virtual hosted-style requests, where bucket name is part of the endpoint. Meant to be used with <code>aws_endpoint</code>. Default, <code>False</code>. <code>aws_s3_express</code> / <code>s3_express</code> Whether to use S3 Express One Zone endpoints. Default, <code>False</code>. See more details below. <code>aws_server_side_encryption</code> The server-side encryption algorithm to use. Must be one of <code>\"AES256\"</code>, <code>\"aws:kms\"</code>, or <code>\"aws:kms:dsse\"</code>. Default, <code>None</code>. <code>aws_sse_kms_key_id</code> The KMS key ID to use for server-side encryption. If set, <code>aws_server_side_encryption</code> must be <code>\"aws:kms\"</code> or <code>\"aws:kms:dsse\"</code>. <code>aws_sse_bucket_key_enabled</code> Whether to use bucket keys for server-side encryption."},{"location":"guide/object_store/#s3-compatible-stores","title":"S3-compatible stores","text":"<p>Lance can also connect to S3-compatible stores, such as MinIO. To do so, you must specify both region and endpoint:</p> <pre><code>import lance\nds = lance.dataset(\n    \"s3://bucket/path\",\n    storage_options={\n        \"region\": \"us-east-1\",\n        \"endpoint\": \"http://minio:9000\",\n    }\n)\n</code></pre> <p>This can also be done with the <code>AWS_ENDPOINT</code> and <code>AWS_DEFAULT_REGION</code> environment variables.</p>"},{"location":"guide/object_store/#s3-express-directory-bucket","title":"S3 Express (Directory Bucket)","text":"<p>Lance supports S3 Express One Zone buckets, a.k.a. S3 directory buckets. S3 Express buckets only support connecting from an EC2 instance within the same region. By default, Lance automatically recognize the <code>--x-s3</code> suffix of an express bucket, there is no special configuration needed.</p> <p>In case of an access point or private link that hides the bucket name, you can configure express bucket access explicitly through storage option <code>s3_express</code>.</p> <pre><code>import lance\nds = lance.dataset(\n    \"s3://my-bucket--use1-az4--x-s3/path/imagenet.lance\",\n    storage_options={\n        \"region\": \"us-east-1\",\n        \"s3_express\": \"true\",\n    }\n)\n</code></pre>"},{"location":"guide/object_store/#google-cloud-storage-configuration","title":"Google Cloud Storage Configuration","text":"<p>GCS credentials are configured by setting the <code>GOOGLE_SERVICE_ACCOUNT</code> environment variable to the path of a JSON file containing the service account credentials. Alternatively, you can pass the path to the JSON file in the <code>storage_options</code></p> <pre><code>import lance\nds = lance.dataset(\n    \"gs://my-bucket/my-dataset\",\n    storage_options={\n        \"service_account\": \"path/to/service-account.json\",\n    }\n)\n</code></pre> <p>Note</p> <p>By default, GCS uses HTTP/1 for communication, as opposed to HTTP/2. This improves maximum throughput significantly. However, if you wish to use HTTP/2 for some reason, you can set the environment variable <code>HTTP1_ONLY</code> to <code>false</code>.</p> <p>The following keys can be used as both environment variables or keys in the <code>storage_options</code> parameter:</p> Key Description <code>google_service_account</code> / <code>service_account</code> Path to the service account JSON file. <code>google_service_account_key</code> / <code>service_account_key</code> The serialized service account key. <code>google_application_credentials</code> / <code>application_credentials</code> Path to the application credentials."},{"location":"guide/object_store/#azure-blob-storage-configuration","title":"Azure Blob Storage Configuration","text":"<p>Azure Blob Storage credentials can be configured by setting the <code>AZURE_STORAGE_ACCOUNT_NAME</code> and <code>AZURE_STORAGE_ACCOUNT_KEY</code> environment variables. Alternatively, you can pass the account name and key in the <code>storage_options</code> parameter:</p> <pre><code>import lance\nds = lance.dataset(\n    \"az://my-container/my-dataset\",\n    storage_options={\n        \"account_name\": \"some-account\",\n        \"account_key\": \"some-key\",\n    }\n)\n</code></pre> <p>These keys can be used as both environment variables or keys in the <code>storage_options</code> parameter:</p> Key Description <code>azure_storage_account_name</code> / <code>account_name</code> The name of the azure storage account. <code>azure_storage_account_key</code> / <code>account_key</code> The serialized service account key. <code>azure_client_id</code> / <code>client_id</code> Service principal client id for authorizing requests. <code>azure_client_secret</code> / <code>client_secret</code> Service principal client secret for authorizing requests. <code>azure_tenant_id</code> / <code>tenant_id</code> Tenant id used in oauth flows. <code>azure_storage_sas_key</code> / <code>azure_storage_sas_token</code> / <code>sas_key</code> / <code>sas_token</code> Shared access signature. The signature is expected to be percent-encoded, much like they are provided in the azure storage explorer or azure portal. <code>azure_storage_token</code> / <code>bearer_token</code> / <code>token</code> Bearer token. <code>azure_storage_use_emulator</code> / <code>object_store_use_emulator</code> / <code>use_emulator</code> Use object store with azurite storage emulator. <code>azure_endpoint</code> / <code>endpoint</code> Override the endpoint used to communicate with blob storage. <code>azure_use_fabric_endpoint</code> / <code>use_fabric_endpoint</code> Use object store with url scheme account.dfs.fabric.microsoft.com. <code>azure_msi_endpoint</code> / <code>azure_identity_endpoint</code> / <code>identity_endpoint</code> / <code>msi_endpoint</code> Endpoint to request a imds managed identity token. <code>azure_object_id</code> / <code>object_id</code> Object id for use with managed identity authentication. <code>azure_msi_resource_id</code> / <code>msi_resource_id</code> Msi resource id for use with managed identity authentication. <code>azure_federated_token_file</code> / <code>federated_token_file</code> File containing token for Azure AD workload identity federation. <code>azure_use_azure_cli</code> / <code>use_azure_cli</code> Use azure cli for acquiring access token. <code>azure_disable_tagging</code> / <code>disable_tagging</code> Disables tagging objects. This can be desirable if not supported by the backing store."},{"location":"guide/performance/","title":"Lance Performance Guide","text":"<p>This guide provides tips and tricks for optimizing the performance of your Lance applications.</p>"},{"location":"guide/performance/#logging","title":"Logging","text":"<p>Lance uses the <code>log</code> crate to log messages. Displaying these log messages will depend on the client library you are using. For rust, you will need to configure a logging subscriber. For more details ses the log docs. The Python and Java clients configure a default logging subscriber that logs to stderr.</p> <p>The Python/Java logger can be configured with several environment variables:</p> <ul> <li><code>LANCE_LOG</code>: Controls log filtering based on log level and target. See the env_logger docs for more details. The <code>LANCE_LOG</code> environment variable replaces the <code>RUST_LOG</code> environment variable.</li> <li><code>LANCE_LOG_STYLE</code>: Controls whether colors are used in the log messages. Valid values are <code>auto</code>, <code>always</code>, <code>never</code>.</li> <li><code>LANCE_LOG_TS_PRECISION</code>: The precision of the timestamp in the log messages. Valid values are <code>ns</code>, <code>us</code>, <code>ms</code>, <code>s</code>.</li> <li><code>LANCE_LOG_FILE</code>: Redirects Rust log messages to the specified file path instead of stderr. When set, Lance will create the file and any necessary parent directories. If the file cannot be created (e.g., due to permission issues), Lance will fall back to logging to stderr.</li> </ul>"},{"location":"guide/performance/#trace-events","title":"Trace Events","text":"<p>Lance uses tracing to log events. If you are running <code>pylance</code> then these events will be emitted to as log messages. For Rust connections you can use the <code>tracing</code> crate to capture these events.</p>"},{"location":"guide/performance/#file-audit","title":"File Audit","text":"<p>File audit events are emitted when significant files are created or deleted.</p> Event Parameter Description <code>lance::file_audit</code> <code>mode</code> The mode of I/O operation (create, delete, delete_unverified) <code>lance::file_audit</code> <code>type</code> The type of file affected (manifest, data file, index file, deletion file)"},{"location":"guide/performance/#io-events","title":"I/O Events","text":"<p>I/O events are emitted when significant I/O operations are performed, particularly those related to indices. These events are NOT emitted when the index is loaded from the in-memory cache. Correct cache utilization is important for performance and these events are intended to help you debug cache usage.</p> Event Parameter Description <code>lance::io_events</code> <code>type</code> The type of I/O operation (open_scalar_index, open_vector_index, load_vector_part, load_scalar_part)"},{"location":"guide/performance/#execution-events","title":"Execution Events","text":"<p>Execution events are emitted when an execution plan is run. These events are useful for debugging query performance.</p> Event Parameter Description <code>lance::execution</code> <code>type</code> The type of execution event (plan_run is the only type today) <code>lance::execution</code> <code>output_rows</code> The number of rows in the output of the plan <code>lance::execution</code> <code>iops</code> The number of I/O operations performed by the plan <code>lance::execution</code> <code>bytes_read</code> The number of bytes read by the plan <code>lance::execution</code> <code>indices_loaded</code> The number of indices loaded by the plan <code>lance::execution</code> <code>parts_loaded</code> The number of index partitions loaded by the plan <code>lance::execution</code> <code>index_comparisons</code> The number of comparisons performed inside the various indices"},{"location":"guide/performance/#threading-model","title":"Threading Model","text":"<p>Lance is designed to be thread-safe and performant. Lance APIs can be called concurrently unless explicitly stated otherwise. Users may create multiple tables and share tables between threads. Operations may run in parallel on the same table, but some operations may lead to conflicts. For details see conflict resolution.</p> <p>Most Lance operations will use multiple threads to perform work in parallel. There are two thread pools in lance: the IO thread pool and the compute thread pool. The IO thread pool is used for reading and writing data from disk. The compute thread pool is used for performing computations on data. The number of threads in each pool can be configured by the user.</p> <p>The IO thread pool is used for reading and writing data from disk. The number of threads in the IO thread pool is determined by the object store that the operation is working with. Local object stores will use 8 threads by default. Cloud object stores will use 64 threads by default. This is a fairly conservative default and you may need 128 or 256 threads to saturate network bandwidth on some cloud providers. The <code>LANCE_IO_THREADS</code> environment variable can be used to override the number of IO threads. If you increase this variable you may also want to increase the <code>io_buffer_size</code>.</p> <p>The compute thread pool is used for performing computations on data. The number of threads in the compute thread pool is determined by the number of cores on the machine. The number of threads in the compute thread pool can be overridden by setting the <code>LANCE_CPU_THREADS</code> environment variable. This is commonly done when running multiple Lance processes on the same machine (e.g when working with tools like Ray). Keep in mind that decoding data is a compute intensive operation, even if a workload seems I/O bound (like scanning a table) it may still need quite a few compute threads to achieve peak performance.</p>"},{"location":"guide/performance/#memory-requirements","title":"Memory Requirements","text":"<p>Lance is designed to be memory efficient. Operations should stream data from disk and not require loading the entire dataset into memory. However, there are a few components of Lance that can use a lot of memory.</p>"},{"location":"guide/performance/#metadata-cache","title":"Metadata Cache","text":"<p>Lance uses a metadata cache to speed up operations. This cache holds various pieces of metadata such as file metadata, dataset manifests, etc. This cache is an LRU cache that is sized by bytes. The default size is 1 GiB.</p> <p>The metadata cache is not shared between tables by default. For best performance you should create a single table and share it across your application. Alternatively, you can create a single session and specify it when you open tables.</p> <p>Keys are often a composite of multiple fields and all keys are scoped to the dataset URI. The following items are stored in the metadata cache:</p> Item Key What is stored Dataset Manifests Dataset URI, version, and etag The manifest for the dataset Transactions Dataset URI, version The transaction for the dataset Deletion Files Dataset URI, fragment_id, version, id, file_type The deletion vector for a frag Row Id Mask Dataset URI, version The row id sequence for the dataset Row Id Index Dataset URI, version The row id index for the dataset Row Id Sequence Dataset URI, fragment_id The row id sequence for a fragment Index Metadata Dataset URI, version The index metadata for the dataset Index Details\u00b9 Dataset URI, index uuid The index details for an index File Global Meta Dataset URI, file path The global metadata for a file File Column Meta Dataset URI, file path, column index The search cache for a column <p>Notes:</p> <ol> <li>This is only stored for very old indexes which don't store their details in the manifest.</li> </ol>"},{"location":"guide/performance/#index-cache","title":"Index Cache","text":"<p>Lance uses an index cache to speed up queries. This caches vector and scalar indices in memory. The max size of this cache can be configured when creating a <code>LanceDataset</code> using the <code>index_cache_size_bytes</code> parameter. This cache is an LRU cached that is sized by bytes. The default size is 6 GiB. You can view the size of this cache by inspecting the result of <code>dataset.session().size_bytes()</code>.</p> <p>The index cache is not shared between tables. For best performance you should create a single table and share it across your application.</p> <p>Note: <code>index_cache_size</code> (specified in entries) was deprecated since version 0.30.0. Use <code>index_cache_size_bytes</code> (specified in bytes) for new code.</p>"},{"location":"guide/performance/#scanning-data","title":"Scanning Data","text":"<p>Searches (e.g. vector search, full text search) do not use a lot of memory to hold data because they don't typically return a lot of data. However, scanning data can use a lot of memory. Scanning is a streaming operation but we need enough memory to hold the data that we are scanning. The amount of memory needed is largely determined by the <code>io_buffer_size</code> and the <code>batch_size</code> variables.</p> <p>Each I/O thread should have enough memory to buffer an entire page of data. Pages today are typically between 8 and 32 MB. This means, as a rule of thumb, you should generally have about 32MB of memory per I/O thread. The default <code>io_buffer_size</code> is 2GB which is enough to buffer 64 pages of data. If you increase the number of I/O threads you should also increase the <code>io_buffer_size</code>.</p> <p>Scans will also decode data (and run any filtering or compute) in parallel on CPU threads. The amount of data decoded at any one time is determined by the <code>batch_size</code> and the size of your rows. Each CPU thread will need enough memory to hold one batch. Once batches are delivered to your application, they are no longer tracked by Lance and so if memory is a concern then you should also be careful not to accumulate memory in your own application (e.g. by running <code>to_table</code> or otherwise collecting all batches in memory.)</p> <p>The default <code>batch_size</code> is 8192 rows. When you are working with mostly scalar data you want to keep batches around 1MB and so the amount of memory needed by the compute threads is fairly small. However, when working with large data you may need to turn down the <code>batch_size</code> to keep memory usage under control. For example, when working with 1024-dimensional vector embeddings (e.g. 32-bit floats) then 8192 rows would be 32MB of data. If you spread that across 16 CPU threads then you would need 512MB of compute memory per scan. You might find working with 1024 rows per batch is more appropriate.</p> <p>In summary, scans could use up to <code>(2 * io_buffer_size) + (batch_size * num_compute_threads)</code> bytes of memory. Keep in mind that <code>io_buffer_size</code> is a soft limit (e.g. we cannot read less than one page at a time right now) and so it is not necessarily a bug if you see memory usage exceed this limit by a small margin.</p> <p>The above limits refer to limits per-scan. There is an additional limit on the number of IOPS that is applied across the entire process. This limit is specified by the <code>LANCE_PROCESS_IO_THREADS_LIMIT</code> environment variable. The default is 128 which is more than enough for most workloads. You can increase this limit if you are working with a high-throughput workload. You can even disable this limit entirely by setting it to zero. Note that this can often lead to issues with excessive retries and timeouts from the object store.</p>"},{"location":"guide/read_and_write/","title":"Read and Write Data","text":""},{"location":"guide/read_and_write/#writing-lance-dataset","title":"Writing Lance Dataset","text":"<p>If you're familiar with Apache PyArrow, you'll find that creating a Lance dataset is straightforward. Begin by writing a <code>pyarrow.Table</code> using the <code>lance.write_dataset</code> function.</p> <pre><code>import lance\nimport pyarrow as pa\n\ntable = pa.Table.from_pylist([{\"name\": \"Alice\", \"age\": 20},\n                              {\"name\": \"Bob\", \"age\": 30}])\nds = lance.write_dataset(table, \"./alice_and_bob.lance\")\n</code></pre> <p>If the dataset is too large to fully load into memory, you can stream data using <code>lance.write_dataset</code> also supports <code>Iterator</code> of <code>pyarrow.RecordBatch</code> es. You will need to provide a <code>pyarrow.Schema</code> for the dataset in this case.</p> <pre><code>def producer() -&gt; Iterator[pa.RecordBatch]:\n    \"\"\"An iterator of RecordBatches.\"\"\"\n    yield pa.RecordBatch.from_pylist([{\"name\": \"Alice\", \"age\": 20}])\n    yield pa.RecordBatch.from_pylist([{\"name\": \"Bob\", \"age\": 30}])\n\nschema = pa.schema([\n    (\"name\", pa.string()),\n    (\"age\", pa.int32()),\n])\n\nds = lance.write_dataset(producer(),\n                         \"./alice_and_bob.lance\",\n                         schema=schema, mode=\"overwrite\")\nprint(ds.count_rows())  # Output: 2\n</code></pre> <p><code>lance.write_dataset</code> supports writing <code>pyarrow.Table</code>, <code>pandas.DataFrame</code>, <code>pyarrow.dataset.Dataset</code>, and <code>Iterator[pyarrow.RecordBatch]</code>.</p>"},{"location":"guide/read_and_write/#adding-rows","title":"Adding Rows","text":"<p>To insert data into your dataset, you can use either <code>LanceDataset.insert</code> or <code>lance.write_dataset</code> with <code>mode=append</code>.</p> <pre><code>import lance\nimport pyarrow as pa\n\ntable = pa.Table.from_pylist([{\"name\": \"Alice\", \"age\": 20},\n                              {\"name\": \"Bob\", \"age\": 30}])\nds = lance.write_dataset(table, \"./insert_example.lance\")\n\nnew_table = pa.Table.from_pylist([{\"name\": \"Carla\", \"age\": 37}])\nds.insert(new_table)\nprint(ds.to_table().to_pandas())\n#     name  age\n# 0  Alice   20\n# 1    Bob   30\n# 2  Carla   37\n\nnew_table2 = pa.Table.from_pylist([{\"name\": \"David\", \"age\": 42}])\nds = lance.write_dataset(new_table2, ds, mode=\"append\")\nprint(ds.to_table().to_pandas())\n#     name  age\n# 0  Alice   20\n# 1    Bob   30\n# 2  Carla   37\n# 3  David   42\n</code></pre>"},{"location":"guide/read_and_write/#deleting-rows","title":"Deleting rows","text":"<p>Lance supports deleting rows from a dataset using a SQL filter, as described in Filter push-down. For example, to delete Bob's row from the dataset above, one could use:</p> <pre><code>import lance\n\ndataset = lance.dataset(\"./alice_and_bob.lance\")\ndataset.delete(\"name = 'Bob'\")\ndataset2 = lance.dataset(\"./alice_and_bob.lance\")\nprint(dataset2.to_table().to_pandas())\n#     name  age\n# 0  Alice   20\n</code></pre> <p>Note</p> <p>Lance Format is immutable. Each write operation creates a new version of the dataset, so users must reopen the dataset to see the changes. Likewise, rows are removed by marking them as deleted in a separate deletion index, rather than rewriting the files. This approach is faster and avoids invalidating any indices that reference the files, ensuring that subsequent queries do not return the deleted rows.</p>"},{"location":"guide/read_and_write/#updating-rows","title":"Updating rows","text":"<p>Lance supports updating rows based on SQL expressions with the <code>lance.LanceDataset.update</code> method. For example, if we notice that Bob's name in our dataset has been sometimes written as <code>Blob</code>, we can fix that with:</p> <pre><code>import lance\n\ndataset = lance.dataset(\"./alice_and_bob.lance\")\ndataset.update({\"name\": \"'Bob'\"}, where=\"name = 'Blob'\")\n</code></pre> <p>The update values are SQL expressions, which is why <code>'Bob'</code> is wrapped in single quotes. This means we can use complex expressions that reference existing columns if we wish. For example, if two years have passed and we wish to update the ages of Alice and Bob in the same example, we could write:</p> <pre><code>import lance\n\ndataset = lance.dataset(\"./alice_and_bob.lance\")\ndataset.update({\"age\": \"age + 2\"})\n</code></pre> <p>If you are trying to update a set of individual rows with new values then it is often more efficient to use the merge insert operation described below.</p> <pre><code>import lance\n\n# Change the ages of both Alice and Bob\nnew_table = pa.Table.from_pylist([{\"name\": \"Alice\", \"age\": 30},\n                                  {\"name\": \"Bob\", \"age\": 20}])\n\n# This works, but is inefficient, see below for a better approach\ndataset = lance.dataset(\"./alice_and_bob.lance\")\nfor idx in range(new_table.num_rows):\n  name = new_table[0][idx].as_py()\n  new_age = new_table[1][idx].as_py()\n  dataset.update({\"age\": new_age}, where=f\"name='{name}'\")\n</code></pre>"},{"location":"guide/read_and_write/#merge-insert","title":"Merge Insert","text":"<p>Lance supports a merge insert operation. This can be used to add new data in bulk while also (potentially) matching against existing data. This operation can be used for a number of different use cases.</p>"},{"location":"guide/read_and_write/#bulk-update","title":"Bulk Update","text":"<p>The <code>lance.LanceDataset.update</code> method is useful for updating rows based on a filter. However, if we want to replace existing rows with new rows then a <code>lance.LanceDataset.merge_insert</code> operation would be more efficient:</p> <pre><code>import lance\n\ndataset = lance.dataset(\"./alice_and_bob.lance\")\nprint(dataset.to_table().to_pandas())\n#     name  age\n# 0  Alice   20\n# 1    Bob   30\n\n# Change the ages of both Alice and Bob\nnew_table = pa.Table.from_pylist([{\"name\": \"Alice\", \"age\": 2},\n                                  {\"name\": \"Bob\", \"age\": 3}])\n# This will use `name` as the key for matching rows.  Merge insert\n# uses a JOIN internally and so you typically want this column to\n# be a unique key or id of some kind.\nrst = dataset.merge_insert(\"name\") \\\n       .when_matched_update_all() \\\n       .execute(new_table)\nprint(dataset.to_table().to_pandas())\n#     name  age\n# 0  Alice    2\n# 1    Bob    3\n</code></pre> <p>Note that, similar to the update operation, rows that are modified will be removed and inserted back into the table, changing their position to the end. Also, the relative order of these rows could change because we are using a hash-join operation internally.</p>"},{"location":"guide/read_and_write/#insert-if-not-exists","title":"Insert if not Exists","text":"<p>Sometimes we only want to insert data if we haven't already inserted it before. This can happen, for example, when we have a batch of data but we don't know which rows we've added previously and we don't want to create duplicate rows. We can use the merge insert operation to achieve this:</p> <pre><code># Bob is already in the table, but Carla is new\nnew_table = pa.Table.from_pylist([{\"name\": \"Bob\", \"age\": 30},\n                                  {\"name\": \"Carla\", \"age\": 37}])\n\ndataset = lance.dataset(\"./alice_and_bob.lance\")\n\n# This will insert Carla but leave Bob unchanged\n_ = dataset.merge_insert(\"name\") \\\n       .when_not_matched_insert_all() \\\n       .execute(new_table)\n# Verify that Carla was added but Bob remains unchanged\nprint(dataset.to_table().to_pandas())\n#     name  age\n# 0  Alice   20\n# 1    Bob   30\n# 2  Carla   37\n</code></pre>"},{"location":"guide/read_and_write/#update-or-insert-upsert","title":"Update or Insert (Upsert)","text":"<p>Sometimes we want to combine both of the above behaviors. If a row already exists we want to update it. If the row does not exist we want to add it. This operation is sometimes called \"upsert\". We can use the merge insert operation to do this as well:</p> <pre><code>import lance\nimport pyarrow as pa\n\n# Change Carla's age and insert David\nnew_table = pa.Table.from_pylist([{\"name\": \"Carla\", \"age\": 27},\n                                  {\"name\": \"David\", \"age\": 42}])\n\ndataset = lance.dataset(\"./alice_and_bob.lance\")\n\n# This will update Carla and insert David\n_ = dataset.merge_insert(\"name\") \\\n       .when_matched_update_all() \\\n       .when_not_matched_insert_all() \\\n       .execute(new_table)\n# Verify the results\nprint(dataset.to_table().to_pandas())\n#     name  age\n# 0  Alice   20\n# 1    Bob   30\n# 2  Carla   27\n# 3  David   42\n</code></pre>"},{"location":"guide/read_and_write/#replace-a-portion-of-data","title":"Replace a Portion of Data","text":"<p>A less common, but still useful, behavior can be to replace some region of existing rows (defined by a filter) with new data. This is similar to performing both a delete and an insert in a single transaction. For example:</p> <pre><code>import lance\nimport pyarrow as pa\n\nnew_table = pa.Table.from_pylist([{\"name\": \"Edgar\", \"age\": 46},\n                                  {\"name\": \"Francene\", \"age\": 44}])\n\ndataset = lance.dataset(\"./alice_and_bob.lance\")\nprint(dataset.to_table().to_pandas())\n#       name  age\n# 0    Alice   20\n# 1      Bob   30\n# 2  Charlie   45\n# 3    Donna   50\n\n# This will remove anyone above 40 and insert our new data\n_ = dataset.merge_insert(\"name\") \\\n       .when_not_matched_insert_all() \\\n       .when_not_matched_by_source_delete(\"age &gt;= 40\") \\\n       .execute(new_table)\n# Verify the results - people over 40 replaced with new data\nprint(dataset.to_table().to_pandas())\n#        name  age\n# 0     Alice   20\n# 1       Bob   30\n# 2     Edgar   46\n# 3  Francene   44\n</code></pre>"},{"location":"guide/read_and_write/#reading-lance-dataset","title":"Reading Lance Dataset","text":"<p>To open a Lance dataset, use the <code>lance.dataset</code> function:</p> <pre><code>import lance\nds = lance.dataset(\"s3://bucket/path/imagenet.lance\")\n# Or local path\nds = lance.dataset(\"./imagenet.lance\")\n</code></pre> <p>Note</p> <p>Lance supports local file system, AWS <code>s3</code> and Google Cloud Storage(<code>gs</code>) as storage backends at the moment. Read more in Object Store Configuration.</p> <p>The most straightforward approach for reading a Lance dataset is to utilize the <code>lance.LanceDataset.to_table</code> method in order to load the entire dataset into memory.</p> <pre><code>table = ds.to_table()\n</code></pre> <p>Due to Lance being a high-performance columnar format, it enables efficient reading of subsets of the dataset by utilizing Column (projection) push-down and filter (predicates) push-downs.</p> <pre><code>table = ds.to_table(\n    columns=[\"image\", \"label\"],\n    filter=\"label = 2 AND text IS NOT NULL\",\n    limit=1000,\n    offset=3000)\n</code></pre> <p>Lance understands the cost of reading heavy columns such as <code>image</code>. Consequently, it employs an optimized query plan to execute the operation efficiently.</p>"},{"location":"guide/read_and_write/#iterative-read","title":"Iterative Read","text":"<p>If the dataset is too large to fit in memory, you can read it in batches using the <code>lance.LanceDataset.to_batches</code> method:</p> <pre><code>for batch in ds.to_batches(columns=[\"image\"], filter=\"label = 10\"):\n    # do something with batch\n    compute_on_batch(batch)\n</code></pre> <p>Unsurprisingly, <code>lance.LanceDataset.to_batches</code> takes the same parameters as <code>lance.LanceDataset.to_table</code> function.</p>"},{"location":"guide/read_and_write/#filter-push-down","title":"Filter push-down","text":"<p>Lance embraces the utilization of standard SQL expressions as predicates for dataset filtering. By pushing down the SQL predicates directly to the storage system, the overall I/O load during a scan is significantly reduced.</p> <p>Currently, Lance supports a growing list of expressions.</p> <ul> <li><code>&gt;</code>, <code>&gt;=</code>, <code>&lt;</code>, <code>&lt;=</code>, <code>=</code></li> <li><code>AND</code>, <code>OR</code>, <code>NOT</code></li> <li><code>IS NULL</code>, <code>IS NOT NULL</code></li> <li><code>IS TRUE</code>, <code>IS NOT TRUE</code>, <code>IS FALSE</code>, <code>IS NOT FALSE</code></li> <li><code>IN</code></li> <li><code>LIKE</code>, <code>NOT LIKE</code></li> <li><code>regexp_match(column, pattern)</code></li> <li><code>CAST</code></li> </ul> <p>For example, the following filter string is acceptable:</p> <pre><code>((label IN [10, 20]) AND (note['email'] IS NOT NULL))\n    OR NOT note['created']\n</code></pre> <p>Nested fields can be accessed using the subscripts. Struct fields can be subscripted using field names, while list fields can be subscripted using indices.</p> <p>If your column name contains special characters or is a SQL Keyword, you can use backtick (<code>`</code>) to escape it. For nested fields, each segment of the path must be wrapped in backticks.</p> <pre><code>`CUBE` = 10 AND `column name with space` IS NOT NULL\n  AND `nested with space`.`inner with space` &lt; 2\n</code></pre> <p>Warning</p> <p>Field names containing periods (<code>.</code>) are not supported.</p> <p>Literals for dates, timestamps, and decimals can be written by writing the string value after the type name. For example</p> <pre><code>date_col = date '2021-01-01'\nand timestamp_col = timestamp '2021-01-01 00:00:00'\nand decimal_col = decimal(8,3) '1.000'\n</code></pre> <p>For timestamp columns, the precision can be specified as a number in the type parameter. Microsecond precision (6) is the default.</p> SQL Time unit <code>timestamp(0)</code> Seconds <code>timestamp(3)</code> Milliseconds <code>timestamp(6)</code> Microseconds <code>timestamp(9)</code> Nanoseconds <p>Lance internally stores data in Arrow format. The mapping from SQL types to Arrow is:</p> SQL type Arrow type <code>boolean</code> <code>Boolean</code> <code>tinyint</code> / <code>tinyint unsigned</code> <code>Int8</code> / <code>UInt8</code> <code>smallint</code> / <code>smallint unsigned</code> <code>Int16</code> / <code>UInt16</code> <code>int</code> or <code>integer</code> / <code>int unsigned</code> or <code>integer unsigned</code> <code>Int32</code> / <code>UInt32</code> <code>bigint</code> / <code>bigint unsigned</code> <code>Int64</code> / <code>UInt64</code> <code>float</code> <code>Float32</code> <code>double</code> <code>Float64</code> <code>decimal(precision, scale)</code> <code>Decimal128</code> <code>date</code> <code>Date32</code> <code>timestamp</code> <code>Timestamp</code> (1) <code>string</code> <code>Utf8</code> <code>binary</code> <code>Binary</code> <p>(1) See precision mapping in previous table.</p>"},{"location":"guide/read_and_write/#random-read","title":"Random read","text":"<p>One distinct feature of Lance, as columnar format, is that it allows you to read random samples quickly.</p> <pre><code># Access the 2nd, 101th and 501th rows\ndata = ds.take([1, 100, 500], columns=[\"image\", \"label\"])\n</code></pre> <p>The ability to achieve fast random access to individual rows plays a crucial role in facilitating various workflows such as random sampling and shuffling in ML training. Additionally, it empowers users to construct secondary indices, enabling swift execution of queries for enhanced performance.</p>"},{"location":"guide/read_and_write/#table-maintenance","title":"Table Maintenance","text":"<p>Some operations over time will cause a Lance dataset to have a poor layout. For example, many small appends will lead to a large number of small fragments. Or deleting many rows will lead to slower queries due to the need to filter out deleted rows.</p> <p>To address this, Lance provides methods for optimizing dataset layout.</p>"},{"location":"guide/read_and_write/#compact-data-files","title":"Compact data files","text":"<p>Data files can be rewritten so there are fewer files. When passing a <code>target_rows_per_fragment</code> to <code>lance.dataset.DatasetOptimizer.compact_files</code>, Lance will skip any fragments that are already above that row count, and rewrite others. Fragments will be merged according to their fragment ids, so the inherent ordering of the data will be preserved.</p> <p>Note</p> <p>Compaction creates a new version of the table. It does not delete the old version of the table and the files referenced by it.</p> <pre><code>import lance\n\ndataset = lance.dataset(\"./alice_and_bob.lance\")\ndataset.optimize.compact_files(target_rows_per_fragment=1024 * 1024)\n</code></pre> <p>During compaction, Lance can also remove deleted rows. Rewritten fragments will not have deletion files. This can improve scan performance since the soft deleted rows don't have to be skipped during the scan.</p> <p>When files are rewritten, the original row addresses are invalidated. This means the affected files are no longer part of any ANN index if they were before. Because of this, it's recommended to rewrite files before re-building indices.</p>"},{"location":"guide/tags/","title":"Manage Tags","text":"<p>Lance, much like Git, employs the <code>LanceDataset.tags</code> property to label specific versions within a dataset's history.</p> <p><code>Tags</code> are particularly useful for tracking the evolution of datasets, especially in machine learning workflows where datasets are frequently updated. For example, you can <code>create</code>, <code>update</code>, and <code>delete</code> or <code>list</code> tags.</p> <p>Note</p> <p>Creating or deleting tags does not generate new dataset versions. Tags exist as auxiliary metadata stored in a separate directory.</p> <pre><code>import lance\nds = lance.dataset(\"./tags.lance\")\nprint(len(ds.versions()))\n# 2\nprint(ds.tags.list())\n# {}\nds.tags.create(\"v1-prod\", 1)\nprint(ds.tags.list())\n# {'v1-prod': {'version': 1, 'manifest_size': ...}}\nds.tags.update(\"v1-prod\", 2)\nprint(ds.tags.list())\n# {'v1-prod': {'version': 2, 'manifest_size': ...}}\nds.tags.delete(\"v1-prod\")\nprint(ds.tags.list())\n# {}\nprint(ds.tags.list_ordered())\n# []\nds.tags.create(\"v1-prod\", 1)\nprint(ds.tags.list_ordered())\n# [('v1-prod', {'version': 1, 'manifest_size': ...})]\nds.tags.update(\"v1-prod\", 2)\nprint(ds.tags.list_ordered())\n# [('v1-prod', {'version': 2, 'manifest_size': ...})]\nds.tags.delete(\"v1-prod\")\nprint(ds.tags.list_ordered())\n# []\n</code></pre> <p>Note</p> <p>Tagged versions are exempted from the <code>LanceDataset.cleanup_old_versions()</code> process.</p> <p>To remove a version that has been tagged, you must first <code>LanceDataset.tags.delete()</code> the associated tag. </p>"},{"location":"guide/tokenizer/","title":"Tokenizers","text":"<p>Currently, Lance has built-in support for Jieba and Lindera. However, it doesn't come with its own language models. If tokenization is needed, you can download language models by yourself. You can specify the location where the language models are stored by setting the environment variable LANCE_LANGUAGE_MODEL_HOME. If it's not set, the default value is</p> <pre><code>${system data directory}/lance/language_models\n</code></pre> <p>It also supports configuring user dictionaries, which makes it convenient for users to expand their own dictionaries without retraining the language models.</p>"},{"location":"guide/tokenizer/#language-models-of-jieba","title":"Language Models of Jieba","text":""},{"location":"guide/tokenizer/#downloading-the-model","title":"Downloading the Model","text":"<pre><code>python -m lance.download jieba\n</code></pre> <p>The language model is stored by default in <code>${LANCE_LANGUAGE_MODEL_HOME}/jieba/default</code>.</p>"},{"location":"guide/tokenizer/#using-the-model","title":"Using the Model","text":"<pre><code>ds.create_scalar_index(\"text\", \"INVERTED\", base_tokenizer=\"jieba/default\")\n</code></pre>"},{"location":"guide/tokenizer/#user-dictionaries","title":"User Dictionaries","text":"<p>Create a file named config.json in the root directory of the current model.</p> <pre><code>{\n    \"main\": \"dict.txt\",\n    \"users\": [\"path/to/user/dict.txt\"]\n}\n</code></pre> <ul> <li>The \"main\" field is optional. If not filled, the default is \"dict.txt\".</li> <li>\"users\" is the path of the user dictionary. For the format of the user dictionary, please refer to https://github.com/messense/jieba-rs/blob/main/src/data/dict.txt.</li> </ul>"},{"location":"guide/tokenizer/#language-models-of-lindera","title":"Language Models of Lindera","text":""},{"location":"guide/tokenizer/#downloading-the-model_1","title":"Downloading the Model","text":"<pre><code>python -m lance.download lindera -l [ipadic|ko-dic|unidic]\n</code></pre> <p>Note that the language models of Lindera need to be compiled. Please install lindera-cli first. For detailed steps, please refer to https://github.com/lindera/lindera/tree/main/lindera-cli.</p> <p>The language model is stored by default in ${LANCE_LANGUAGE_MODEL_HOME}/lindera/[ipadic|ko-dic|unidic]</p>"},{"location":"guide/tokenizer/#using-the-model_1","title":"Using the Model","text":"<pre><code>ds.create_scalar_index(\"text\", \"INVERTED\", base_tokenizer=\"lindera/ipadic\")\n</code></pre>"},{"location":"guide/tokenizer/#user-dictionaries_1","title":"User Dictionaries","text":"<p>Create a file named config.yml in the root directory of your model, or specify a custom YAML file using the <code>LINDERA_CONFIG_PATH</code> environment variable. If both are provided, the config.yml in the root directory will be used. For more detailed configuration methods, see the lindera documentation at https://github.com/lindera/lindera/.</p> <pre><code>segmenter:\n    mode: \"normal\"\n    dictionary:\n        # Note: in lance, the `kind` field is not supported. You need to specify the model path using the `path` field instead.\n        path: /path/to/lindera/ipadic/main\n</code></pre>"},{"location":"guide/tokenizer/#create-your-own-language-model","title":"Create your own language model","text":"<p>Put your language model into <code>LANCE_LANGUAGE_MODEL_HOME</code>. </p>"},{"location":"integrations/datafusion/","title":"Apache DataFusion Integration","text":"<p>Lance datasets can be queried with Apache Datafusion,  an extensible query engine written in Rust that uses Apache Arrow as its in-memory format.  This means you can write complex SQL queries to analyze your data in Lance.</p> <p>The integration allows users to pass down column selections and basic filters to Lance,  reducing the amount of scanned data when executing your query.  Additionally, the integration allows streaming data from Lance datasets, which allows users to do aggregation larger-than-memory.</p>"},{"location":"integrations/datafusion/#rust","title":"Rust","text":"<p>Lance includes a DataFusion table provider <code>lance::datafusion::LanceTableProvider</code>. Users can register a Lance dataset as a table in DataFusion and run SQL with it:</p>"},{"location":"integrations/datafusion/#simple-sql","title":"Simple SQL","text":"<pre><code>use datafusion::prelude::SessionContext;\nuse lance::datafusion::LanceTableProvider;\n\nlet ctx = SessionContext::new();\n\nctx.register_table(\"dataset\",\n    Arc::new(LanceTableProvider::new(\n    Arc::new(dataset.clone()),\n    /* with_row_id */ false,\n    /* with_row_addr */ false,\n    )))?;\n\nlet df = ctx.sql(\"SELECT * FROM dataset LIMIT 10\").await?;\nlet result = df.collect().await?;\n</code></pre>"},{"location":"integrations/datafusion/#join-2-tables","title":"Join 2 Tables","text":"<pre><code>use datafusion::prelude::SessionContext;\nuse lance::datafusion::LanceTableProvider;\n\nlet ctx = SessionContext::new();\n\nctx.register_table(\"orders\",\n    Arc::new(LanceTableProvider::new(\n    Arc::new(orders_dataset.clone()),\n    /* with_row_id */ false,\n    /* with_row_addr */ false,\n    )))?;\n\nctx.register_table(\"customers\",\n    Arc::new(LanceTableProvider::new(\n    Arc::new(customers_dataset.clone()),\n    /* with_row_id */ false,\n    /* with_row_addr */ false,\n    )))?;\n\nlet df = ctx.sql(\"\n    SELECT o.order_id, o.amount, c.customer_name \n    FROM orders o \n    JOIN customers c ON o.customer_id = c.customer_id\n    LIMIT 10\n\").await?;\n\nlet result = df.collect().await?;\n</code></pre>"},{"location":"integrations/datafusion/#register-udf","title":"Register UDF","text":"<p>Lance provides some built-in UDFs, which users can manually register and use in queries. The following example demonstrates how to register and use <code>contains_tokens</code>.</p> <pre><code>use datafusion::prelude::SessionContext;\nuse lance::datafusion::LanceTableProvider;\nuse lance_datafusion::udf::register_functions;\n\nlet ctx = SessionContext::new();\n\n// Register built-in UDFs\nregister_functions(&amp;ctx);\n\nctx.register_table(\"dataset\",\n    Arc::new(LanceTableProvider::new(\n    Arc::new(dataset.clone()),\n    /* with_row_id */ false,\n    /* with_row_addr */ false,\n    )))?;\n\nlet df = ctx.sql(\"SELECT * FROM dataset WHERE contains_tokens(text, 'cat')\").await?;\nlet result = df.collect().await?;\n</code></pre>"},{"location":"integrations/datafusion/#json-functions","title":"JSON Functions","text":"<p>Lance provides comprehensive JSON support through a set of built-in UDFs that are automatically registered when you use <code>register_functions()</code>. These functions enable you to query and filter JSON data efficiently.</p> <p>For a complete guide to JSON functions including: - <code>json_extract</code> - Extract values using JSONPath - <code>json_get</code>, <code>json_get_string</code>, <code>json_get_int</code>, <code>json_get_float</code>, <code>json_get_bool</code> - Type-safe value extraction - <code>json_exists</code> - Check if a path exists - <code>json_array_contains</code>, <code>json_array_length</code> - Array operations</p> <p>See the JSON Support Guide for detailed documentation and examples.</p> <p>Example: Querying JSON in SQL <pre><code>// After registering functions as shown above\nlet df = ctx.sql(\"\n    SELECT * FROM dataset \n    WHERE json_get_string(metadata, 'category') = 'electronics'\n    AND json_array_contains(metadata, '$.tags', 'featured')\n\").await?;\n</code></pre></p>"},{"location":"integrations/datafusion/#python","title":"Python","text":"<p>In Python, this integration is done via Datafusion FFI. An FFI table provider <code>FFILanceTableProvider</code> is included in <code>pylance</code>. For example, if I want to query <code>my_lance_dataset</code>:</p> <pre><code>from datafusion import SessionContext # pip install datafusion\nfrom lance import FFILanceTableProvider\n\nctx = SessionContext()\n\ntable1 = FFILanceTableProvider(\n    my_lance_dataset, with_row_id=True, with_row_addr=True\n)\nctx.register_table(\"table1\", table1)\nctx.table(\"table1\")\nctx.sql(\"SELECT * FROM table1 LIMIT 10\")\n</code></pre>"},{"location":"integrations/duckdb/","title":"DuckDB","text":"<p>In Python, Lance datasets can also be queried with DuckDB, an in-process SQL OLAP database. This means you can write complex SQL queries to analyze your data in Lance.</p> <p>This integration is done via DuckDB SQL on Apache Arrow, which provides zero-copy data sharing between LanceDB and DuckDB. DuckDB is capable of passing down column selections and basic filters to Lance, reducing the amount of data that needs to be scanned to perform your query. Finally, the integration allows streaming data from Lance tables, allowing you to aggregate tables that won't fit into memory. All of this uses the same mechanism described in DuckDB's blog post DuckDB quacks Arrow.</p> <p>A <code>LanceDataset</code> is accessible to DuckDB through the Arrow compatibility layer directly. To query the resulting Lance dataset in DuckDB, all you need to do is reference the dataset by the same name in your SQL query.</p> <pre><code>import duckdb # pip install duckdb\nimport lance\n\nds = lance.dataset(\"./my_lance_dataset.lance\")\n\nduckdb.query(\"SELECT * FROM ds\")\n# \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n# \u2502   vector    \u2502  item   \u2502 price  \u2502\n# \u2502   float[]   \u2502 varchar \u2502 double \u2502\n# \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n# \u2502 [3.1, 4.1]  \u2502 foo     \u2502   10.0 \u2502\n# \u2502 [5.9, 26.5] \u2502 bar     \u2502   20.0 \u2502\n# \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nduckdb.query(\"SELECT mean(price) FROM ds\")\n# \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n# \u2502 mean(price) \u2502\n# \u2502   double    \u2502\n# \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n# \u2502        15.0 \u2502\n# \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"integrations/huggingface/","title":"HuggingFace Integration","text":"<p>The HuggingFace Hub has become the go to place for ML practitioners to find pre-trained models and useful datasets.</p> <p>HuggingFace datasets can be written directly into Lance format by using the <code>lance.write_dataset</code> method. You can write the entire dataset or a particular split. For example:</p> <pre><code>import datasets # pip install datasets\nimport lance\n\nlance.write_dataset(datasets.load_dataset(\n    \"poloclub/diffusiondb\", split=\"train[:10]\",\n), \"diffusiondb_train.lance\")\n</code></pre>"},{"location":"integrations/pytorch/","title":"PyTorch Integration","text":"<p>Machine learning users can use <code>lance.torch.data.LanceDataset</code>, a subclass of <code>torch.utils.data.IterableDataset</code>, that to use Lance data directly PyTorch training and inference loops.</p> <p>It starts with creating a ML dataset for training. With the HuggingFace integration, it takes just one line of Python to convert a HuggingFace dataset to a Lance dataset.</p> <pre><code>import datasets # pip install datasets\nimport lance\n\nhf_ds = datasets.load_dataset(\n    \"poloclub/diffusiondb\",\n    split=\"train\",\n    # name=\"2m_first_1k\",  # for a smaller subset of the dataset\n)\nlance.write_dataset(hf_ds, \"diffusiondb_train.lance\")\n</code></pre> <p>Then, you can use the Lance dataset in PyTorch training and inference loops.</p> <p>Note:</p> <ol> <li> <p>the PyTorch dataset automatically convert data into <code>torch.Tensor</code></p> </li> <li> <p>lance is not fork-safe. If you are using multiprocessing, use spawn instead. The safe dataloader uses the spawning method.</p> </li> </ol>"},{"location":"integrations/pytorch/#unsafe-dataloader","title":"UnSafe Dataloader","text":"<pre><code>import torch\nimport lance.torch.data\n\n# Load lance dataset into a PyTorch IterableDataset.\n# with only columns \"image\" and \"prompt\".\ndataset = lance.torch.data.LanceDataset(\n    \"diffusiondb_train.lance\",\n    columns=[\"image\", \"prompt\"],\n    batch_size=128,\n    batch_readahead=8,  # Control multi-threading reads.\n)\n\n# Create a PyTorch DataLoader\ndataloader = torch.utils.data.DataLoader(dataset)\n\n# Inference loop\nfor batch in dataloader:\n    inputs, targets = batch[\"prompt\"], batch[\"image\"]\n    outputs = model(inputs)\n    ...\n</code></pre>"},{"location":"integrations/pytorch/#safe-dataloader","title":"Safe Dataloader","text":"<pre><code>from lance.torch.data import SafeLanceDataset, get_safe_loader\n\ndataset = SafeLanceDataset(temp_lance_dataset)\n# use spawn method to avoid fork-safe issue\nloader = get_safe_loader(\n    dataset,\n    num_workers=2,\n    batch_size=16,\n    drop_last=False,\n)\n\ntotal_samples = 0\nfor batch in loader:\n    total_samples += batch[\"id\"].shape[0]\n</code></pre> <p><code>lance.torch.data.LanceDataset</code> can composite with the <code>lance.sampler.Sampler</code> classes to control the sampling strategy. For example, you can use <code>lance.sampler.ShardedFragmentSampler</code> to use it in a distributed training environment. If not specified, it is a full scan.</p> <pre><code>from lance.sampler import ShardedFragmentSampler\nfrom lance.torch.data import LanceDataset\n\n# Load lance dataset into a PyTorch IterableDataset.\n# with only columns \"image\" and \"prompt\".\ndataset = LanceDataset(\n    \"diffusiondb_train.lance\",\n    columns=[\"image\", \"prompt\"],\n    batch_size=128,\n    batch_readahead=8,  # Control multi-threading reads.\n    sampler=ShardedFragmentSampler(\n        rank=1,  # Rank of the current process\n        world_size=8,  # Total number of processes\n    ),\n)\n</code></pre> <p>Available samplers:</p> <ul> <li><code>lance.sampler.ShardedFragmentSampler</code></li> <li><code>lance.sampler.ShardedBatchSampler</code></li> </ul> <p>Warning</p> <p>For multiprocessing you should probably not use fork as lance is multi-threaded internally and fork and multi-thread do not work well. Refer to this discussion. </p>"},{"location":"integrations/tensorflow/","title":"Tensorflow Integration","text":"<p>Lance can be used as a regular tf.data.Dataset in Tensorflow.</p> <p>Warning</p> <p>This feature is experimental and the APIs may change in the future.</p>"},{"location":"integrations/tensorflow/#reading-from-lance","title":"Reading from Lance","text":"<p>Using <code>lance.tf.data.from_lance</code>, you can create an <code>tf.data.Dataset</code> easily.</p> <pre><code>import tensorflow as tf\nimport lance\n\n# Create tf dataset\nds = lance.tf.data.from_lance(\"s3://my-bucket/my-dataset\")\n\n# Chain tf dataset with other tf primitives\n\nfor batch in ds.shuffling(32).map(lambda x: tf.io.decode_png(x[\"image\"])):\n    print(batch)\n</code></pre> <p>Backed by the Lance columnar format, using <code>lance.tf.data.from_lance</code> supports efficient column selection, filtering, and more.</p> <pre><code>ds = lance.tf.data.from_lance(\n    \"s3://my-bucket/my-dataset\",\n    columns=[\"image\", \"label\"],\n    filter=\"split = 'train' AND collected_time &gt; timestamp '2020-01-01'\",\n    batch_size=256)\n</code></pre> <p>By default, Lance will infer the Tensor spec from the projected columns. You can also specify <code>tf.TensorSpec</code> manually.</p> <pre><code>batch_size = 256\nds = lance.tf.data.from_lance(\n    \"s3://my-bucket/my-dataset\",\n    columns=[\"image\", \"labels\"],\n    batch_size=batch_size,\n    output_signature={\n        \"image\": tf.TensorSpec(shape=(), dtype=tf.string),\n        \"labels\": tf.RaggedTensorSpec(\n            dtype=tf.int32, shape=(batch_size, None), ragged_rank=1),\n    },\n</code></pre>"},{"location":"integrations/tensorflow/#distributed-training-and-shuffling","title":"Distributed Training and Shuffling","text":"<p>Since a Lance Dataset is a set of Fragments, we can distribute and shuffle Fragments to different workers.</p> <pre><code>import tensorflow as tf\nfrom lance.tf.data import from_lance, lance_fragments\n\nworld_size = 32\nrank = 10\nseed = 123  #\nepoch = 100\n\ndataset_uri = \"s3://my-bucket/my-dataset\"\n\n# Shuffle fragments distributedly.\nfragments =\n    lance_fragments(\"s3://my-bucket/my-dataset\")\n    .shuffling(32, seed=seed)\n    .repeat(epoch)\n    .enumerate()\n    .filter(lambda i, _: i % world_size == rank)\n    .map(lambda _, fid: fid)\n\nds = from_lance(\n    uri,\n    columns=[\"image\", \"label\"],\n    fragments=fragments,\n    batch_size=32\n    )\nfor batch in ds:\n    print(batch)\n</code></pre> <p>Warning</p> <p>For multiprocessing you should probably not use fork as lance is multi-threaded internally and fork and multi-thread do not work well. Refer to this discussion. </p>"},{"location":"integrations/ray/","title":"Lance-Ray Integration","text":"<p>Welcome to the Lance-Ray documentation!  Lance-Ray combines the distributed computing capabilities of Ray  with the efficient Lance storage format,  enabling scalable data processing workflows with optimal performance.</p>"},{"location":"integrations/ray/#features","title":"Features","text":"<ul> <li>Distributed Lance Operations: Leverage Ray's distributed computing for Lance dataset operations</li> <li>Seamless Data Conversion: Easy conversion between Ray datasets and Lance datasets</li> <li>Optimized I/O: Efficient reading and writing of Lance datasets with Ray integration</li> <li>Schema Validation: Automatic schema compatibility checking between Ray and Lance</li> <li>Flexible Filtering: Support for complex filtering pushdown on distributed Lance data</li> <li>Data Evolution: Support for data evolution to add new columns and distributedly backfill data using a Ray UDF</li> <li>Catalog Integration: Support for working with Lance datasets stored in various catalog services (e.g. Hive MetaStore, Iceberg REST Catalog, Unity, Gravitino, AWS Glue, etc.)</li> </ul>"},{"location":"integrations/ray/#quickstart","title":"Quickstart","text":""},{"location":"integrations/ray/#installation","title":"Installation","text":"<pre><code>pip install lance-ray\n</code></pre>"},{"location":"integrations/ray/#simple-example","title":"Simple Example","text":"<pre><code>import ray\nfrom lance_ray import read_lance, write_lance\n\n# Initialize Ray\nray.init()\n\n# Create a Ray dataset\ndata = ray.data.range(1000).map(lambda row: {\"id\": row[\"id\"], \"value\": row[\"id\"] * 2})\n\n# Write to Lance format\nwrite_lance(data, \"my_dataset.lance\")\n\n# Read Lance dataset back as Ray dataset\nray_dataset = read_lance(\"my_dataset.lance\")\n\n# Perform distributed operations\nresult = ray_dataset.filter(lambda row: row[\"value\"] &gt; 100).count()\nprint(f\"Filtered count: {result}\")\n</code></pre>"},{"location":"integrations/ray/data-evolution/","title":"Data Evolution","text":""},{"location":"integrations/ray/data-evolution/#add_columns","title":"<code>add_columns</code>","text":"<pre><code>add_columns(\n    uri=None, \n    *, \n    namespace=None, \n    table_id=None, \n    transform, \n    **kwargs)\n</code></pre> <p>Add columns to an existing Lance dataset using Ray's distributed processing.</p> <p>Parameters:</p> <ul> <li><code>uri</code>: Path to the Lance dataset (either uri OR namespace+table_id required)</li> <li><code>namespace</code>: LanceNamespace instance for metadata catalog integration (requires table_id)</li> <li><code>table_id</code>: Table identifier as list of strings (requires namespace)</li> <li><code>transform</code>: Transform function to apply for adding columns</li> <li><code>filter</code>: Optional filter expression to apply</li> <li><code>read_columns</code>: Optional list of columns to read from original dataset</li> <li><code>reader_schema</code>: Optional schema for the reader</li> <li><code>read_version</code>: Optional version to read</li> <li><code>ray_remote_args</code>: Optional kwargs for Ray remote tasks</li> <li><code>storage_options</code>: Optional storage configuration dictionary</li> <li><code>batch_size</code>: Batch size for processing (default: 1024)</li> <li><code>concurrency</code>: Optional number of concurrent processes</li> </ul> <p>Returns: None</p>"},{"location":"integrations/ray/distributed-indexing/","title":"Distributed Index Building","text":"<p>Lance-Ray provides distributed index building functionality that leverages Ray's distributed computing capabilities to efficiently create text indices for Lance datasets. This is particularly useful for large-scale datasets as it can distribute index building work across multiple Ray worker nodes.</p>"},{"location":"integrations/ray/distributed-indexing/#new-distributed-apis","title":"New Distributed APIs","text":"<p><code>create_scalar_index()</code> - Distributedly create scalar index index using ray. Currently only Inverted/FTS/BTREE are supported. Will add more index type support in the future.</p>"},{"location":"integrations/ray/distributed-indexing/#how-it-works","title":"How It Works","text":"<p>The <code>create_scalar_index</code> function allows you to create full-text search indices for Lance datasets using the Ray distributed computing framework. This function distributes the index building process across multiple Ray worker nodes, with each node responsible for building indices for a subset of dataset fragments. These indices are then merged and committed as a single index.</p> <p>Backward Compatibility:    - Automatically detect availability of new APIs across different Lance versions    - Gracefully fallback to raise tips when new APIs are unavailable</p> <p><code>create_scalar_index</code></p> <pre><code>def create_scalar_index(\n    dataset: Union[str, \"lance.LanceDataset\"],\n    column: str,\n    index_type: Union[\n        Literal[\"BTREE\"],\n        Literal[\"BITMAP\"],\n        Literal[\"LABEL_LIST\"],\n        Literal[\"INVERTED\"],\n        Literal[\"FTS\"],\n        Literal[\"NGRAM\"],\n        Literal[\"ZONEMAP\"],\n        IndexConfig,\n    ],\n    name: Optional[str] = None,\n    *,\n    replace: bool = True,\n    train: bool = True,\n    fragment_ids: Optional[list[int]] = None,\n    fragment_uuid: Optional[str] = None,\n    num_workers: int = 4,\n    storage_options: Optional[dict[str, str]] = None,\n    ray_remote_args: Optional[dict[str, Any]] = None,\n    **kwargs: Any,\n) -&gt; \"lance.LanceDataset\":\n</code></pre>"},{"location":"integrations/ray/distributed-indexing/#parameters","title":"Parameters","text":"Parameter Type Description <code>dataset</code> <code>str</code> or <code>lance.LanceDataset</code> Lance dataset or its URI <code>column</code> <code>str</code> Column name to index <code>index_type</code> <code>str</code> or <code>IndexConfig</code> Index type, can be <code>\"INVERTED\"</code>, <code>\"FTS\"</code>, <code>\"BTREE\"</code>, <code>\"BITMAP\"</code>, <code>\"LABEL_LIST\"</code>, <code>\"NGRAM\"</code>, <code>\"ZONEMAP\"</code>, or <code>IndexConfig</code> object <code>name</code> <code>str</code>, optional Index name, auto-generated if not provided <code>replace</code> <code>bool</code>, optional Whether to replace existing index with the same name, default is <code>True</code> <code>train</code> <code>bool</code>, optional Whether to train the index, default is <code>True</code> <code>fragment_ids</code> <code>list[int]</code>, optional Optional list of fragment IDs to build index on <code>fragment_uuid</code> <code>str</code>, optional Optional fragment UUID for distributed indexing <code>num_workers</code> <code>int</code>, optional Number of Ray worker nodes to use, default is 4 <code>storage_options</code> <code>Dict[str, str]</code>, optional Storage options for the dataset <code>ray_remote_args</code> <code>Dict[str, Any]</code>, optional Ray task options (e.g., <code>num_cpus</code>, <code>resources</code>) <code>**kwargs</code> <code>Any</code> Additional arguments passed to <code>create_scalar_index</code> <p>Note: For distributed indexing, currently only <code>\"INVERTED\"</code>,<code>\"FTS\"</code> and <code>\"BTREE\"</code> index types are supported.</p>"},{"location":"integrations/ray/distributed-indexing/#return-value","title":"Return Value","text":"<p>The function returns an updated Lance dataset with the newly created index.</p>"},{"location":"integrations/ray/distributed-indexing/#examples","title":"Examples","text":""},{"location":"integrations/ray/distributed-indexing/#fts-index","title":"FTS Index","text":"<pre><code>import lance\nimport lance_ray as lr\n\n# Create or load Lance dataset\ndataset = lance.dataset(\"path/to/dataset\")\n\n# Build distributed index\nupdated_dataset = lr.create_scalar_index(\n   dataset=dataset,\n   column=\"text\",\n   index_type=\"INVERTED\",\n   num_workers=4\n)\n\n# Verify index creation\nindices = updated_dataset.list_indices()\nprint(f\"Index list: {indices}\")\n\n# Use index for search\nresults = updated_dataset.scanner(\n   full_text_query=\"search term\",\n   columns=[\"id\", \"text\"]\n).to_table()\nprint(f\"Search results: {results}\")\n</code></pre>"},{"location":"integrations/ray/distributed-indexing/#btree-index","title":"BTREE Index","text":"<pre><code># Assume a LanceDataset with a numeric column \"id\" exists at this path\nimport lance_ray as lr\n\nupdated_dataset = lr.create_scalar_index(\n    dataset=\"path/to/dataset\",\n    column=\"id\",\n    index_type=\"BTREE\",\n    name=\"btree_multiple_fragment_idx\",\n    replace=False,\n    num_workers=4,\n)\n\n# Example queries\nupdated_dataset.scanner(filter=\"id = 100\", columns=[\"id\", \"text\"]).to_table()\nupdated_dataset.scanner(filter=\"id &gt;= 200 AND id &lt; 800\", columns=[\"id\", \"text\"]).to_table()\n</code></pre>"},{"location":"integrations/ray/distributed-indexing/#custom-index-name","title":"Custom Index Name","text":"<pre><code>updated_dataset = lr.create_scalar_index(\n   dataset=\"path/to/dataset\",\n   column=\"text\",\n   index_type=\"INVERTED\",\n   name=\"custom_text_index\",\n   num_workers=4\n)\n</code></pre>"},{"location":"integrations/ray/distributed-indexing/#custom-ray-options","title":"Custom Ray Options","text":"<pre><code>updated_dataset = lr.create_scalar_index(\n   dataset=\"path/to/dataset\",\n   column=\"text\",\n   index_type=\"INVERTED\",\n   num_workers=4,\n   ray_remote_args={\"num_cpus\": 2, \"resources\": {\"custom_resource\": 1}}\n)\n</code></pre>"},{"location":"integrations/ray/distributed-indexing/#index-replacement-control","title":"Index Replacement Control","text":"<pre><code># Create index with custom name\nupdated_dataset = lr.create_scalar_index(\n   dataset=\"path/to/dataset\",\n   column=\"text\",\n   index_type=\"INVERTED\",\n   name=\"my_text_index\",\n   num_workers=4\n)\n\n# Try to create another index with the same name (will replace by default)\nupdated_dataset = lr.create_scalar_index(\n   dataset=\"path/to/dataset\",\n   column=\"text\",\n   index_type=\"INVERTED\",\n   name=\"my_text_index\",  # Same name as before\n   replace=True,          # Explicitly allow replacement (default behavior)\n   num_workers=4\n)\n\n# Prevent index replacement\nimport lance_ray as lr\n\ntry:\n    updated_dataset = lr.create_scalar_index(\n       dataset=\"path/to/dataset\",\n       column=\"text\",\n       index_type=\"INVERTED\",\n       name=\"my_text_index\",  # Same name as existing index\n       replace=False,         # Prevent replacement\n       num_workers=4\n    )\nexcept ValueError as e:\n    print(f\"Index creation failed: {e}\")\n    # Handle the error appropriately\n</code></pre>"},{"location":"integrations/ray/distributed-indexing/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>For very large datasets, it's recommended to use more powerful CPU/memory ray worker nodes. Increasing <code>num_workers</code> can improve index building speed, but requires more computational nodes.</li> <li>Too many num_workers can cause large number of partitions, which cause FTS queries slowness as lots of index partitions need to be loaded when searching.</li> <li>If <code>num_workers</code> is greater than the number of fragments, it will be automatically adjusted to match the fragment count</li> </ul>"},{"location":"integrations/ray/distributed-indexing/#important-notes","title":"Important Notes","text":"<ul> <li>Index Type Support: For distributed indexing, currently only <code>\"INVERTED\"</code>/<code>\"FTS\"</code>/<code>\"BTREE\"</code> index types are supported, even though the function signature accepts other index types.</li> <li>Default Behavior: The <code>replace</code> parameter defaults to <code>True</code>, meaning existing indices with the same name will be replaced without warning. Set <code>replace=False</code> to prevent accidental overwrites.</li> <li>Fragment Selection: Use <code>fragment_ids</code> parameter to build indices on specific fragments only. This is useful for incremental index building or testing.</li> <li>Error Handling: When <code>replace=False</code> and an index with the same name exists, a <code>ValueError</code> or <code>RuntimeError</code> will be raised depending on the execution context.</li> </ul>"},{"location":"integrations/ray/examples/","title":"Examples","text":"<p>Here are some examples to try out. See the <code>examples/</code> directory for more comprehensive usage examples.</p>"},{"location":"integrations/ray/examples/#basic-read-write","title":"Basic Read &amp; Write","text":"<pre><code>import pandas as pd\nimport ray\nfrom lance_ray import read_lance, write_lance\n\n# Initialize Ray\nray.init()\n\n# Create sample data\nsample_data = {\n    \"user_id\": range(100),\n    \"name\": [f\"User_{i}\" for i in range(100)],\n    \"age\": [20 + (i % 50) for i in range(100)],\n    \"score\": [50.0 + (i % 100) * 0.5 for i in range(100)],\n}\ndf = pd.DataFrame(sample_data)\n\n# Create Ray dataset\nds = ray.data.from_pandas(df)\n\n# Write to Lance format\nwrite_lance(ds, \"sample_dataset.lance\")\n\n# Read Lance dataset back\nds = read_lance(\"sample_dataset.lance\")\n\n# Perform distributed operations\nfiltered_ds = ds.filter(lambda row: row[\"age\"] &gt; 30)\nprint(f\"Filtered count: {filtered_ds.count()}\")\n\n# Read with column selection and filtering\nds_filtered = read_lance(\n    \"sample_dataset.lance\",\n    columns=[\"user_id\", \"name\", \"score\"],\n    filter=\"score &gt; 75.0\"\n)\nprint(f\"Schema: {ds_filtered.schema()}\")\n</code></pre>"},{"location":"integrations/ray/examples/#data-evolution","title":"Data Evolution","text":"<pre><code># Add columns using metadata catalog\nfrom lance_ray import add_columns\nimport pyarrow as pa\n\ndef add_computed_column(batch: pa.RecordBatch) -&gt; pa.RecordBatch:\n    df = batch.to_pandas()\n    df['computed'] = df['value'] * 2 + df['id']\n    return pa.RecordBatch.from_pandas(df[[\"computed\"]])\n\nadd_columns(\n    uri=\"sample_dataset.lance\",\n    transform=add_computed_column,\n    concurrency=4\n)\n</code></pre>"},{"location":"integrations/ray/examples/#using-namespace","title":"Using Namespace","text":"<p>For enterprise environments with metadata catalogs, you can use Lance Namespace integration:</p> <pre><code>import ray\nimport lance_namespace as ln\nfrom lance_ray import read_lance, write_lance\n\n# Initialize Ray\nray.init()\n\n# Connect to a metadata catalog (directory-based example)\nnamespace = ln.connect(\"dir\", {\"root\": \"/path/to/tables\"})\n\n# Create a Ray dataset\ndata = ray.data.range(1000).map(lambda row: {\"id\": row[\"id\"], \"value\": row[\"id\"] * 2})\n\n# Write to Lance format using metadata catalog\nwrite_lance(data, namespace=namespace, table_id=[\"my_table\"])\n\n# Read Lance dataset back using metadata catalog\nray_dataset = read_lance(namespace=namespace, table_id=[\"my_table\"])\n\n# Perform distributed operations\nresult = ray_dataset.filter(lambda row: row[\"value\"] &gt; 100).count()\nprint(f\"Filtered count: {result}\")\n</code></pre> <p>The package dependency comes with the directory and REST namespace implementations to use by default. To use another implementation, install the specific extra dependency.  For example to use it with AWS Glue catalog:</p> <pre><code>pip install lance-namespace[glue]\n</code></pre> <p>And then you can do:</p> <pre><code>import ray\nimport lance_namespace as ln\nfrom lance_ray import read_lance, write_lance\n\n# Initialize Ray\nray.init()\n\n# Connect to AWS Glue catalog \n# using the default account and region in the current AWS environment\nnamespace = ln.connect(\"glue\", {})\n\n# Create a Ray dataset\ndata = ray.data.range(1000).map(lambda row: {\"id\": row[\"id\"], \"value\": row[\"id\"] * 2})\n\n# Write to Lance format using metadata catalog\nwrite_lance(\n    data, \n    uri=\"s3://my-bucket/my-table\", \n    namespace=namespace, \n    table_id=[\"default\", \"my_table\"]\n)\n\n# Read Lance dataset back using metadata catalog\nray_dataset = read_lance(namespace=namespace, table_id=[\"default\", \"my_table\"])\n\n# Perform distributed operations\nresult = ray_dataset.filter(lambda row: row[\"value\"] &gt; 100).count()\nprint(f\"Filtered count: {result}\")\n</code></pre>"},{"location":"integrations/ray/read/","title":"Reading Lance Datasets","text":""},{"location":"integrations/ray/read/#read_lance","title":"<code>read_lance</code>","text":"<pre><code>read_lance(\n    uri=None, \n    *, \n    namespace=None, \n    table_id=None, \n    columns=None, \n    filter=None, \n    storage_options=None, \n    **kwargs)\n</code></pre> <p>Read a Lance dataset and return a Ray Dataset.</p> <p>Parameters:</p> <ul> <li><code>uri</code>: The URI of the Lance dataset to read from (either uri OR namespace+table_id required)</li> <li><code>namespace</code>: LanceNamespace instance for metadata catalog integration (requires table_id)</li> <li><code>table_id</code>: Table identifier as list of strings (requires namespace)</li> <li><code>columns</code>: Optional list of column names to read</li> <li><code>filter</code>: Optional filter expression to apply</li> <li><code>storage_options</code>: Optional storage configuration dictionary</li> <li><code>scanner_options</code>: Optional scanner configuration dictionary</li> <li><code>ray_remote_args</code>: Optional kwargs for Ray remote tasks</li> <li><code>concurrency</code>: Optional maximum number of concurrent Ray tasks</li> <li><code>override_num_blocks</code>: Optional override for number of output blocks</li> </ul> <p>Returns: Ray Dataset</p>"},{"location":"integrations/ray/write/","title":"Writing to Lance Dataset","text":""},{"location":"integrations/ray/write/#write_lance","title":"<code>write_lance</code>","text":"<pre><code>write_lance(\n    ds, \n    uri=None, \n    *, \n    namespace=None, \n    table_id=None, \n    schema=None, \n    mode=\"create\", \n    **kwargs)\n</code></pre> <p>Write a Ray Dataset to Lance format.</p> <p>Parameters:</p> <ul> <li><code>ds</code>: Ray Dataset to write</li> <li><code>uri</code>: Path to the destination Lance dataset (either uri OR namespace+table_id required)</li> <li><code>namespace</code>: LanceNamespace instance for metadata catalog integration (requires table_id)</li> <li><code>table_id</code>: Table identifier as list of strings (requires namespace)</li> <li><code>schema</code>: Optional PyArrow schema</li> <li><code>mode</code>: Write mode - \"create\", \"append\", or \"overwrite\"</li> <li><code>min_rows_per_file</code>: Minimum rows per file (default: 1024 * 1024)</li> <li><code>max_rows_per_file</code>: Maximum rows per file (default: 64 * 1024 * 1024)</li> <li><code>data_storage_version</code>: Optional data storage version</li> <li><code>storage_options</code>: Optional storage configuration dictionary</li> <li><code>ray_remote_args</code>: Optional kwargs for Ray remote tasks</li> <li><code>concurrency</code>: Optional maximum number of concurrent Ray tasks</li> </ul> <p>Returns: None</p>"},{"location":"integrations/spark/","title":"Spark Lance Connector","text":""},{"location":"integrations/spark/#introduction","title":"Introduction","text":"<p>The Apache Spark Connector for Lance allows Apache Spark to efficiently read datasets stored in Lance format.</p> <p>By using the Apache Spark Connector for Lance, you can leverage Apache Spark's powerful data processing, SQL querying, and machine learning training capabilities on the AI data lake powered by Lance.</p>"},{"location":"integrations/spark/#features","title":"Features","text":"<p>The connector is built using the Spark DatasourceV2 (DSv2) API. Please check this presentation to learn more about DSv2 features. Specifically, you can use the Apache Spark Connector for Lance to:</p> <ul> <li>Read &amp; Write Lance Datasets: Seamlessly read and write datasets stored in the Lance format using Spark.</li> <li>Distributed, Parallel Scans: Leverage Spark's distributed computing capabilities to perform parallel scans on Lance datasets.</li> <li>Column and Filter Pushdown: Optimize query performance by pushing down column selections and filters to the data source.</li> </ul>"},{"location":"integrations/spark/#quick-start","title":"Quick Start","text":"<p>The project contains a docker image in the <code>docker</code> folder you can build and run a simple example notebook. To do so, clone the repo and run:</p> <pre><code>make docker-build\nmake docker-up\n</code></pre> <p>And then open the notebook at <code>http://localhost:8888</code>.</p>"},{"location":"integrations/spark/config/","title":"Configuration","text":"<p>Spark DSV2 catalog integrates with Lance through Lance Namespace.</p>"},{"location":"integrations/spark/config/#basic-setup","title":"Basic Setup","text":"<p>Configure Spark with the <code>LanceNamespaceSparkCatalog</code> by setting the appropriate Spark catalog implementation  and namespace-specific options:</p> Parameter Type Required Description <code>spark.sql.catalog.{name}</code> String \u2713 Set to <code>com.lancedb.lance.spark.LanceNamespaceSparkCatalog</code> <code>spark.sql.catalog.{name}.impl</code> String \u2713 Namespace implementation, short name like <code>dir</code>, <code>rest</code>, <code>hive3</code>, <code>glue</code> or full Java implementation class"},{"location":"integrations/spark/config/#example-namespace-implementations","title":"Example Namespace Implementations","text":""},{"location":"integrations/spark/config/#directory-namespace","title":"Directory Namespace","text":"ScalaJavaSpark ShellSpark Submit <pre><code>import org.apache.spark.sql.SparkSession\n\nval spark = SparkSession.builder()\n    .appName(\"lance-dir-example\")\n    .config(\"spark.sql.catalog.lance\", \"com.lancedb.lance.spark.LanceNamespaceSparkCatalog\")\n    .config(\"spark.sql.catalog.lance.impl\", \"dir\")\n    .config(\"spark.sql.catalog.lance.root\", \"/path/to/lance/database\")\n    .getOrCreate()\n</code></pre> <pre><code>import org.apache.spark.sql.SparkSession;\n\nSparkSession spark = SparkSession.builder()\n    .appName(\"lance-dir-example\")\n    .config(\"spark.sql.catalog.lance\", \"com.lancedb.lance.spark.LanceNamespaceSparkCatalog\")\n    .config(\"spark.sql.catalog.lance.impl\", \"dir\")\n    .config(\"spark.sql.catalog.lance.root\", \"/path/to/lance/database\")\n    .getOrCreate();\n</code></pre> <pre><code>spark-shell \\\n  --packages com.lancedb:lance-spark-bundle-3.5_2.12:0.0.7 \\\n  --conf spark.sql.catalog.lance=com.lancedb.lance.spark.LanceNamespaceSparkCatalog \\\n  --conf spark.sql.catalog.lance.impl=dir \\\n  --conf spark.sql.catalog.lance.root=/path/to/lance/database\n</code></pre> <pre><code>spark-submit \\\n  --packages com.lancedb:lance-spark-bundle-3.5_2.12:0.0.7 \\\n  --conf spark.sql.catalog.lance=com.lancedb.lance.spark.LanceNamespaceSparkCatalog \\\n  --conf spark.sql.catalog.lance.impl=dir \\\n  --conf spark.sql.catalog.lance.root=/path/to/lance/database \\\n  your-application.jar\n</code></pre>"},{"location":"integrations/spark/config/#directory-configuration-parameters","title":"Directory Configuration Parameters","text":"Parameter Required Description <code>spark.sql.catalog.{name}.root</code> \u2717 Storage root location (default: current directory) <code>spark.sql.catalog.{name}.storage.*</code> \u2717 Additional OpenDAL storage configuration options <code>spark.sql.catalog.{name}.extra_level</code> \u2717 Virtual level for 2-level namespaces (auto-set to <code>default</code>). See Note on Namespace Levels for more details. <p>Example settings:</p> Local StorageS3MinIO <pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName(\"lance-dir-local-example\") \\\n    .config(\"spark.sql.catalog.lance\", \"com.lancedb.lance.spark.LanceNamespaceSparkCatalog\") \\\n    .config(\"spark.sql.catalog.lance.impl\", \"dir\") \\\n    .config(\"spark.sql.catalog.lance.root\", \"/path/to/lance/database\") \\\n    .getOrCreate()\n</code></pre> <pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName(\"lance-dir-minio-example\") \\\n    .config(\"spark.sql.catalog.lance\", \"com.lancedb.lance.spark.LanceNamespaceSparkCatalog\") \\\n    .config(\"spark.sql.catalog.lance.impl\", \"dir\") \\\n    .config(\"spark.sql.catalog.lance.root\", \"s3://bucket-name/lance-data\") \\\n    .config(\"spark.sql.catalog.lance.storage.access_key_id\", \"abc\") \\\n    .config(\"spark.sql.catalog.lance.storage.secret_access_key\", \"def\")\n    .config(\"spark.sql.catalog.lance.storage.session_token\", \"ghi\") \\\n    .getOrCreate()\n</code></pre> <pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName(\"lance-dir-minio-example\") \\\n    .config(\"spark.sql.catalog.lance\", \"com.lancedb.lance.spark.LanceNamespaceSparkCatalog\") \\\n    .config(\"spark.sql.catalog.lance.impl\", \"dir\") \\\n    .config(\"spark.sql.catalog.lance.root\", \"s3://bucket-name/lance-data\") \\\n    .config(\"spark.sql.catalog.lance.storage.endpoint\", \"http://minio:9000\") \\\n    .config(\"spark.sql.catalog.lance.storage.aws_allow_http\", \"true\") \\\n    .config(\"spark.sql.catalog.lance.storage.access_key_id\", \"admin\") \\\n    .config(\"spark.sql.catalog.lance.storage.secret_access_key\", \"password\") \\\n    .getOrCreate()\n</code></pre>"},{"location":"integrations/spark/config/#rest-namespace","title":"REST Namespace","text":"<p>Here we use LanceDB Cloud as an example of the REST namespace:</p> PySparkScalaJavaSpark ShellSpark Submit <pre><code>spark = SparkSession.builder \\\n    .appName(\"lance-rest-example\") \\\n    .config(\"spark.sql.catalog.lance\", \"com.lancedb.lance.spark.LanceNamespaceSparkCatalog\") \\\n    .config(\"spark.sql.catalog.lance.impl\", \"rest\") \\\n    .config(\"spark.sql.catalog.lance.headers.x-api-key\", \"your-api-key\") \\\n    .config(\"spark.sql.catalog.lance.headers.x-lancedb-database\", \"your-database\") \\\n    .config(\"spark.sql.catalog.lance.uri\", \"https://your-database.us-east-1.api.lancedb.com\") \\\n    .getOrCreate()\n</code></pre> <pre><code>val spark = SparkSession.builder()\n    .appName(\"lance-rest-example\")\n    .config(\"spark.sql.catalog.lance\", \"com.lancedb.lance.spark.LanceNamespaceSparkCatalog\")\n    .config(\"spark.sql.catalog.lance.impl\", \"rest\")\n    .config(\"spark.sql.catalog.lance.headers.x-api-key\", \"your-api-key\")\n    .config(\"spark.sql.catalog.lance.headers.x-lancedb-database\", \"your-database\")\n    .config(\"spark.sql.catalog.lance.uri\", \"https://your-database.us-east-1.api.lancedb.com\")\n    .getOrCreate()\n</code></pre> <pre><code>SparkSession spark = SparkSession.builder()\n    .appName(\"lance-rest-example\")\n    .config(\"spark.sql.catalog.lance\", \"com.lancedb.lance.spark.LanceNamespaceSparkCatalog\")\n    .config(\"spark.sql.catalog.lance.impl\", \"rest\")\n    .config(\"spark.sql.catalog.lance.headers.x-api-key\", \"your-api-key\")\n    .config(\"spark.sql.catalog.lance.headers.x-lancedb-database\", \"your-database\")\n    .config(\"spark.sql.catalog.lance.uri\", \"https://your-database.us-east-1.api.lancedb.com\")\n    .getOrCreate();\n</code></pre> <pre><code>spark-shell \\\n  --packages com.lancedb:lance-spark-bundle-3.5_2.12:0.0.7 \\\n  --conf spark.sql.catalog.lance=com.lancedb.lance.spark.LanceNamespaceSparkCatalog \\\n  --conf spark.sql.catalog.lance.impl=rest \\\n  --conf spark.sql.catalog.lance.headers.x-api-key=your-api-key \\\n  --conf spark.sql.catalog.lance.headers.x-lancedb-database=your-database \\\n  --conf spark.sql.catalog.lance.uri=https://your-database.us-east-1.api.lancedb.com\n</code></pre> <pre><code>spark-submit \\\n  --packages com.lancedb:lance-spark-bundle-3.5_2.12:0.0.7 \\\n  --conf spark.sql.catalog.lance=com.lancedb.lance.spark.LanceNamespaceSparkCatalog \\\n  --conf spark.sql.catalog.lance.impl=rest \\\n  --conf spark.sql.catalog.lance.headers.x-api-key=your-api-key \\\n  --conf spark.sql.catalog.lance.headers.x-lancedb-database=your-database \\\n  --conf spark.sql.catalog.lance.uri=https://your-database.us-east-1.api.lancedb.com \\\n  your-application.jar\n</code></pre>"},{"location":"integrations/spark/config/#rest-configuration-parameters","title":"REST Configuration Parameters","text":"Parameter Required Description <code>spark.sql.catalog.{name}.uri</code> \u2713 REST API endpoint URL (e.g., <code>https://api.lancedb.com</code>) <code>spark.sql.catalog.{name}.headers.*</code> \u2717 HTTP headers for authentication (e.g., <code>headers.x-api-key</code>)"},{"location":"integrations/spark/config/#aws-glue-namespace","title":"AWS Glue Namespace","text":"<p>AWS Glue is Amazon's managed metastore service that provides a centralized catalog for your data assets.</p> PySparkScalaJava <pre><code>spark = SparkSession.builder \\\n    .appName(\"lance-glue-example\") \\\n    .config(\"spark.sql.catalog.lance\", \"com.lancedb.lance.spark.LanceNamespaceSparkCatalog\") \\\n    .config(\"spark.sql.catalog.lance.impl\", \"glue\") \\\n    .config(\"spark.sql.catalog.lance.region\", \"us-east-1\") \\\n    .config(\"spark.sql.catalog.lance.catalog_id\", \"123456789012\") \\\n    .config(\"spark.sql.catalog.lance.access_key_id\", \"your-access-key\") \\\n    .config(\"spark.sql.catalog.lance.secret_access_key\", \"your-secret-key\") \\\n    .config(\"spark.sql.catalog.lance.root\", \"s3://your-bucket/lance\") \\\n    .getOrCreate()\n</code></pre> <pre><code>val spark = SparkSession.builder()\n    .appName(\"lance-glue-example\")\n    .config(\"spark.sql.catalog.lance\", \"com.lancedb.lance.spark.LanceNamespaceSparkCatalog\")\n    .config(\"spark.sql.catalog.lance.impl\", \"glue\")\n    .config(\"spark.sql.catalog.lance.region\", \"us-east-1\")\n    .config(\"spark.sql.catalog.lance.catalog_id\", \"123456789012\")\n    .config(\"spark.sql.catalog.lance.access_key_id\", \"your-access-key\")\n    .config(\"spark.sql.catalog.lance.secret_access_key\", \"your-secret-key\")\n    .config(\"spark.sql.catalog.lance.root\", \"s3://your-bucket/lance\")\n    .getOrCreate()\n</code></pre> <pre><code>SparkSession spark = SparkSession.builder()\n    .appName(\"lance-glue-example\")\n    .config(\"spark.sql.catalog.lance\", \"com.lancedb.lance.spark.LanceNamespaceSparkCatalog\")\n    .config(\"spark.sql.catalog.lance.impl\", \"glue\")\n    .config(\"spark.sql.catalog.lance.region\", \"us-east-1\")\n    .config(\"spark.sql.catalog.lance.catalog_id\", \"123456789012\")\n    .config(\"spark.sql.catalog.lance.access_key_id\", \"your-access-key\")\n    .config(\"spark.sql.catalog.lance.secret_access_key\", \"your-secret-key\")\n    .config(\"spark.sql.catalog.lance.root\", \"s3://your-bucket/lance\")\n    .getOrCreate();\n</code></pre>"},{"location":"integrations/spark/config/#additional-dependencies","title":"Additional Dependencies","text":"<p>Using the Glue namespace requires additional dependencies beyond the main Lance Spark bundle: - <code>lance-namespace-glue</code>: Lance Glue namespace implementation - AWS Glue related dependencies: The easiest way is to use <code>software.amazon.awssdk:bundle</code> which includes all necessary AWS SDK components, though you can specify individual dependencies if preferred</p> <p>Example with Spark Shell: <pre><code>spark-shell \\\n  --packages com.lancedb:lance-spark-bundle-3.5_2.12:0.0.7,com.lancedb:lance-namespace-glue:0.0.7,software.amazon.awssdk:bundle:2.20.0 \\\n  --conf spark.sql.catalog.lance=com.lancedb.lance.spark.LanceNamespaceSparkCatalog \\\n  --conf spark.sql.catalog.lance.impl=glue \\\n  --conf spark.sql.catalog.lance.root=s3://your-bucket/lance\n</code></pre></p>"},{"location":"integrations/spark/config/#glue-configuration-parameters","title":"Glue Configuration Parameters","text":"Parameter Required Description <code>spark.sql.catalog.{name}.region</code> \u2717 AWS region for Glue operations (e.g., <code>us-east-1</code>). If not specified, derives from the default AWS region chain <code>spark.sql.catalog.{name}.catalog_id</code> \u2717 Glue catalog ID, defaults to the AWS account ID of the caller <code>spark.sql.catalog.{name}.endpoint</code> \u2717 Custom Glue service endpoint for connecting to compatible metastores <code>spark.sql.catalog.{name}.access_key_id</code> \u2717 AWS access key ID for static credentials <code>spark.sql.catalog.{name}.secret_access_key</code> \u2717 AWS secret access key for static credentials <code>spark.sql.catalog.{name}.session_token</code> \u2717 AWS session token for temporary credentials <code>spark.sql.catalog.{name}.root</code> \u2717 Storage root location (e.g., <code>s3://bucket/path</code>), defaults to current directory <code>spark.sql.catalog.{name}.storage.*</code> \u2717 Additional storage configuration options"},{"location":"integrations/spark/config/#apache-hive-namespace","title":"Apache Hive Namespace","text":"<p>Lance supports both Hive 2.x and Hive 3.x metastores for metadata management.</p>"},{"location":"integrations/spark/config/#hive-3x-namespace","title":"Hive 3.x Namespace","text":"PySparkScalaJava <pre><code>spark = SparkSession.builder \\\n    .appName(\"lance-hive3-example\") \\\n    .config(\"spark.sql.catalog.lance\", \"com.lancedb.lance.spark.LanceNamespaceSparkCatalog\") \\\n    .config(\"spark.sql.catalog.lance.impl\", \"hive3\") \\\n    .config(\"spark.sql.catalog.lance.parent\", \"hive\") \\\n    .config(\"spark.sql.catalog.lance.parent_delimiter\", \".\") \\\n    .config(\"spark.sql.catalog.lance.hadoop.hive.metastore.uris\", \"thrift://metastore:9083\") \\\n    .config(\"spark.sql.catalog.lance.client.pool-size\", \"5\") \\\n    .config(\"spark.sql.catalog.lance.root\", \"hdfs://namenode:8020/lance\") \\\n    .getOrCreate()\n</code></pre> <pre><code>val spark = SparkSession.builder()\n    .appName(\"lance-hive3-example\")\n    .config(\"spark.sql.catalog.lance\", \"com.lancedb.lance.spark.LanceNamespaceSparkCatalog\")\n    .config(\"spark.sql.catalog.lance.impl\", \"hive3\")\n    .config(\"spark.sql.catalog.lance.parent\", \"hive\")\n    .config(\"spark.sql.catalog.lance.parent_delimiter\", \".\")\n    .config(\"spark.sql.catalog.lance.hadoop.hive.metastore.uris\", \"thrift://metastore:9083\")\n    .config(\"spark.sql.catalog.lance.client.pool-size\", \"5\")\n    .config(\"spark.sql.catalog.lance.root\", \"hdfs://namenode:8020/lance\")\n    .getOrCreate()\n</code></pre> <pre><code>SparkSession spark = SparkSession.builder()\n    .appName(\"lance-hive3-example\")\n    .config(\"spark.sql.catalog.lance\", \"com.lancedb.lance.spark.LanceNamespaceSparkCatalog\")\n    .config(\"spark.sql.catalog.lance.impl\", \"hive3\")\n    .config(\"spark.sql.catalog.lance.parent\", \"hive\")\n    .config(\"spark.sql.catalog.lance.parent_delimiter\", \".\")\n    .config(\"spark.sql.catalog.lance.hadoop.hive.metastore.uris\", \"thrift://metastore:9083\")\n    .config(\"spark.sql.catalog.lance.client.pool-size\", \"5\")\n    .config(\"spark.sql.catalog.lance.root\", \"hdfs://namenode:8020/lance\")\n    .getOrCreate();\n</code></pre>"},{"location":"integrations/spark/config/#hive-2x-namespace","title":"Hive 2.x Namespace","text":"PySparkScalaJava <pre><code>spark = SparkSession.builder \\\n    .appName(\"lance-hive2-example\") \\\n    .config(\"spark.sql.catalog.lance\", \"com.lancedb.lance.spark.LanceNamespaceSparkCatalog\") \\\n    .config(\"spark.sql.catalog.lance.impl\", \"hive2\") \\\n    .config(\"spark.sql.catalog.lance.hadoop.hive.metastore.uris\", \"thrift://metastore:9083\") \\\n    .config(\"spark.sql.catalog.lance.client.pool-size\", \"3\") \\\n    .config(\"spark.sql.catalog.lance.root\", \"hdfs://namenode:8020/lance\") \\\n    .getOrCreate()\n</code></pre> <pre><code>val spark = SparkSession.builder()\n    .appName(\"lance-hive2-example\")\n    .config(\"spark.sql.catalog.lance\", \"com.lancedb.lance.spark.LanceNamespaceSparkCatalog\")\n    .config(\"spark.sql.catalog.lance.impl\", \"hive2\")\n    .config(\"spark.sql.catalog.lance.hadoop.hive.metastore.uris\", \"thrift://metastore:9083\")\n    .config(\"spark.sql.catalog.lance.client.pool-size\", \"3\")\n    .config(\"spark.sql.catalog.lance.root\", \"hdfs://namenode:8020/lance\")\n    .getOrCreate()\n</code></pre> <pre><code>SparkSession spark = SparkSession.builder()\n    .appName(\"lance-hive2-example\")\n    .config(\"spark.sql.catalog.lance\", \"com.lancedb.lance.spark.LanceNamespaceSparkCatalog\")\n    .config(\"spark.sql.catalog.lance.impl\", \"hive2\")\n    .config(\"spark.sql.catalog.lance.hadoop.hive.metastore.uris\", \"thrift://metastore:9083\")\n    .config(\"spark.sql.catalog.lance.client.pool-size\", \"3\")\n    .config(\"spark.sql.catalog.lance.root\", \"hdfs://namenode:8020/lance\")\n    .getOrCreate();\n</code></pre>"},{"location":"integrations/spark/config/#additional-dependencies_1","title":"Additional Dependencies","text":"<p>Using Hive namespaces requires additional JARs beyond the main Lance Spark bundle: - For Hive 2.x: <code>lance-namespace-hive2</code> - For Hive 3.x: <code>lance-namespace-hive3</code></p> <p>Example with Spark Shell for Hive 3.x: <pre><code>spark-shell \\\n  --packages com.lancedb:lance-spark-bundle-3.5_2.12:0.0.7,com.lancedb:lance-namespace-hive3:0.0.7 \\\n  --conf spark.sql.catalog.lance=com.lancedb.lance.spark.LanceNamespaceSparkCatalog \\\n  --conf spark.sql.catalog.lance.impl=hive3 \\\n  --conf spark.sql.catalog.lance.hadoop.hive.metastore.uris=thrift://metastore:9083 \\\n  --conf spark.sql.catalog.lance.root=hdfs://namenode:8020/lance\n</code></pre></p>"},{"location":"integrations/spark/config/#hive-configuration-parameters","title":"Hive Configuration Parameters","text":"Parameter Required Description <code>spark.sql.catalog.{name}.hadoop.*</code> \u2717 Additional Hadoop configuration options, will override the default Hadoop configuration <code>spark.sql.catalog.{name}.client.pool-size</code> \u2717 Connection pool size for metastore clients (default: 3) <code>spark.sql.catalog.{name}.root</code> \u2717 Storage root location for Lance tables (default: current directory) <code>spark.sql.catalog.{name}.storage.*</code> \u2717 Additional storage configuration options <code>spark.sql.catalog.{name}.parent</code> \u2717 Parent prefix for multi-level namespaces (Hive 3.x only, default: <code>hive</code>). See Note on Namespace Levels for more details."},{"location":"integrations/spark/config/#note-on-namespace-levels","title":"Note on Namespace Levels","text":"<p>Spark provides at least a 3 level hierarchy of catalog \u2192 multi-level namespace \u2192 table. Most users treat Spark as a 3 level hierarchy with 1 level namespace.</p>"},{"location":"integrations/spark/config/#for-namespaces-with-less-than-3-levels","title":"For Namespaces with Less Than 3 Levels","text":"<p>Since Lance allows a 2 level hierarchy of root namespace \u2192 table for namespaces like <code>DirectoryNamespace</code>, the <code>LanceNamespaceSparkCatalog</code> provides a configuration <code>extra_level</code> which puts an additional dummy level to match the Spark hierarchy and make it root namespace \u2192 dummy extra level \u2192 table.</p> <p>Currently, this is automatically set with <code>extra_level=default</code> for <code>DirectoryNamespace</code> and when <code>RestNamespace</code> if it cannot respond to <code>ListNamespaces</code> operation. If you have a custom namespace implementation of the same behavior, you can also set the config to add the extra level.</p>"},{"location":"integrations/spark/config/#for-namespaces-with-more-than-3-levels","title":"For Namespaces with More Than 3 Levels","text":"<p>Some namespace implementations like Hive3 support more than 3 levels of hierarchy. For example, Hive3 has a  4 level hierarchy: root metastore \u2192 catalog \u2192 database \u2192 table.</p> <p>To handle this, the <code>LanceNamespaceSparkCatalog</code> provides <code>parent</code> and <code>parent_delimiter</code> configurations which allow you to specify a parent prefix that gets prepended to all namespace operations.</p>"},{"location":"integrations/spark/config/#for-example-with-hive3","title":"For example, with Hive3:","text":"<ul> <li>Setting <code>parent=hive</code> and <code>parent_delimiter=.</code> </li> <li>When Spark requests namespace <code>[\"database1\"]</code>, it gets transformed to <code>[\"hive\", \"database1\"]</code> for the API call</li> <li>This allows the 4-level Hive 3 structure to work within Spark's 3-level model.</li> </ul> <p>The parent configuration effectively \"anchors\" your Spark catalog at a specific level within the deeper namespace hierarchy, making the extra levels transparent to Spark users while maintaining compatibility with the underlying namespace implementation.</p>"},{"location":"integrations/spark/install/","title":"Install","text":""},{"location":"integrations/spark/install/#maven-central-packages","title":"Maven Central Packages","text":"<p>The connector packages are published to Maven Central under the <code>com.lancedb</code> namespace. Choose the appropriate artifact based on your use case:</p>"},{"location":"integrations/spark/install/#available-artifacts","title":"Available Artifacts","text":"Artifact Type Name Pattern Description Example Base Jar <code>lance-spark-base_&lt;scala_version&gt;</code> Jar with logic shared by different versions of Spark Lance connectors. lance-spark-base_2.12 Lean Jar <code>lance-spark-&lt;spark-version&gt;_&lt;scala_version&gt;</code> Jar with only the Spark Lance connector logic lance-spark-3.5_2.12 Bundled Jar <code>lance-spark-bundle-&lt;spark-version&gt;_&lt;scala_version&gt;</code> Jar with all necessary non-Spark dependencies lance-spark-bundle-3.5_2.12"},{"location":"integrations/spark/install/#choosing-the-right-artifact","title":"Choosing the Right Artifact","text":"<ul> <li>Bundled Jar: Recommended for most users. Use this if you want to quickly get started or use the connector in a Spark shell/notebook environment.</li> <li>Lean Jar: Use this if you're building a custom Spark application and want to manage and bundle dependencies yourself.</li> <li>Base Jar: Internal use only. Use this if you would like to build a custom Spark Lance connector with a different Spark version.</li> </ul>"},{"location":"integrations/spark/install/#dependency-configuration","title":"Dependency Configuration","text":""},{"location":"integrations/spark/install/#in-spark-application-code","title":"In Spark Application Code","text":"<p>Typically, you use the bundled jar in your Spark application as a provided (compile only) dependency. The actual jar is supplied to the Spark cluster separately. If you want to also include the bundled jar in your own bundle, remove the provided (compile only) annotation.</p> MavenGradlesbt <pre><code>&lt;!-- For Spark 3.5 (Scala 2.12) --&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;com.lancedb&lt;/groupId&gt;\n    &lt;artifactId&gt;lance-spark-bundle-3.5_2.12&lt;/artifactId&gt;\n    &lt;version&gt;0.0.7&lt;/version&gt;\n    &lt;scope&gt;provided&lt;/scope&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>dependencies {\n    // For Spark 3.5 (Scala 2.12)\n    compileOnly 'com.lancedb:lance-spark-bundle-3.5_2.12:0.0.7'\n}\n</code></pre> <pre><code>libraryDependencies ++= Seq(\n  // For Spark 3.5 (Scala 2.12)\n  \"com.lancedb\" %% \"lance-spark-bundle-3.5_2.12\" % \"0.0.7\" % \"provided\"\n)\n</code></pre>"},{"location":"integrations/spark/install/#in-spark-cluster","title":"In Spark Cluster","text":"<p>You can either download the bundled jar dependency from Maven and add it to your Spark classpath, or supply the dependency dynamically to a Spark cluster through <code>--packages</code> flag. See Configuration for more details.</p>"},{"location":"integrations/spark/install/#requirements","title":"Requirements","text":""},{"location":"integrations/spark/install/#java","title":"Java","text":"Java Version Support Status Notes Java 8 \u274c Not Supported No longer supported Java 11 \u2705 Supported Minimum required version Java 17 \u2705 Supported Latest LTS version (Spark 4.0 dropped Java 8 and 11 support) Java 21+ \u26a0\ufe0f Untested May work but not officially tested"},{"location":"integrations/spark/install/#scala","title":"Scala","text":"Scala Version Support Status Notes Scala 2.12 \u2705 Supported Fully supported Scala 2.13 \u2705 Supported Fully supported Scala 3.x \u274c Not Supported Not currently planned"},{"location":"integrations/spark/install/#apache-spark","title":"Apache Spark","text":"Spark Version Support Status Notes Spark 4.0 \u2705 Supported Scala 2.13 only (Spark 4.0 dropped Scala 2.12 support) Spark 3.5 \u2705 Supported Scala 2.12 and 2.13 Spark 3.4 \u2705 Supported Scala 2.12 and 2.13 Spark 3.1, 3.2, 3.3 \u26a0\ufe0f Untested May work but not officially tested Spark 2.x \u274c Not Supported"},{"location":"integrations/spark/install/#operating-system","title":"Operating System","text":"Operating System Architecture Support Status Notes Linux x86_64 \u2705 Supported Linux ARM64 \u2705 Supported Including Apple Silicon via Rosetta macOS x86_64 \u2705 Supported Intel-based Macs macOS ARM64 \u2705 Supported Apple Silicon (M1/M2/M3) Windows x86_64 \ud83d\udea7 In Progress Support planned for future releases"},{"location":"integrations/spark/operations/ddl/create-namespace/","title":"CREATE NAMESPACE","text":"<p>Create new child namespaces in the current Lance namespace.</p> <p>Create a simple namespace:</p> <pre><code>CREATE NAMESPACE company;\n</code></pre> <p>Create nested namespace:</p> <pre><code>CREATE NAMESPACE company.analytics;\n</code></pre> <p>Create namespace if it doesn't exist:</p> <pre><code>CREATE NAMESPACE IF NOT EXISTS company;\n</code></pre> <p>Create namespace with properties:</p> <pre><code>CREATE NAMESPACE company WITH PROPERTIES (\n    'description' = 'Company data namespace',\n    'owner' = 'data-team'\n);\n</code></pre>"},{"location":"integrations/spark/operations/ddl/create-table/","title":"CREATE TABLE","text":"<p>Create new Lance tables with SQL DDL statements.</p>"},{"location":"integrations/spark/operations/ddl/create-table/#basic-table-creation","title":"Basic Table Creation","text":"<p>Create a simple table:</p> <pre><code>CREATE TABLE users (\n    id BIGINT NOT NULL,\n    name STRING,\n    email STRING,\n    created_at TIMESTAMP\n);\n</code></pre> <p>Create table with complex data types:</p> <pre><code>CREATE TABLE events (\n    event_id BIGINT NOT NULL,\n    user_id BIGINT,\n    event_type STRING,\n    tags ARRAY&lt;STRING&gt;,\n    metadata STRUCT&lt;\n        source: STRING,\n        version: INT,\n        processed_at: TIMESTAMP\n    &gt;,\n    occurred_at TIMESTAMP\n);\n</code></pre>"},{"location":"integrations/spark/operations/ddl/create-table/#vector-columns","title":"Vector Columns","text":"<p>Lance supports vector (embedding) columns for AI workloads. These columns are stored internally as Arrow <code>FixedSizeList[n]</code> where <code>n</code> is the vector dimension. Since Spark SQL doesn't have a native fixed-size array type, you must use <code>ARRAY&lt;FLOAT&gt;</code> or <code>ARRAY&lt;DOUBLE&gt;</code> with table properties to specify the fixed dimension. The Lance-Spark connector will automatically convert these to the appropriate Arrow FixedSizeList format during write operations.</p>"},{"location":"integrations/spark/operations/ddl/create-table/#supported-types","title":"Supported Types","text":"<ul> <li>Element Types: <code>FLOAT</code> (float32), <code>DOUBLE</code> (float64)</li> <li>Requirements:<ul> <li>Vectors must be non-nullable</li> <li>All vectors in a column must have the same dimension</li> <li>Dimension is specified via table properties</li> </ul> </li> </ul>"},{"location":"integrations/spark/operations/ddl/create-table/#creating-vector-columns","title":"Creating Vector Columns","text":"<p>To create a table with vector columns, use the table property pattern <code>&lt;column_name&gt;.arrow.fixed-size-list.size</code> with the dimension as the value:</p> <pre><code>CREATE TABLE embeddings_table (\n    id INT NOT NULL,\n    text STRING,\n    embeddings ARRAY&lt;FLOAT&gt; NOT NULL\n) USING lance\nTBLPROPERTIES (\n    'embeddings.arrow.fixed-size-list.size' = '128'\n);\n</code></pre> <p>Create table with multiple vector columns of different dimensions:</p> <pre><code>CREATE TABLE multi_vector_table (\n    id INT NOT NULL,\n    title STRING,\n    text_embeddings ARRAY&lt;FLOAT&gt; NOT NULL,\n    image_embeddings ARRAY&lt;DOUBLE&gt; NOT NULL\n) USING lance\nTBLPROPERTIES (\n    'text_embeddings.arrow.fixed-size-list.size' = '384',\n    'image_embeddings.arrow.fixed-size-list.size' = '512'\n);\n</code></pre>"},{"location":"integrations/spark/operations/ddl/create-table/#working-with-vector-tables","title":"Working with Vector Tables","text":"<p>Once created, you can insert data using SQL:</p> <pre><code>-- Insert vector data (example with small vectors for clarity)\nINSERT INTO embeddings_table VALUES\n    (1, 'first text', array(0.1, 0.2, 0.3, ...)), -- 128 float values\n    (2, 'second text', array(0.4, 0.5, 0.6, ...));\n</code></pre> <p>Query vector tables:</p> <p><pre><code>-- Select vectors\nSELECT id, text FROM embeddings_table WHERE id = 1;\n\n-- Count rows\nSELECT COUNT(*) FROM embeddings_table;\n</code></pre> Note: When reading vector columns back, they are automatically converted to Spark's <code>ARRAY&lt;FLOAT&gt;</code> or <code>ARRAY&lt;DOUBLE&gt;</code> types for compatibility with Spark operations.</p>"},{"location":"integrations/spark/operations/ddl/create-table/#vector-indexing","title":"Vector Indexing","text":"<p>After creating and populating vector columns, you can create vector indexes using Lance Python API for similarity search:</p> <pre><code>import lance\n\n# Open the dataset\nds = lance.dataset(\"/path/to/embeddings_table.lance\")\n\n# Create a vector index on the embeddings column\nds.create_index(\n    \"embeddings\",\n    index_type=\"IVF_PQ\",\n)\n\n# Perform similarity search\nimport numpy as np\nquery_vector = np.random.rand(128).astype(np.float32)\nresults = ds.to_table(\n    nearest={\"column\": \"embeddings\", \"q\": query_vector, \"k\": 10}\n).to_pandas()\n</code></pre>"},{"location":"integrations/spark/operations/ddl/create-table/#blob-columns","title":"Blob Columns","text":"<p>Lance supports blob encoding for large binary data. Blob columns store large binary values (typically &gt; 64KB) out-of-line in a separate blob file, which improves query performance when not accessing the blob data directly.</p>"},{"location":"integrations/spark/operations/ddl/create-table/#supported-types_1","title":"Supported Types","text":"<ul> <li>Column Type: <code>BINARY</code></li> <li>Requirements:<ul> <li>Column must be nullable (blob data is not materialized when read, so nullability is required)</li> <li>Blob encoding is specified via table properties</li> </ul> </li> </ul>"},{"location":"integrations/spark/operations/ddl/create-table/#creating-blob-columns","title":"Creating Blob Columns","text":"<p>To create a table with blob columns, use the table property pattern <code>&lt;column_name&gt;.lance.encoding</code> with the value <code>'blob'</code>:</p> <pre><code>CREATE TABLE documents (\\n    id INT NOT NULL,\\n    title STRING,\\n    content BINARY\\n) USING lance\\nTBLPROPERTIES (\\n    'content.lance.encoding' = 'blob'\\n);\n</code></pre> <p>Create table with multiple blob columns:</p> <pre><code>CREATE TABLE media_files (\\n    id INT NOT NULL,\\n    filename STRING,\\n    image_data BINARY,\\n    video_data BINARY\\n) USING lance\\nTBLPROPERTIES (\\n    'image_data.lance.encoding' = 'blob',\\n    'video_data.lance.encoding' = 'blob'\\n);\n</code></pre>"},{"location":"integrations/spark/operations/ddl/create-table/#working-with-blob-tables","title":"Working with Blob Tables","text":"<p>Insert blob data using SQL or DataFrames:</p> <pre><code>-- Insert blob data (example with binary literals)\nINSERT INTO documents VALUES\\n    (1, 'Document 1', X'48656C6C6F'),\\n    (2, 'Document 2', X'576F726C64');\n</code></pre> <p>Query blob tables:</p> <pre><code>-- Select non-blob columns\nSELECT id, title FROM documents WHERE id = 1;\n\n-- Count rows\nSELECT COUNT(*) FROM documents;\n</code></pre> <p>Note: When reading blob columns, they are returned as empty byte arrays since the data is not materialized by default. To access the actual blob data, you need to use the blob position and size virtual columns.</p>"},{"location":"integrations/spark/operations/ddl/create-table/#blob-virtual-columns","title":"Blob Virtual Columns","text":"<p>When you have a blob column, Lance automatically provides virtual columns that give you access to the blob's position and size in the blob file:</p> <ul> <li><code>&lt;column_name&gt;__blob_pos</code> - The byte position of the blob in the blob file</li> <li><code>&lt;column_name&gt;__blob_size</code> - The size of the blob in bytes</li> </ul> <pre><code>-- Select blob metadata\nSELECT id, title, content__blob_pos, content__blob_size\nFROM documents\nWHERE id = 1;\n</code></pre> <p>These virtual columns can be used to: - Verify that blob data was written successfully - Implement custom blob retrieval logic - Monitor blob storage statistics</p>"},{"location":"integrations/spark/operations/ddl/dataframe-create-table/","title":"DataFrame Create Table","text":"<p>Create Lance tables from DataFrames using the DataSource V2 API.</p>"},{"location":"integrations/spark/operations/ddl/dataframe-create-table/#basic-dataframe-creation","title":"Basic DataFrame Creation","text":"PythonScalaJava <pre><code># Create DataFrame\ndata = [\n(1, \"Alice\", \"alice@example.com\"),\n(2, \"Bob\", \"bob@example.com\"),\n(3, \"Charlie\", \"charlie@example.com\")\n]\ndf = spark.createDataFrame(data, [\"id\", \"name\", \"email\"])\n\n# Write as new table using catalog\ndf.writeTo(\"users\").create()\n</code></pre> <pre><code>import spark.implicits._\n\n// Create DataFrame\nval data = Seq(\n    (1, \"Alice\", \"alice@example.com\"),\n    (2, \"Bob\", \"bob@example.com\"),\n    (3, \"Charlie\", \"charlie@example.com\")\n)\nval df = data.toDF(\"id\", \"name\", \"email\")\n\n// Write as new table using catalog\ndf.writeTo(\"users\").create()\n</code></pre> <pre><code>import org.apache.spark.sql.types.*;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.RowFactory;\n\n// Create DataFrame\nList&lt;Row&gt; data = Arrays.asList(\n    RowFactory.create(1L, \"Alice\", \"alice@example.com\"),\n    RowFactory.create(2L, \"Bob\", \"bob@example.com\"),\n    RowFactory.create(3L, \"Charlie\", \"charlie@example.com\")\n);\n\nStructType schema = new StructType(new StructField[]{\n    new StructField(\"id\", DataTypes.LongType, false, Metadata.empty()),\n    new StructField(\"name\", DataTypes.StringType, true, Metadata.empty()),\n    new StructField(\"email\", DataTypes.StringType, true, Metadata.empty())\n});\n\nDataset&lt;Row&gt; df = spark.createDataFrame(data, schema);\n\n// Write as new table using catalog\ndf.writeTo(\"users\").create();\n</code></pre>"},{"location":"integrations/spark/operations/ddl/dataframe-create-table/#creating-tables-with-vector-columns","title":"Creating Tables with Vector Columns","text":"<p>Lance supports vector (embedding) columns for AI workloads. These columns are stored internally as Arrow <code>FixedSizeList[n]</code> where <code>n</code> is the vector dimension. Since Spark DataFrames don't have a native fixed-size array type, you need to add metadata to your schema fields to indicate that an <code>ArrayType(FloatType)</code> or <code>ArrayType(DoubleType)</code> should be converted to Arrow FixedSizeList.</p> <p>The metadata key <code>\"arrow.fixed-size-list.size\"</code> with a value like <code>128</code> tells the Lance-Spark connector to convert that array column to a <code>FixedSizeList[128]</code> during write operations.</p>"},{"location":"integrations/spark/operations/ddl/dataframe-create-table/#supported-types","title":"Supported Types","text":"<ul> <li>Element Types: <code>FloatType</code> (float32), <code>DoubleType</code> (float64)</li> <li>Array Requirements:</li> <li>Must have <code>containsNull = false</code></li> <li>Column must be non-nullable</li> <li>All arrays must have exactly the specified dimension</li> </ul>"},{"location":"integrations/spark/operations/ddl/dataframe-create-table/#examples","title":"Examples","text":"PythonScalaJava <pre><code>from pyspark.sql.types import StructType, StructField, IntegerType, ArrayType, FloatType\nfrom pyspark.sql.types import Metadata\n\n# Create schema with vector column\nvector_metadata = {\"arrow.fixed-size-list.size\": 128}\nschema = StructType([\n    StructField(\"id\", IntegerType(), False),\n    StructField(\"embeddings\", ArrayType(FloatType(), False), False, vector_metadata)\n])\n\n# Create DataFrame with vector data\nimport numpy as np\ndata = [(i, np.random.rand(128).astype(np.float32).tolist()) for i in range(100)]\ndf = spark.createDataFrame(data, schema)\n\n# Write to Lance format\ndf.writeTo(\"vectors_table\").create()\n</code></pre> <pre><code>import org.apache.spark.sql.types._\n\n// Create metadata for vector column\nval vectorMetadata = new MetadataBuilder()\n  .putLong(\"arrow.fixed-size-list.size\", 128)\n  .build()\n\n// Create schema with vector column\nval schema = StructType(Array(\n  StructField(\"id\", IntegerType, false),\n  StructField(\"embeddings\", ArrayType(FloatType, false), false, vectorMetadata)\n))\n\n// Create DataFrame with vector data\nimport scala.util.Random\nval data = (0 until 100).map { i =&gt;\n  (i, Array.fill(128)(Random.nextFloat()))\n}\nval df = spark.createDataFrame(data).toDF(\"id\", \"embeddings\")\n\n// Write to Lance format\ndf.writeTo(\"vectors_table\").create()\n</code></pre> <pre><code>import org.apache.spark.sql.types.*;\n\n// Create metadata for vector column\nMetadata vectorMetadata = new MetadataBuilder()\n    .putLong(\"arrow.fixed-size-list.size\", 128)\n    .build();\n\n// Create schema with vector column\nStructType schema = new StructType(new StructField[] {\n    DataTypes.createStructField(\"id\", DataTypes.IntegerType, false),\n    DataTypes.createStructField(\"embeddings\", \n        DataTypes.createArrayType(DataTypes.FloatType, false),\n        false, vectorMetadata)\n});\n\n// Create DataFrame with vector data\nList&lt;Row&gt; rows = new ArrayList&lt;&gt;();\nRandom random = new Random();\nfor (int i = 0; i &lt; 100; i++) {\n    float[] vector = new float[128];\n    for (int j = 0; j &lt; 128; j++) {\n        vector[j] = random.nextFloat();\n    }\n    rows.add(RowFactory.create(i, vector));\n}\nDataset&lt;Row&gt; df = spark.createDataFrame(rows, schema);\n\n// Write to Lance format\ndf.writeTo(\"vectors_table\").create();\n</code></pre>"},{"location":"integrations/spark/operations/ddl/dataframe-create-table/#creating-multiple-vector-columns","title":"Creating Multiple Vector Columns","text":"<p>You can create DataFrames with multiple vector columns, each with different dimensions:</p> Python <pre><code>from pyspark.sql.types import DoubleType\n\n# Create schema with multiple vector columns\ntext_metadata = {\"arrow.fixed-size-list.size\": 384}\nimage_metadata = {\"arrow.fixed-size-list.size\": 512}\n\nschema = StructType([\n    StructField(\"id\", IntegerType(), False),\n    StructField(\"text_embeddings\", ArrayType(FloatType(), False), False, text_metadata),\n    StructField(\"image_embeddings\", ArrayType(DoubleType(), False), False, image_metadata)\n])\n\n# Create DataFrame with multiple vector columns\ndata = [\n    (i, \n     np.random.rand(384).astype(np.float32).tolist(),\n     np.random.rand(512).tolist())\n    for i in range(100)\n]\ndf = spark.createDataFrame(data, schema)\n\n# Write to Lance format\ndf.writeTo(\"multi_vectors_table\").create()\n</code></pre>"},{"location":"integrations/spark/operations/ddl/dataframe-create-table/#vector-indexing","title":"Vector Indexing","text":"<p>After creating tables with vector columns, you can create vector indexes for efficient similarity search using the Lance Python API:</p> <pre><code>import lance\nimport numpy as np\n\n# Open the dataset\nds = lance.dataset(\"/path/to/vectors_table.lance\")\n\n# Create a vector index on the embeddings column\nds.create_index(\n    \"embeddings\",\n    index_type=\"IVF_PQ\",\n    num_partitions=256,\n    num_sub_vectors=16\n)\n\n# Perform similarity search\nquery_vector = np.random.rand(128).astype(np.float32)\nresults = ds.to_table(\n    nearest={\"column\": \"embeddings\", \"q\": query_vector, \"k\": 10}\n).to_pandas()\n</code></pre> <p>Note: When reading vector columns back into Spark DataFrames, they are automatically converted to regular <code>ArrayType(FloatType)</code> or <code>ArrayType(DoubleType)</code> for compatibility with Spark operations.</p>"},{"location":"integrations/spark/operations/ddl/describe-namespace/","title":"DESCRIBE NAMESPACE","text":"<p>Display information about a namespace.</p> <p>Describe a namespace:</p> <pre><code>DESCRIBE NAMESPACE company;\n</code></pre> <p>Show extended namespace information:</p> <pre><code>DESCRIBE NAMESPACE EXTENDED company;\n</code></pre> <p>Describe nested namespace:</p> <pre><code>DESCRIBE NAMESPACE company.analytics;\n</code></pre>"},{"location":"integrations/spark/operations/ddl/describe-table/","title":"DESCRIBE TABLE","text":"<p>Inspect table structure and metadata.</p> <p>Describe table structure:</p> <pre><code>DESCRIBE TABLE users;\n</code></pre> <p>Show detailed table information:</p> <pre><code>DESCRIBE EXTENDED users;\n</code></pre>"},{"location":"integrations/spark/operations/ddl/drop-namespace/","title":"DROP NAMESPACE","text":"<p>Remove a child namespace from the current Lance namespace.</p> <p>Drop an empty namespace:</p> <pre><code>DROP NAMESPACE company;\n</code></pre> <p>Drop namespace if it exists (no error if it doesn't exist):</p> <pre><code>DROP NAMESPACE IF EXISTS company;\n</code></pre> <p>Drop namespace and all its contents (CASCADE):</p> <pre><code>DROP NAMESPACE company CASCADE;\n</code></pre> <p>Drop nested namespace:</p> <pre><code>DROP NAMESPACE company.analytics;\n</code></pre>"},{"location":"integrations/spark/operations/ddl/drop-table/","title":"DROP TABLE","text":"<p>Remove Lance tables from the catalog.</p> <p>Drop table (also removes data):</p> <pre><code>DROP TABLE users;\n</code></pre> <p>Drop table if it exists (no error if table doesn't exist):</p> <pre><code>DROP TABLE IF EXISTS users;\n</code></pre>"},{"location":"integrations/spark/operations/ddl/show-namespaces/","title":"SHOW NAMESPACES","text":"<p>List available child namespaces.</p> <p>Show all namespaces:</p> <pre><code>SHOW NAMESPACES;\n</code></pre> <p>Show namespaces in a specific namespace:</p> <pre><code>SHOW NAMESPACES IN company;\n</code></pre> <p>Show namespaces with pattern matching:</p> <pre><code>SHOW NAMESPACES LIKE 'comp*';\n</code></pre>"},{"location":"integrations/spark/operations/ddl/show-tables/","title":"SHOW TABLES","text":"<p>Display available tables in Lance namespaces.</p> <p>Show all tables in the default namespace:</p> <pre><code>SHOW TABLES;\n</code></pre> <p>Show all tables in a specific namespace <code>ns2</code>:</p> <pre><code>SHOW TABLES IN ns2;\n</code></pre>"},{"location":"integrations/spark/operations/dml/dataframe-write/","title":"DataFrame Write","text":"<p>Append data to existing Lance tables using DataFrames.</p> PythonScalaJava <pre><code># Create new data\nnew_data = [\n    (8, \"Henry\", \"henry@example.com\"),\n    (9, \"Ivy\", \"ivy@example.com\")\n]\nnew_df = spark.createDataFrame(new_data, [\"id\", \"name\", \"email\"])\n\n# Append to existing table\nnew_df.writeTo(\"users\").append()\n\n# Alternative: use traditional write API with mode\nnew_df.write.mode(\"append\").saveAsTable(\"users\")\n</code></pre> <pre><code>// Create new data\nval newData = Seq(\n    (8, \"Henry\", \"henry@example.com\"),\n    (9, \"Ivy\", \"ivy@example.com\")\n)\nval newDF = newData.toDF(\"id\", \"name\", \"email\")\n\n// Append to existing table\nnewDF.writeTo(\"users\").append()\n\n// Alternative: use traditional write API with mode\nnewDF.write.mode(\"append\").saveAsTable(\"users\")\n</code></pre> <pre><code>// Create new data\nList&lt;Row&gt; newData = Arrays.asList(\n    RowFactory.create(8L, \"Henry\", \"henry@example.com\"),\n    RowFactory.create(9L, \"Ivy\", \"ivy@example.com\")\n);\nDataset&lt;Row&gt; newDF = spark.createDataFrame(newData, schema);\n\n// Append to existing table\nnewDF.writeTo(\"users\").append();\n\n// Alternative: use traditional write API with mode\nnewDF.write().mode(\"append\").saveAsTable(\"users\");\n</code></pre>"},{"location":"integrations/spark/operations/dml/dataframe-write/#writing-blob-data","title":"Writing Blob Data","text":"<p>When writing binary data to blob columns, you need to add metadata to the DataFrame schema to indicate blob encoding.</p> PythonScalaJava <pre><code>from pyspark.sql.types import StructType, StructField, IntegerType, StringType, BinaryType\n\n# Create schema with blob metadata\nschema = StructType([\n    StructField(\"id\", IntegerType(), False),\n    StructField(\"title\", StringType(), True),\n    StructField(\"content\", BinaryType(), True,\n               metadata={\"lance-encoding:blob\": \"true\"})\n])\n\n# Create data with large binary content\ndata = [\n    (1, \"Document 1\", bytearray(b\"Large binary content...\" * 10000)),\n    (2, \"Document 2\", bytearray(b\"Another large file...\" * 10000))\n]\n\ndf = spark.createDataFrame(data, schema)\n\n# Write to blob table\ndf.writeTo(\"documents\").append()\n</code></pre> <pre><code>import org.apache.spark.sql.types._\n\n// Create schema with blob metadata\nval metadata = new MetadataBuilder()\n  .putString(\"lance-encoding:blob\", \"true\")\n  .build()\n\nval schema = StructType(Array(\n  StructField(\"id\", IntegerType, nullable = false),\n  StructField(\"title\", StringType, nullable = true),\n  StructField(\"content\", BinaryType, nullable = true, metadata)\n))\n\n// Create data with large binary content\nval data = Seq(\n  (1, \"Document 1\", Array.fill[Byte](1000000)(0x42)),\n  (2, \"Document 2\", Array.fill[Byte](1000000)(0x43))\n)\n\nval df = spark.createDataFrame(data).toDF(\"id\", \"title\", \"content\")\n\n// Write to blob table\ndf.writeTo(\"documents\").append()\n</code></pre> <pre><code>import org.apache.spark.sql.types.*;\nimport java.util.Arrays;\nimport java.util.List;\n\n// Create schema with blob metadata\nMetadata blobMetadata = new MetadataBuilder()\n    .putString(\"lance-encoding:blob\", \"true\")\n    .build();\n\nStructType schema = new StructType(new StructField[]{\n    DataTypes.createStructField(\"id\", DataTypes.IntegerType, false),\n    DataTypes.createStructField(\"title\", DataTypes.StringType, true),\n    DataTypes.createStructField(\"content\", DataTypes.BinaryType, true, blobMetadata)\n});\n\n// Create data with large binary content\nbyte[] largeData1 = new byte[1000000];\nbyte[] largeData2 = new byte[1000000];\nArrays.fill(largeData1, (byte) 0x42);\nArrays.fill(largeData2, (byte) 0x43);\n\nList&lt;Row&gt; data = Arrays.asList(\n    RowFactory.create(1, \"Document 1\", largeData1),\n    RowFactory.create(2, \"Document 2\", largeData2)\n);\n\nDataset&lt;Row&gt; df = spark.createDataFrame(data, schema);\n\n// Write to blob table\ndf.writeTo(\"documents\").append();\n</code></pre> <p>Important Notes:</p> <ul> <li>Blob metadata must be added to the DataFrame schema using the key <code>\"lance-encoding:blob\"</code> with value <code>\"true\"</code></li> <li>The binary column must be nullable in the schema</li> <li>Blob encoding is most beneficial for large binary values (typically &gt; 64KB)</li> <li>When writing to an existing blob table, ensure the schema metadata matches the table definition</li> </ul>"},{"location":"integrations/spark/operations/dml/delete/","title":"DELETE","text":"<p>Currently, delete only supports for Spark 3.5+.</p> <p>Delete with condition:</p> <pre><code>DELETE FROM users WHERE id = 4;\n</code></pre> <p>Delete all rows:</p> <pre><code>DELETE FROM users;\n</code></pre>"},{"location":"integrations/spark/operations/dml/insert-into/","title":"INSERT INTO","text":"<p>Add data to existing Lance tables.</p> <p>Insert individual rows:</p> <pre><code>INSERT INTO users VALUES \n    (4, 'David', 'david@example.com', '2024-01-15 10:30:00'),\n    (5, 'Eva', 'eva@example.com', '2024-01-15 11:45:00');\n</code></pre> <p>Insert with column specification:</p> <pre><code>INSERT INTO users (id, name, email) VALUES \n    (6, 'Frank', 'frank@example.com'),\n    (7, 'Grace', 'grace@example.com');\n</code></pre> <p>Insert from SELECT query:</p> <pre><code>INSERT INTO users\nSELECT user_id as id, username as name, email_address as email, signup_date as created_at\nFROM staging.user_signups\nWHERE signup_date &gt;= '2024-01-01';\n</code></pre> <p>Insert with complex data types:</p> <pre><code>INSERT INTO events VALUES (\n    1001,\n    123,\n    'page_view',\n    array('web', 'desktop'),\n    struct('web_app', 1, '2024-01-15 12:00:00'),\n    '2024-01-15 12:00:00'\n);\n</code></pre>"},{"location":"integrations/spark/operations/dml/update/","title":"UPDATE","text":"<p>Currently, update only supports for Spark 3.5+.</p> <p>Update with condition:</p> <pre><code>UPDATE users \nSET name = 'Updated Name' \nWHERE id = 4;\n</code></pre> <p>Update with complex data types:</p> <pre><code>UPDATE events \nSET metadata = named_struct('source', 'ios', 'version', 1, 'processed_at', timestamp'2024-01-15 13:00:00') \nWHERE event_id = 1001;\n</code></pre> <p>Update struct's field:</p> <pre><code>UPDATE events \nSET metadata = named_struct('source', metadata.source, 'version', 2, 'processed_at', timestamp'2024-01-15 13:00:00') \nWHERE event_id = 1001;\n</code></pre> <p>Update array field:</p> <pre><code>UPDATE events\nSET tags = ARRAY('ios', 'mobile')\nWHERE event_id = 1001;\n</code></pre>"},{"location":"integrations/spark/operations/dql/dataframe-read/","title":"DataFrame Read","text":"<p>Load Lance tables as DataFrames for programmatic data access.</p> PythonScalaJava <pre><code># Load table as DataFrame\nusers_df = spark.table(\"users\")\n\n# Use DataFrame operations\nfiltered_users = users_df.filter(\"age &gt; 25\").select(\"name\", \"email\")\nfiltered_users.show()\n</code></pre> <pre><code>// Load table as DataFrame\nval usersDF = spark.table(\"users\")\n\n// Use DataFrame operations\nval filteredUsers = usersDF.filter(\"age &gt; 25\").select(\"name\", \"email\")\nfilteredUsers.show()\n</code></pre> <pre><code>// Load table as DataFrame\nDataset&lt;Row&gt; usersDF = spark.table(\"users\");\n\n// Use DataFrame operations\nDataset&lt;Row&gt; filteredUsers = usersDF.filter(\"age &gt; 25\").select(\"name\", \"email\");\nfilteredUsers.show();\n</code></pre>"},{"location":"integrations/spark/operations/dql/dataframe-read/#reading-blob-data","title":"Reading Blob Data","text":"<p>When reading from tables with blob columns, the blob data itself is not materialized. Instead, you can access blob metadata through virtual columns.</p> PythonScalaJava <pre><code># Read table with blob column\ndocuments_df = spark.table(\"documents\")\n\n# Access blob metadata using virtual columns\nblob_metadata = documents_df.select(\n    \"id\",\n    \"title\",\n    \"content__blob_pos\",\n    \"content__blob_size\"\n)\nblob_metadata.show()\n\n# Filter by blob size\nlarge_blobs = documents_df.filter(\"content__blob_size &gt; 1000000\")\nlarge_blobs.select(\"id\", \"title\", \"content__blob_size\").show()\n</code></pre> <pre><code>// Read table with blob column\nval documentsDF = spark.table(\"documents\")\n\n// Access blob metadata using virtual columns\nval blobMetadata = documentsDF.select(\n  \"id\",\n  \"title\",\n  \"content__blob_pos\",\n  \"content__blob_size\"\n)\nblobMetadata.show()\n\n// Filter by blob size\nval largeBlobs = documentsDF.filter(\"content__blob_size &gt; 1000000\")\nlargeBlobs.select(\"id\", \"title\", \"content__blob_size\").show()\n</code></pre> <pre><code>// Read table with blob column\nDataset&lt;Row&gt; documentsDF = spark.table(\"documents\");\n\n// Access blob metadata using virtual columns\nDataset&lt;Row&gt; blobMetadata = documentsDF.select(\n    \"id\",\n    \"title\",\n    \"content__blob_pos\",\n    \"content__blob_size\"\n);\nblobMetadata.show();\n\n// Filter by blob size\nDataset&lt;Row&gt; largeBlobs = documentsDF.filter(\"content__blob_size &gt; 1000000\");\nlargeBlobs.select(\"id\", \"title\", \"content__blob_size\").show();\n</code></pre>"},{"location":"integrations/spark/operations/dql/dataframe-read/#blob-virtual-columns","title":"Blob Virtual Columns","text":"<p>For each blob column, Lance provides two virtual columns:</p> <ul> <li><code>&lt;column_name&gt;__blob_pos</code> - The byte position of the blob in the blob file</li> <li><code>&lt;column_name&gt;__blob_size</code> - The size of the blob in bytes</li> </ul> <p>These virtual columns can be used for:</p> <ul> <li>Monitoring blob storage statistics</li> <li>Filtering rows by blob size</li> <li>Implementing custom blob retrieval logic</li> <li>Verifying successful blob writes</li> </ul> <p>Note: The blob column itself returns empty byte arrays when read. To access the actual blob data, you would need to use the position and size information to read from the blob file using external tools.</p>"},{"location":"integrations/spark/operations/dql/dataframe-read/#count-optimization","title":"Count Optimization","text":"<p>Lance-Spark automatically optimizes <code>count()</code> operations through aggregate pushdown. When counting rows, only the <code>_rowid</code> metadata column is scanned instead of reading all data columns, significantly improving performance for tables with large binary data or many columns.</p> PythonScalaJava <pre><code># Optimized count\ntotal_count = spark.table(\"users\").count()\n\n# Count with filter (also optimized)\nfiltered_count = spark.table(\"users\").filter(\"age &gt; 25\").count()\n\n# Group by count (optimized)\ndept_counts = spark.table(\"users\").groupBy(\"department\").count()\ndept_counts.show()\n</code></pre> <pre><code>// Optimized count\nval totalCount = spark.table(\"users\").count()\n\n// Count with filter (also optimized)\nval filteredCount = spark.table(\"users\").filter(\"age &gt; 25\").count()\n\n// Group by count (optimized)\nval deptCounts = spark.table(\"users\").groupBy(\"department\").count()\ndeptCounts.show()\n</code></pre> <pre><code>// Optimized count\nlong totalCount = spark.table(\"users\").count();\n\n// Count with filter (also optimized)\nlong filteredCount = spark.table(\"users\").filter(\"age &gt; 25\").count();\n\n// Group by count (optimized)\nDataset&lt;Row&gt; deptCounts = spark.table(\"users\").groupBy(\"department\").count();\ndeptCounts.show();\n</code></pre> <p>This optimization is automatic and requires no special configuration. For tables with blob columns or large datasets, count operations can be orders of magnitude faster than scanning all columns.</p>"},{"location":"integrations/spark/operations/dql/select/","title":"SELECT","text":"<p>Query data from Lance tables using standard SQL SELECT statements.</p> <p>Select all data from a table:</p> <pre><code>SELECT * FROM users;\n</code></pre> <p>Select specific columns:</p> <pre><code>SELECT id, name, email FROM users;\n</code></pre> <p>Query with WHERE clause:</p> <pre><code>SELECT * FROM users WHERE age &gt; 25;\n</code></pre> <p>Aggregate queries:</p> <pre><code>SELECT department, COUNT(*) as employee_count\nFROM users\nGROUP BY department;\n</code></pre>"},{"location":"integrations/spark/operations/dql/select/#count-star-optimization","title":"Count Star Optimization","text":"<p>Lance-Spark automatically optimizes <code>COUNT(*)</code> queries through aggregate pushdown. When you use <code>COUNT(*)</code>, the query scans only the <code>_rowid</code> metadata column instead of reading all data columns, which significantly improves performance especially for tables with large binary data or many columns.</p> <pre><code>-- Optimized count query\nSELECT COUNT(*) FROM users;\n\n-- Count with filter (also optimized)\nSELECT COUNT(*) FROM users WHERE age &gt; 25;\n\n-- Group by count (optimized)\nSELECT department, COUNT(*)\nFROM users\nGROUP BY department;\n</code></pre> <p>This optimization is automatic and requires no special configuration. For tables with blob columns or large datasets, <code>COUNT(*)</code> queries can be orders of magnitude faster than scanning all columns.</p> <p>Join queries:</p> <pre><code>SELECT u.name, p.title\nFROM users u\nJOIN projects p ON u.id = p.user_id;\n</code></pre>"},{"location":"integrations/spark/operations/dql/select/#querying-blob-columns","title":"Querying Blob Columns","text":"<p>When querying tables with blob columns, the blob data itself is not materialized by default. Instead, you can access blob metadata through virtual columns.</p>"},{"location":"integrations/spark/operations/dql/select/#selecting-blob-metadata","title":"Selecting Blob Metadata","text":"<p>Query blob position and size information:</p> <pre><code>SELECT id, title, content__blob_pos, content__blob_size\nFROM documents\nWHERE id = 1;\n</code></pre> <p>The virtual columns available for blob columns are: - <code>&lt;column_name&gt;__blob_pos</code> - Byte position in the blob file - <code>&lt;column_name&gt;__blob_size</code> - Size of the blob in bytes</p>"},{"location":"integrations/spark/operations/dql/select/#filtering-and-aggregating","title":"Filtering and Aggregating","text":"<p>You can filter and aggregate using blob metadata:</p> <pre><code>-- Find large blobs\nSELECT id, title, content__blob_size\nFROM documents\nWHERE content__blob_size &gt; 1000000;\n\n-- Get blob statistics\nSELECT\n    COUNT(*) as blob_count,\n    AVG(content__blob_size) as avg_size,\n    MAX(content__blob_size) as max_size\nFROM documents;\n</code></pre>"},{"location":"integrations/spark/operations/dql/select/#selecting-non-blob-columns","title":"Selecting Non-Blob Columns","text":"<p>When you don't need the blob data, select only the non-blob columns for better performance:</p> <pre><code>-- Select without blob column\nSELECT id, title, created_at\nFROM documents\nWHERE title LIKE '%report%';\n</code></pre> <p>Note: The blob column itself returns empty byte arrays when selected. To access actual blob data, you would need to use the position and size information to read from the blob file using external tools or custom logic.</p>"},{"location":"quickstart/","title":"Getting Started with Lance Tables","text":"<p>This quickstart guide will walk you through the core features of Lance including creating datasets, versioning, and vector search.</p> <p>By the end of this tutorial, you'll be able to create Lance datasets from pandas DataFrames and convert existing Parquet files to Lance format. You'll also understand the basic workflow for working with Lance datasets and be prepared to explore advanced features like versioning and vector search.</p>"},{"location":"quickstart/#install-the-python-sdk","title":"Install the Python SDK","text":"<p>The easiest way to get started with Lance is via our Python SDK <code>pylance</code>:</p> <pre><code>pip install pylance\n</code></pre> <p>For the latest features and bug fixes, you can install the preview version:</p> <pre><code>pip install --pre --extra-index-url https://pypi.fury.io/lancedb/pylance\n</code></pre> <p>Note: Preview releases receive the same level of testing as regular releases.</p>"},{"location":"quickstart/#set-up-your-environment","title":"Set Up Your Environment","text":"<p>First, let's import the necessary libraries:</p> <pre><code>import shutil\nimport lance\nimport numpy as np\nimport pandas as pd\nimport pyarrow as pa\n</code></pre>"},{"location":"quickstart/#create-your-first-dataset","title":"Create Your First Dataset","text":"<p>Lance is built on top of Apache Arrow, making it incredibly easy to work with pandas DataFrames and Arrow tables. You can create Lance datasets from various data sources including pandas DataFrames, Arrow tables, and existing Parquet files. Lance automatically handles the conversion and optimization for you.</p>"},{"location":"quickstart/#create-a-simple-dataset","title":"Create a Simple Dataset","text":"<p>You'll create a simple dataframe and then write it to Lance format. This demonstrates the basic workflow for creating Lance datasets.</p> <p>Create a simple dataframe:</p> <pre><code>df = pd.DataFrame({\"a\": [5]})\ndf\n</code></pre> <p>Now you'll write this dataframe to Lance format and verify the data was saved correctly:</p> <pre><code>shutil.rmtree(\"/tmp/test.lance\", ignore_errors=True)\n\ndataset = lance.write_dataset(df, \"/tmp/test.lance\")\ndataset.to_table().to_pandas()\n</code></pre>"},{"location":"quickstart/#convert-your-existing-parquet-files","title":"Convert Your Existing Parquet Files","text":"<p>You'll convert an existing Parquet file to Lance format. This shows how to migrate your existing data to Lance.</p> <p>First, you'll create a Parquet file and then convert it to Lance:</p> <pre><code>shutil.rmtree(\"/tmp/test.parquet\", ignore_errors=True)\nshutil.rmtree(\"/tmp/test.lance\", ignore_errors=True)\n\ntbl = pa.Table.from_pandas(df)\npa.dataset.write_dataset(tbl, \"/tmp/test.parquet\", format='parquet')\n\nparquet = pa.dataset.dataset(\"/tmp/test.parquet\")\nparquet.to_table().to_pandas()\n</code></pre> <p>Now you'll convert the Parquet dataset to Lance format in a single line:</p> <pre><code>dataset = lance.write_dataset(parquet, \"/tmp/test.lance\")\n\n# Make sure it's the same\ndataset.to_table().to_pandas()\n</code></pre>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<p>Now that you've mastered the basics of creating Lance datasets, here's what you can explore next:</p> <ul> <li>Versioning Your Datasets with Lance - Learn how to track changes over time with native versioning</li> <li>Vector Indexing and Vector Search With Lance - Build high-performance vector search capabilities with ANN indexes</li> </ul>"},{"location":"quickstart/vector-search/","title":"Vector Indexing and Vector Search With Lance","text":"<p>Lance provides high-performance vector search capabilities with ANN (Approximate Nearest Neighbor) indexes.</p> <p>By the end of this tutorial, you'll be able to build and use ANN indexes to dramatically speed up vector search operations while maintaining high accuracy. You'll also learn how to tune search parameters for optimal performance and combine vector search with metadata queries in a single operation.</p>"},{"location":"quickstart/vector-search/#install-the-python-sdk","title":"Install the Python SDK","text":"<pre><code>pip install pylance\n</code></pre>"},{"location":"quickstart/vector-search/#set-up-your-environment","title":"Set Up Your Environment","text":"<p>First, import the necessary libraries:</p> <pre><code>import shutil\nimport lance\nimport numpy as np\nimport pandas as pd\nimport pyarrow as pa\nimport duckdb\n</code></pre>"},{"location":"quickstart/vector-search/#prepare-your-vector-embeddings","title":"Prepare Your Vector Embeddings","text":"<p>For this tutorial, download and prepare the SIFT 1M dataset for vector search experiments.</p> <ul> <li>Download <code>ANN_SIFT1M</code> from: http://corpus-texmex.irisa.fr/</li> <li>Direct link: <code>ftp://ftp.irisa.fr/local/texmex/corpus/sift.tar.gz</code></li> </ul> <p>You can just use <code>wget</code>:</p> <pre><code>rm -rf sift* vec_data.lance\nwget ftp://ftp.irisa.fr/local/texmex/corpus/sift.tar.gz\ntar -xzf sift.tar.gz\n</code></pre>"},{"location":"quickstart/vector-search/#convert-your-data-to-lance-format","title":"Convert Your Data to Lance Format","text":"<p>Then, convert the raw vector data into Lance format for efficient storage and querying.</p> <pre><code>from lance.vector import vec_to_table\nimport struct\n\nuri = \"vec_data.lance\"\n\nwith open(\"sift/sift_base.fvecs\", mode=\"rb\") as fobj:\n    buf = fobj.read()\n    data = np.array(struct.unpack(\"&lt;128000000f\", buf[4 : 4 + 4 * 1000000 * 128])).reshape((1000000, 128))\n    dd = dict(zip(range(1000000), data))\n\ntable = vec_to_table(dd)\nlance.write_dataset(table, uri, max_rows_per_group=8192, max_rows_per_file=1024*1024)\n</code></pre> <p>Now you can load the dataset:</p> <pre><code>uri = \"vec_data.lance\"\nsift1m = lance.dataset(uri)\n</code></pre>"},{"location":"quickstart/vector-search/#search-without-an-index","title":"Search Without an Index","text":"<p>You'll perform vector search without an index to see the baseline performance, then compare it with indexed search.</p> <p>First, let's sample some query vectors:</p> <pre><code>import duckdb\n# Make sure DuckDB v0.7+ is installed\nsamples = duckdb.query(\"SELECT vector FROM sift1m USING SAMPLE 100\").to_df().vector\n</code></pre> <pre><code>0     [29.0, 10.0, 1.0, 50.0, 7.0, 89.0, 95.0, 51.0,...\n1     [7.0, 5.0, 39.0, 49.0, 17.0, 12.0, 83.0, 117.0...\n2     [0.0, 0.0, 0.0, 10.0, 12.0, 31.0, 6.0, 0.0, 0....\n3     [0.0, 2.0, 9.0, 1.793662034335766e-43, 30.0, 1...\n4     [54.0, 112.0, 16.0, 0.0, 0.0, 7.0, 112.0, 44.0...\n                            ...\n95    [1.793662034335766e-43, 33.0, 47.0, 28.0, 0.0,...\n96    [1.0, 4.0, 2.0, 32.0, 3.0, 7.0, 119.0, 116.0, ...\n97    [17.0, 46.0, 12.0, 0.0, 0.0, 3.0, 23.0, 58.0, ...\n98    [0.0, 11.0, 30.0, 14.0, 34.0, 7.0, 0.0, 0.0, 1...\n99    [20.0, 8.0, 121.0, 98.0, 37.0, 77.0, 9.0, 18.0...\nName: vector, Length: 100, dtype: object\n</code></pre> <p>Now, perform nearest neighbor search without an index:</p> <pre><code>import time\n\nstart = time.time()\ntbl = sift1m.to_table(columns=[\"id\"], nearest={\"column\": \"vector\", \"q\": samples[0], \"k\": 10})\nend = time.time()\n\nprint(f\"Time(sec): {end-start}\")\nprint(tbl.to_pandas())\n</code></pre> <p>Expected output: <pre><code>Time(sec): 0.10735273361206055\n       id                                             vector    score\n0  144678  [29.0, 10.0, 1.0, 50.0, 7.0, 89.0, 95.0, 51.0,...      0.0\n1  575538  [2.0, 0.0, 1.0, 42.0, 3.0, 38.0, 152.0, 27.0, ...  76908.0\n2  241428  [11.0, 0.0, 2.0, 118.0, 11.0, 108.0, 116.0, 21...  92877.0\n...\n</code></pre></p> <p>Without the index, the search will scan throughout the whole dataset to compute the distance between each data point. For practical real-time performance with, you will get much better performance with an ANN index.</p>"},{"location":"quickstart/vector-search/#build-the-search-index","title":"Build the Search Index","text":"<p>If you build an ANN index - you can dramatically speed up vector search operations while maintaining high accuracy. In this example, we will build the <code>IVF_PQ</code> index: </p> <pre><code>sift1m.create_index(\n    \"vector\",\n    index_type=\"IVF_PQ\", # specify the IVF_PQ index type\n    num_partitions=256,  # IVF\n    num_sub_vectors=16,  # PQ\n)\n</code></pre> <p>The sample response should look like this:</p> <pre><code>Building vector index: IVF256,PQ16\nCPU times: user 2min 23s, sys: 2.77 s, total: 2min 26s\nWall time: 22.7 s\nSample 65536 out of 1000000 to train kmeans of 128 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\nSample 65536 out of 1000000 to train kmeans of 8 dim, 256 clusters\n</code></pre> <p>Index Creation Performance</p> <p>If you're trying this on your own data, make sure your vector (dimensions / num_sub_vectors) % 8 == 0, or else index creation will take much longer than expected due to SIMD misalignment.</p>"},{"location":"quickstart/vector-search/#vector-search-with-the-ann-index","title":"Vector Search with the ANN Index","text":"<p>You can now perform the same search operation using your newly created index and see the dramatic performance improvement.</p> <pre><code>sift1m = lance.dataset(uri)\n\nimport time\n\ntot = 0\nfor q in samples:\n    start = time.time()\n    tbl = sift1m.to_table(nearest={\"column\": \"vector\", \"q\": q, \"k\": 10})\n    end = time.time()\n    tot += (end - start)\n\nprint(f\"Avg(sec): {tot / len(samples)}\")\nprint(tbl.to_pandas())\n</code></pre> <p>Expected output: <pre><code>Avg(sec): 0.0009334301948547364\n       id                                             vector         score\n0  378825  [20.0, 8.0, 121.0, 98.0, 37.0, 77.0, 9.0, 18.0...  16560.197266\n1  143787  [11.0, 24.0, 122.0, 122.0, 53.0, 4.0, 0.0, 3.0...  61714.941406\n2  356895  [0.0, 14.0, 67.0, 122.0, 83.0, 23.0, 1.0, 0.0,...  64147.218750\n3  535431  [9.0, 22.0, 118.0, 118.0, 4.0, 5.0, 4.0, 4.0, ...  69092.593750\n4  308778  [1.0, 7.0, 48.0, 123.0, 73.0, 36.0, 8.0, 4.0, ...  69131.812500\n5  222477  [14.0, 73.0, 39.0, 4.0, 16.0, 94.0, 19.0, 8.0,...  69244.195312\n6  672558  [2.0, 1.0, 0.0, 11.0, 36.0, 23.0, 7.0, 10.0, 0...  70264.828125\n7  365538  [54.0, 43.0, 97.0, 59.0, 34.0, 17.0, 10.0, 15....  70273.710938\n8  659787  [10.0, 9.0, 23.0, 121.0, 38.0, 26.0, 38.0, 9.0...  70374.703125\n9  603930  [32.0, 32.0, 122.0, 122.0, 70.0, 4.0, 15.0, 12...  70583.375000\n</code></pre></p> <p>Performance Note</p> <p>Your actual numbers will vary by your storage. These numbers are from local disk on an M2 MacBook Air. If you're querying S3 directly, HDD, or network drives, performance will be slower.</p>"},{"location":"quickstart/vector-search/#tune-the-search-parameters","title":"Tune the Search Parameters","text":"<p>You need to adjust search parameters to balance between speed and accuracy, finding the optimal settings for your use case.</p> <p>The latency vs recall is tunable via: - nprobes: how many IVF partitions to search - refine_factor: determines how many vectors are retrieved during re-ranking</p> <pre><code>%%time\n\nsift1m.to_table(\n    nearest={\n        \"column\": \"vector\",\n        \"q\": samples[0],\n        \"k\": 10,\n        \"nprobes\": 10,\n        \"refine_factor\": 5,\n    }\n).to_pandas()\n</code></pre> <p>Parameter Explanation: - <code>q</code> =&gt; sample vector - <code>k</code> =&gt; how many neighbors to return - <code>nprobes</code> =&gt; how many partitions (in the coarse quantizer) to probe - <code>refine_factor</code> =&gt; controls \"re-ranking\". If k=10 and refine_factor=5 then retrieve 50 nearest neighbors by ANN and re-sort using actual distances then return top 10. This improves recall without sacrificing performance too much</p> <p>Memory Usage</p> <p>The latencies above include file I/O as Lance currently doesn't hold anything in memory. Along with index building speed, creating a purely in-memory version of the dataset would make the biggest impact on performance.</p>"},{"location":"quickstart/vector-search/#combine-features-and-vectors","title":"Combine Features and Vectors","text":"<p>You can add metadata columns to your vector dataset and query both vectors and features together in a single operation.</p> <p>In real-life situations, users have other feature or metadata columns that need to be stored and fetched together. If you're managing data and the index separately, you have to do a bunch of annoying plumbing to put stuff together. </p> <p>With Lance, you can add columns directly to the dataset using <code>add_columns()</code>. For basic use cases, you can use SQL:</p> <p><pre><code>sift1m.add_columns(\n    {\n        \"item_id\": \"id + 1000000\",\n        \"revenue\": \"random() * 1000 + 5000\",\n    }\n)\n</code></pre> For more complex columns, you can provide a Python function to generate the new column data: <pre><code>@lance.batch_udf()\ndef add_columns_func(batch: pa.Table) -&gt; pd.DataFrame:\n    \"\"\"Add item_id and revenue columns to a batch of data.\n\n    Args:\n        batch: PyArrow Table containing the original data\n\n    Returns:\n        Pandas DataFrame with added item_id and revenue columns\n    \"\"\"\n    item_ids: np.ndarray = np.arange(batch.num_rows)\n    revenue: np.ndarray = (np.random.randn(batch.num_rows) + 5) * 1000\n    return pd.DataFrame({\"item_id\": item_ids, \"revenue\": revenue})\n\n\nsift1m.add_columns(add_columns_func)\n</code></pre> You can then query both vectors and metadata together:</p> <pre><code># Get vectors and metadata together\nresult = sift1m.to_table(\n    columns=[\"item_id\", \"revenue\"],\n    nearest={\"column\": \"vector\", \"q\": samples[0], \"k\": 10}\n)\nprint(result.to_pandas())\n</code></pre>"},{"location":"quickstart/vector-search/#next-steps","title":"Next Steps","text":"<p>You should check out Versioning Your Datasets with Lance. We'll show you how to version your vector datasets and track changes over time.</p>"},{"location":"quickstart/versioning/","title":"Versioning Your Datasets with Lance","text":"<p>Lance supports versioning natively, allowing you to track changes over time. </p> <p>In this tutorial, you'll learn how to append new data to existing datasets while preserving historical versions and access specific versions using version numbers or meaningful tags. You'll also understand how to implement proper data governance practices with Lance's native versioning capabilities.</p>"},{"location":"quickstart/versioning/#install-the-python-sdk","title":"Install the Python SDK","text":"<pre><code>pip install pylance\n</code></pre>"},{"location":"quickstart/versioning/#set-up-your-environment","title":"Set Up Your Environment","text":"<p>First, you should import the necessary libraries:</p> <pre><code>import shutil\nimport lance\nimport numpy as np\nimport pandas as pd\nimport pyarrow as pa\n</code></pre>"},{"location":"quickstart/versioning/#append-new-data-to-your-dataset","title":"Append New Data to Your Dataset","text":"<p>You can add new rows to your existing dataset, creating a new version while preserving the original data. Here is how to append rows:</p> <pre><code>df = pd.DataFrame({\"a\": [10]})\ntbl = pa.Table.from_pandas(df)\ndataset = lance.write_dataset(tbl, \"/tmp/test.lance\", mode=\"append\")\n\ndataset.to_table().to_pandas()\n</code></pre>"},{"location":"quickstart/versioning/#overwrite-your-dataset","title":"Overwrite Your Dataset","text":"<p>You can completely replace your dataset with new data, creating a new version while keeping the old version accessible.</p> <p>Here is how to overwrite the data and create a new version:</p> <pre><code>df = pd.DataFrame({\"a\": [50, 100]})\ntbl = pa.Table.from_pandas(df)\ndataset = lance.write_dataset(tbl, \"/tmp/test.lance\", mode=\"overwrite\")\n\ndataset.to_table().to_pandas()\n</code></pre>"},{"location":"quickstart/versioning/#access-previous-dataset-versions","title":"Access Previous Dataset Versions","text":"<p>You can also check what versions are available and then access specific versions of your dataset.</p> <p>List all versions of a dataset with this request:</p> <pre><code>dataset.versions()\n</code></pre> <p>You can also access any available version:</p> <pre><code># Version 1\nlance.dataset('/tmp/test.lance', version=1).to_table().to_pandas()\n\n# Version 2\nlance.dataset('/tmp/test.lance', version=2).to_table().to_pandas()\n</code></pre>"},{"location":"quickstart/versioning/#tag-your-important-versions","title":"Tag Your Important Versions","text":"<p>Create named tags for important versions, making it easier to reference specific versions by meaningful names. To create tags for relevant versions, do this:</p> <pre><code>dataset.tags.create(\"stable\", 2)\ndataset.tags.create(\"nightly\", 3)\ndataset.tags.list()\n</code></pre> <p>Tags can be checked out like versions:</p> <pre><code>lance.dataset('/tmp/test.lance', version=\"stable\").to_table().to_pandas()\n</code></pre>"},{"location":"quickstart/versioning/#next-steps","title":"Next Steps","text":"<p>Now that you've mastered dataset versioning with Lance, check out Vector Indexing and Vector Search With Lance. You can learn how to build high-performance vector search capabilities on top of your Lance tables.</p> <p>This will teach you how to build fast, scalable search capabilities for your versioned datasets. </p>"}]}