{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2cDKD6ufSFoFzytAgPxIS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eto-ai/lance/blob/lei%2Fdino_coco/dino_coco.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection\n",
        "\n",
        "https://github.com/IDEA-Research/DINO\n",
        "\n",
        "[Papers With Code Link](https://paperswithcode.com/paper/focal-modulation-networks)"
      ],
      "metadata": {
        "id": "XB-jitxQsQuj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgFU4gGiroou"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet pylance duckdb torch torchvision transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build and install [DINO]() Model"
      ],
      "metadata": {
        "id": "bj2GEDTCvW1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git -C DINO pull || git clone https://github.com/IDEACVR/DINO\n",
        "%cd DINO\n",
        "\n",
        "!pip install --quiet -r requirements.txt \\\n",
        "  && cd models/dino/ops \\\n",
        "  && python setup.py -q build install"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCFwh-_AtGqR",
        "outputId": "494fd141-17f1-4b20-c75a-b95fcb9e26ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n",
            "/content/DINO\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/cpp_extension.py:411: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "  warnings.warn(msg.format('we could not find ninja.'))\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/cpp_extension.py:813: UserWarning: The detected CUDA version (11.2) has a minor version mismatch with the version that was used to compile PyTorch (11.3). Most likely this shouldn't be a problem.\n",
            "  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "__pycache__.MultiScaleDeformableAttention.cpython-38: module references __file__\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# See https://github.com/IDEA-Research/DINO/blob/main/inference_and_visualization.ipynb\n",
        "# for instruction to load model\n",
        "from util.slconfig import SLConfig\n",
        "from main import build_model_main\n",
        "model_config_path = \"config/DINO/DINO_4scale.py\"\n",
        "\n",
        "args = SLConfig.fromfile(model_config_path) \n",
        "args.device = 'cuda' \n",
        "model, criterion, postprocessors = build_model_main(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zL2g1T-lveXO",
        "outputId": "08d51393-d5e1-49ab-9693-1f8eed9189f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloads weights\n",
        "\n",
        "# Download DINO-4scale weights\n",
        "! [[ -f /tmp/model.pt ]] || gsutil cp gs://eto-public/models/dino/checkpoint0033_4scale.pth /tmp/model.pt\n",
        "import torch\n",
        "model_checkpoint_path = \"/tmp/model.pt\"\n",
        "checkpoint = torch.load(model_checkpoint_path)\n",
        "model.load_state_dict(checkpoint['model'])\n",
        "_ = model.eval()"
      ],
      "metadata": {
        "id": "27ah_AGcxHS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xxfGl-V6xeqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1kIOvcR2RxT",
        "outputId": "fa4a8aaa-6424-47d2-8121-c5f18a52e1e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DINO(\n",
              "  (transformer): DeformableTransformer(\n",
              "    (encoder): TransformerEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0): DeformableTransformerEncoderLayer(\n",
              "          (self_attn): MSDeformAttn(\n",
              "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (dropout1): Dropout(p=0.0, inplace=False)\n",
              "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
              "          (dropout2): Dropout(p=0.0, inplace=False)\n",
              "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
              "          (dropout3): Dropout(p=0.0, inplace=False)\n",
              "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): DeformableTransformerEncoderLayer(\n",
              "          (self_attn): MSDeformAttn(\n",
              "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (dropout1): Dropout(p=0.0, inplace=False)\n",
              "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
              "          (dropout2): Dropout(p=0.0, inplace=False)\n",
              "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
              "          (dropout3): Dropout(p=0.0, inplace=False)\n",
              "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): DeformableTransformerEncoderLayer(\n",
              "          (self_attn): MSDeformAttn(\n",
              "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (dropout1): Dropout(p=0.0, inplace=False)\n",
              "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
              "          (dropout2): Dropout(p=0.0, inplace=False)\n",
              "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
              "          (dropout3): Dropout(p=0.0, inplace=False)\n",
              "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): DeformableTransformerEncoderLayer(\n",
              "          (self_attn): MSDeformAttn(\n",
              "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (dropout1): Dropout(p=0.0, inplace=False)\n",
              "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
              "          (dropout2): Dropout(p=0.0, inplace=False)\n",
              "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
              "          (dropout3): Dropout(p=0.0, inplace=False)\n",
              "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): DeformableTransformerEncoderLayer(\n",
              "          (self_attn): MSDeformAttn(\n",
              "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (dropout1): Dropout(p=0.0, inplace=False)\n",
              "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
              "          (dropout2): Dropout(p=0.0, inplace=False)\n",
              "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
              "          (dropout3): Dropout(p=0.0, inplace=False)\n",
              "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): DeformableTransformerEncoderLayer(\n",
              "          (self_attn): MSDeformAttn(\n",
              "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (dropout1): Dropout(p=0.0, inplace=False)\n",
              "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
              "          (dropout2): Dropout(p=0.0, inplace=False)\n",
              "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
              "          (dropout3): Dropout(p=0.0, inplace=False)\n",
              "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (decoder): TransformerDecoder(\n",
              "      (layers): ModuleList(\n",
              "        (0): DeformableTransformerDecoderLayer(\n",
              "          (cross_attn): MSDeformAttn(\n",
              "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (dropout1): Dropout(p=0.0, inplace=False)\n",
              "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (dropout2): Dropout(p=0.0, inplace=False)\n",
              "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
              "          (dropout3): Dropout(p=0.0, inplace=False)\n",
              "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
              "          (dropout4): Dropout(p=0.0, inplace=False)\n",
              "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): DeformableTransformerDecoderLayer(\n",
              "          (cross_attn): MSDeformAttn(\n",
              "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (dropout1): Dropout(p=0.0, inplace=False)\n",
              "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (dropout2): Dropout(p=0.0, inplace=False)\n",
              "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
              "          (dropout3): Dropout(p=0.0, inplace=False)\n",
              "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
              "          (dropout4): Dropout(p=0.0, inplace=False)\n",
              "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): DeformableTransformerDecoderLayer(\n",
              "          (cross_attn): MSDeformAttn(\n",
              "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (dropout1): Dropout(p=0.0, inplace=False)\n",
              "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (dropout2): Dropout(p=0.0, inplace=False)\n",
              "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
              "          (dropout3): Dropout(p=0.0, inplace=False)\n",
              "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
              "          (dropout4): Dropout(p=0.0, inplace=False)\n",
              "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): DeformableTransformerDecoderLayer(\n",
              "          (cross_attn): MSDeformAttn(\n",
              "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (dropout1): Dropout(p=0.0, inplace=False)\n",
              "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (dropout2): Dropout(p=0.0, inplace=False)\n",
              "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
              "          (dropout3): Dropout(p=0.0, inplace=False)\n",
              "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
              "          (dropout4): Dropout(p=0.0, inplace=False)\n",
              "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): DeformableTransformerDecoderLayer(\n",
              "          (cross_attn): MSDeformAttn(\n",
              "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (dropout1): Dropout(p=0.0, inplace=False)\n",
              "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (dropout2): Dropout(p=0.0, inplace=False)\n",
              "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
              "          (dropout3): Dropout(p=0.0, inplace=False)\n",
              "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
              "          (dropout4): Dropout(p=0.0, inplace=False)\n",
              "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): DeformableTransformerDecoderLayer(\n",
              "          (cross_attn): MSDeformAttn(\n",
              "            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (dropout1): Dropout(p=0.0, inplace=False)\n",
              "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (dropout2): Dropout(p=0.0, inplace=False)\n",
              "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
              "          (dropout3): Dropout(p=0.0, inplace=False)\n",
              "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
              "          (dropout4): Dropout(p=0.0, inplace=False)\n",
              "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (ref_point_head): MLP(\n",
              "        (layers): ModuleList(\n",
              "          (0): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (1): Linear(in_features=256, out_features=256, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (bbox_embed): ModuleList(\n",
              "        (0): MLP(\n",
              "          (layers): ModuleList(\n",
              "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
              "          )\n",
              "        )\n",
              "        (1): MLP(\n",
              "          (layers): ModuleList(\n",
              "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
              "          )\n",
              "        )\n",
              "        (2): MLP(\n",
              "          (layers): ModuleList(\n",
              "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
              "          )\n",
              "        )\n",
              "        (3): MLP(\n",
              "          (layers): ModuleList(\n",
              "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
              "          )\n",
              "        )\n",
              "        (4): MLP(\n",
              "          (layers): ModuleList(\n",
              "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
              "          )\n",
              "        )\n",
              "        (5): MLP(\n",
              "          (layers): ModuleList(\n",
              "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (class_embed): ModuleList(\n",
              "        (0): Linear(in_features=256, out_features=91, bias=True)\n",
              "        (1): Linear(in_features=256, out_features=91, bias=True)\n",
              "        (2): Linear(in_features=256, out_features=91, bias=True)\n",
              "        (3): Linear(in_features=256, out_features=91, bias=True)\n",
              "        (4): Linear(in_features=256, out_features=91, bias=True)\n",
              "        (5): Linear(in_features=256, out_features=91, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (tgt_embed): Embedding(900, 256)\n",
              "    (enc_output): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "    (enc_out_bbox_embed): MLP(\n",
              "      (layers): ModuleList(\n",
              "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (enc_out_class_embed): Linear(in_features=256, out_features=91, bias=True)\n",
              "  )\n",
              "  (label_enc): Embedding(92, 256)\n",
              "  (input_proj): ModuleList(\n",
              "    (0): Sequential(\n",
              "      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "    )\n",
              "    (1): Sequential(\n",
              "      (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "    )\n",
              "    (2): Sequential(\n",
              "      (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "    )\n",
              "    (3): Sequential(\n",
              "      (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "    )\n",
              "  )\n",
              "  (backbone): Joiner(\n",
              "    (0): Backbone(\n",
              "      (body): IntermediateLayerGetter(\n",
              "        (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "        (bn1): FrozenBatchNorm2d()\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "        (layer1): Sequential(\n",
              "          (0): Bottleneck(\n",
              "            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): FrozenBatchNorm2d()\n",
              "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): FrozenBatchNorm2d()\n",
              "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): FrozenBatchNorm2d()\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (downsample): Sequential(\n",
              "              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): FrozenBatchNorm2d()\n",
              "            )\n",
              "          )\n",
              "          (1): Bottleneck(\n",
              "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): FrozenBatchNorm2d()\n",
              "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): FrozenBatchNorm2d()\n",
              "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): FrozenBatchNorm2d()\n",
              "            (relu): ReLU(inplace=True)\n",
              "          )\n",
              "          (2): Bottleneck(\n",
              "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): FrozenBatchNorm2d()\n",
              "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): FrozenBatchNorm2d()\n",
              "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): FrozenBatchNorm2d()\n",
              "            (relu): ReLU(inplace=True)\n",
              "          )\n",
              "        )\n",
              "        (layer2): Sequential(\n",
              "          (0): Bottleneck(\n",
              "            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): FrozenBatchNorm2d()\n",
              "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "            (bn2): FrozenBatchNorm2d()\n",
              "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): FrozenBatchNorm2d()\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (downsample): Sequential(\n",
              "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "              (1): FrozenBatchNorm2d()\n",
              "            )\n",
              "          )\n",
              "          (1): Bottleneck(\n",
              "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): FrozenBatchNorm2d()\n",
              "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): FrozenBatchNorm2d()\n",
              "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): FrozenBatchNorm2d()\n",
              "            (relu): ReLU(inplace=True)\n",
              "          )\n",
              "          (2): Bottleneck(\n",
              "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): FrozenBatchNorm2d()\n",
              "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): FrozenBatchNorm2d()\n",
              "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): FrozenBatchNorm2d()\n",
              "            (relu): ReLU(inplace=True)\n",
              "          )\n",
              "          (3): Bottleneck(\n",
              "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): FrozenBatchNorm2d()\n",
              "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): FrozenBatchNorm2d()\n",
              "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): FrozenBatchNorm2d()\n",
              "            (relu): ReLU(inplace=True)\n",
              "          )\n",
              "        )\n",
              "        (layer3): Sequential(\n",
              "          (0): Bottleneck(\n",
              "            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): FrozenBatchNorm2d()\n",
              "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "            (bn2): FrozenBatchNorm2d()\n",
              "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): FrozenBatchNorm2d()\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (downsample): Sequential(\n",
              "              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "              (1): FrozenBatchNorm2d()\n",
              "            )\n",
              "          )\n",
              "          (1): Bottleneck(\n",
              "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): FrozenBatchNorm2d()\n",
              "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): FrozenBatchNorm2d()\n",
              "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): FrozenBatchNorm2d()\n",
              "            (relu): ReLU(inplace=True)\n",
              "          )\n",
              "          (2): Bottleneck(\n",
              "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): FrozenBatchNorm2d()\n",
              "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): FrozenBatchNorm2d()\n",
              "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): FrozenBatchNorm2d()\n",
              "            (relu): ReLU(inplace=True)\n",
              "          )\n",
              "          (3): Bottleneck(\n",
              "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): FrozenBatchNorm2d()\n",
              "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): FrozenBatchNorm2d()\n",
              "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): FrozenBatchNorm2d()\n",
              "            (relu): ReLU(inplace=True)\n",
              "          )\n",
              "          (4): Bottleneck(\n",
              "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): FrozenBatchNorm2d()\n",
              "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): FrozenBatchNorm2d()\n",
              "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): FrozenBatchNorm2d()\n",
              "            (relu): ReLU(inplace=True)\n",
              "          )\n",
              "          (5): Bottleneck(\n",
              "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): FrozenBatchNorm2d()\n",
              "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): FrozenBatchNorm2d()\n",
              "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): FrozenBatchNorm2d()\n",
              "            (relu): ReLU(inplace=True)\n",
              "          )\n",
              "        )\n",
              "        (layer4): Sequential(\n",
              "          (0): Bottleneck(\n",
              "            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): FrozenBatchNorm2d()\n",
              "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "            (bn2): FrozenBatchNorm2d()\n",
              "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): FrozenBatchNorm2d()\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (downsample): Sequential(\n",
              "              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "              (1): FrozenBatchNorm2d()\n",
              "            )\n",
              "          )\n",
              "          (1): Bottleneck(\n",
              "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): FrozenBatchNorm2d()\n",
              "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): FrozenBatchNorm2d()\n",
              "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): FrozenBatchNorm2d()\n",
              "            (relu): ReLU(inplace=True)\n",
              "          )\n",
              "          (2): Bottleneck(\n",
              "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): FrozenBatchNorm2d()\n",
              "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (bn2): FrozenBatchNorm2d()\n",
              "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): FrozenBatchNorm2d()\n",
              "            (relu): ReLU(inplace=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (1): PositionEmbeddingSineHW()\n",
              "  )\n",
              "  (bbox_embed): ModuleList(\n",
              "    (0): MLP(\n",
              "      (layers): ModuleList(\n",
              "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (1): MLP(\n",
              "      (layers): ModuleList(\n",
              "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (2): MLP(\n",
              "      (layers): ModuleList(\n",
              "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (3): MLP(\n",
              "      (layers): ModuleList(\n",
              "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (4): MLP(\n",
              "      (layers): ModuleList(\n",
              "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (5): MLP(\n",
              "      (layers): ModuleList(\n",
              "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (class_embed): ModuleList(\n",
              "    (0): Linear(in_features=256, out_features=91, bias=True)\n",
              "    (1): Linear(in_features=256, out_features=91, bias=True)\n",
              "    (2): Linear(in_features=256, out_features=91, bias=True)\n",
              "    (3): Linear(in_features=256, out_features=91, bias=True)\n",
              "    (4): Linear(in_features=256, out_features=91, bias=True)\n",
              "    (5): Linear(in_features=256, out_features=91, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}